{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Y5wau-_3zTpo","executionInfo":{"status":"ok","timestamp":1659706557712,"user_tz":-120,"elapsed":23089,"user":{"displayName":"AmirHossein Dabiri Aghdam","userId":"03377875757005762455"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4fac042d-d6a3-454b-e691-8cfc455042ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 365 kB 5.3 MB/s \n","\u001b[K     |████████████████████████████████| 4.7 MB 47.8 MB/s \n","\u001b[K     |████████████████████████████████| 1.2 MB 49.1 MB/s \n","\u001b[K     |████████████████████████████████| 212 kB 62.1 MB/s \n","\u001b[K     |████████████████████████████████| 141 kB 35.3 MB/s \n","\u001b[K     |████████████████████████████████| 115 kB 11.3 MB/s \n","\u001b[K     |████████████████████████████████| 101 kB 7.1 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 55.0 MB/s \n","\u001b[K     |████████████████████████████████| 127 kB 46.8 MB/s \n","\u001b[K     |████████████████████████████████| 6.6 MB 47.2 MB/s \n","\u001b[?25h"]}],"source":["!pip install -q datasets transformers sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_2_D7Xqzt4MN"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import datasets\n","from datasets import Dataset, DatasetDict\n","import os\n","import random\n","import regex as re\n","import logging\n","import gc\n","from tqdm import tqdm, trange\n","from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report\n","from datasets import load_dataset\n","import torch\n","import torch.nn as nn\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n","from transformers import XLMRobertaTokenizer\n","from transformers.models.xlm_roberta.modeling_xlm_roberta import  XLMRobertaModel, XLMRobertaConfig\n","from transformers import BertConfig, BertTokenizer, BertModel\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","plt.style.use('seaborn')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":522,"status":"ok","timestamp":1659707170604,"user":{"displayName":"AmirHossein Dabiri Aghdam","userId":"03377875757005762455"},"user_tz":-120},"id":"ct-dAfGrzifl","colab":{"base_uri":"https://localhost:8080/","height":86,"referenced_widgets":["f13b5142164b4713893b9ad346cac1a6","07491f0f915a4dfb94a59f71beef609b","a533081adb344ecb9ed11ad92435f90a","418c3ec7af8b4b1caad2a879b7a83dca","63c5350f33af45ada71f7a14ba0ba324","d2c8425505a74d30b20f79924fe363aa","f46eda44964640baa28ee568db4b7b42","474f7378c06d413c9b1723473e9f5b41","9bbd95b977ba4c5b8f2dbae61612ac09","1729294c101b4b528bc7dfefdb5d6450","3ea50171e2fd41d8bc6004cef5fbf619"]},"outputId":"1697ec2f-d7e3-43ef-a23a-3574eb634ce5"},"outputs":[{"output_type":"stream","name":"stderr","text":["08/05/2022 13:46:10 - WARNING - datasets.builder -   Reusing dataset scitail (/root/.cache/huggingface/datasets/scitail/tsv_format/1.1.0/0f221f9167f070d3e492b35010923cdf411c288f9ed5b8dc51f0cb011e773ee5)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f13b5142164b4713893b9ad346cac1a6"}},"metadata":{}}],"source":["dataset = load_dataset('scitail', 'tsv_format')"]},{"cell_type":"code","source":["# from torch.utils.data import Subset\n","# dataset['train'] = dataset['train'].shuffle(seed=42)\n","# dataset['train'] = Subset(dataset[\"train\"], list(range(1523)))"],"metadata":{"id":"Um341FdFNH1z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Entailment_Forgettables_Info = pd.read_csv('/content/Entailment_Forgettables_Info.csv')\n","Entailment_Forgettables_Info"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"WScA2d-qNJy5","executionInfo":{"status":"ok","timestamp":1659706571394,"user_tz":-120,"elapsed":6,"user":{"displayName":"AmirHossein Dabiri Aghdam","userId":"03377875757005762455"}},"outputId":"c2426eb9-35fe-481e-bf56-15c402332a98"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      Unnamed: 0  sample_index  forgetting_events   True_label_indexes\n","0              0            29                  1      [2 4 5 6 7 8 9]\n","1              1            33                  1  [0 2 3 4 5 6 7 8 9]\n","2              2            34                  1        [0 5 6 7 8 9]\n","3              3            50                  1    [0 3 4 5 6 7 8 9]\n","4              4            51                  1  [0 1 3 4 5 6 7 8 9]\n","...          ...           ...                ...                  ...\n","1518        1518         23032                  1  [0 1 2 3 4 6 7 8 9]\n","1519        1519         23048                  1  [0 1 3 4 5 6 7 8 9]\n","1520        1520         23067                  1  [0 1 2 3 5 6 7 8 9]\n","1521        1521         23074                  1  [0 1 3 4 5 6 7 8 9]\n","1522        1522         23094                  1  [0 2 3 4 5 6 7 8 9]\n","\n","[1523 rows x 4 columns]"],"text/html":["\n","  <div id=\"df-0dc048d8-f229-49ab-b93a-8a87e6ff04dc\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>sample_index</th>\n","      <th>forgetting_events</th>\n","      <th>True_label_indexes</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>29</td>\n","      <td>1</td>\n","      <td>[2 4 5 6 7 8 9]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>33</td>\n","      <td>1</td>\n","      <td>[0 2 3 4 5 6 7 8 9]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>34</td>\n","      <td>1</td>\n","      <td>[0 5 6 7 8 9]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>50</td>\n","      <td>1</td>\n","      <td>[0 3 4 5 6 7 8 9]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>51</td>\n","      <td>1</td>\n","      <td>[0 1 3 4 5 6 7 8 9]</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1518</th>\n","      <td>1518</td>\n","      <td>23032</td>\n","      <td>1</td>\n","      <td>[0 1 2 3 4 6 7 8 9]</td>\n","    </tr>\n","    <tr>\n","      <th>1519</th>\n","      <td>1519</td>\n","      <td>23048</td>\n","      <td>1</td>\n","      <td>[0 1 3 4 5 6 7 8 9]</td>\n","    </tr>\n","    <tr>\n","      <th>1520</th>\n","      <td>1520</td>\n","      <td>23067</td>\n","      <td>1</td>\n","      <td>[0 1 2 3 5 6 7 8 9]</td>\n","    </tr>\n","    <tr>\n","      <th>1521</th>\n","      <td>1521</td>\n","      <td>23074</td>\n","      <td>1</td>\n","      <td>[0 1 3 4 5 6 7 8 9]</td>\n","    </tr>\n","    <tr>\n","      <th>1522</th>\n","      <td>1522</td>\n","      <td>23094</td>\n","      <td>1</td>\n","      <td>[0 2 3 4 5 6 7 8 9]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1523 rows × 4 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0dc048d8-f229-49ab-b93a-8a87e6ff04dc')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0dc048d8-f229-49ab-b93a-8a87e6ff04dc button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0dc048d8-f229-49ab-b93a-8a87e6ff04dc');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["dataset['train'] = Dataset.from_pandas(pd.DataFrame.from_dict([dataset['train'][idx] for idx in Entailment_Forgettables_Info['sample_index']]))\n","dataset['train']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FFdfZ67KNMxo","executionInfo":{"status":"ok","timestamp":1659706571695,"user_tz":-120,"elapsed":306,"user":{"displayName":"AmirHossein Dabiri Aghdam","userId":"03377875757005762455"}},"outputId":"bdd28014-8140-4178-e586-702ee6344442"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['premise', 'hypothesis', 'label'],\n","    num_rows: 1523\n","})"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":364},"executionInfo":{"elapsed":511,"status":"ok","timestamp":1659706572204,"user":{"displayName":"AmirHossein Dabiri Aghdam","userId":"03377875757005762455"},"user_tz":-120},"id":"cDwxzAdY677O","outputId":"936d3feb-2d6f-41e8-b0da-65ae22f4070a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'entails': 0.4783180026281209, 'neutral': 0.5223390275952694}"]},"metadata":{},"execution_count":8},{"output_type":"display_data","data":{"text/plain":["<Figure size 576x396 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["freq_dict = dict()\n","for data in dataset['train']:\n","    if (data['label'] in freq_dict): freq_dict[data['label']] += 1\n","    else: freq_dict[data['label']] = 1\n","\n","for k,v in freq_dict.items():\n","    freq_dict[k] = v / (len(dataset['train']) - 1)\n","\n","sns.countplot(x=dataset['train']['label'])\n","freq_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":364},"executionInfo":{"elapsed":626,"status":"ok","timestamp":1659706572828,"user":{"displayName":"AmirHossein Dabiri Aghdam","userId":"03377875757005762455"},"user_tz":-120},"id":"WIE7Yr64AEk3","outputId":"6a1b6545-4a4d-406c-dca4-b9c205920e4d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'entails': 0.5038343558282209, 'neutral': 0.49616564417177916}"]},"metadata":{},"execution_count":9},{"output_type":"display_data","data":{"text/plain":["<Figure size 576x396 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["freq_dict = dict()\n","for data in dataset['validation']:\n","    if (data['label'] in freq_dict): freq_dict[data['label']] += 1\n","    else: freq_dict[data['label']] = 1\n","\n","for k,v in freq_dict.items():\n","    freq_dict[k] = v / (len(dataset['validation']))\n","\n","sns.countplot(x=dataset['validation']['label'])\n","freq_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":364},"executionInfo":{"elapsed":649,"status":"ok","timestamp":1659706573474,"user":{"displayName":"AmirHossein Dabiri Aghdam","userId":"03377875757005762455"},"user_tz":-120},"id":"CErTCZLp6lAY","outputId":"833afef0-3360-4af0-ed2f-a385cac0b4cd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'entails': 0.3964218455743879, 'neutral': 0.6045197740112994}"]},"metadata":{},"execution_count":10},{"output_type":"display_data","data":{"text/plain":["<Figure size 576x396 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["freq_dict = dict()\n","for data in dataset['test']:\n","    if (data['label'] in freq_dict): freq_dict[data['label']] += 1\n","    else: freq_dict[data['label']] = 1\n","\n","for k,v in freq_dict.items():\n","    freq_dict[k] = v / (len(dataset['test']) - 2)\n","\n","sns.countplot(x=dataset['test']['label'])\n","freq_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NrJ4LDo2AEoy"},"outputs":[],"source":["class Textual_EntailementClassifier(nn.Module):\n","    def __init__(self, input_dim, num_labels=3, dropout_rate=0.):\n","        super(Textual_EntailementClassifier, self).__init__()\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.linear1 = nn.Linear(input_dim, input_dim // 3)\n","        self.relu = nn.ReLU()\n","        self.linear2 = nn.Linear(input_dim // 3, num_labels)\n","\n","    def forward(self, x):\n","        x = self.dropout(x)\n","        x = self.linear1(x)\n","        x = self.relu(x)\n","        return self.linear2(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qX9IcSy-Yzhk"},"outputs":[],"source":["class BERT_Textual_Entailement(BertModel):\n","    def __init__(self, config, args, label_lst):\n","        super(BERT_Textual_Entailement, self).__init__(config)\n","        self.args = args\n","        self.num_labels = len(set(label_lst))\n","        self.bert = BertModel(config=config) # Load pretrained Bert\n","        self.classifier = Textual_EntailementClassifier(config.hidden_size, self.num_labels, args.dropout_rate)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids, label_ids):\n","        outputs = self.bert(input_ids, attention_mask, token_type_ids)  # sequence_output, pooled_output, (hidden_states), (attentions)\n","        pooled_output = outputs[1]  # [CLS]\n","        label_logits = self.classifier(pooled_output)\n","\n","        total_loss = 0\n","        # Softmax\n","        if label_ids is not None:\n","            loss = 0\n","            if self.num_labels == 1:\n","                loss_fct = nn.MSELoss()\n","                loss = loss_fct(label_logits.view(-1), label_ids.view(-1))\n","            else:\n","                loss_fct = nn.CrossEntropyLoss()\n","                loss = loss_fct(label_logits.view(-1, self.num_labels), label_ids.view(-1))\n","            total_loss += loss\n","\n","        outputs = (label_logits,) + outputs[2:]  # add hidden states and attention if they are here\n","        outputs = (total_loss,) + outputs\n","\n","        return outputs  # (loss), logits, (hidden_states), (attentions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TT9SxAJmDY8_"},"outputs":[],"source":["class ParsBERT_Textual_Entailement(BertModel):\n","    def __init__(self, config, args, label_lst):\n","        super(ParsBERT_Textual_Entailement, self).__init__(config)\n","        self.args = args\n","        self.num_labels = len(set(label_lst))\n","        self.parsbert = BertModel(config=config) # Load pretrained ParsBert\n","        self.classifier = Textual_EntailementClassifier(config.hidden_size, self.num_labels, args.dropout_rate)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids, label_ids):\n","        outputs = self.parsbert(input_ids, attention_mask, token_type_ids)  # sequence_output, pooled_output, (hidden_states), (attentions)\n","        pooled_output = outputs[1]  # [CLS]\n","        label_logits = self.classifier(pooled_output)\n","\n","        total_loss = 0\n","        # Softmax\n","        if label_ids is not None:\n","            loss = 0\n","            if self.num_labels == 1:\n","                loss_fct = nn.MSELoss()\n","                loss = loss_fct(label_logits.view(-1), label_ids.view(-1))\n","            else:\n","                loss_fct = nn.CrossEntropyLoss()\n","                loss = loss_fct(label_logits.view(-1, self.num_labels), label_ids.view(-1))\n","            total_loss += loss\n","\n","        outputs = (label_logits,) + outputs[2:]  # add hidden states and attention if they are here\n","        outputs = (total_loss,) + outputs\n","\n","        return outputs  # (loss), logits, (hidden_states), (attentions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xRlNFn16WA4M"},"outputs":[],"source":["MODEL_CLASSES = {\n","    'bert': (BertConfig, BERT_Textual_Entailement, BertTokenizer),\n","    'parsbert': (BertConfig, ParsBERT_Textual_Entailement, BertTokenizer),\n","}\n","\n","MODEL_PATH_MAP = {\n","    # 'XLMRoberta': 'xlm-roberta-base',\n","    'bert': 'bert-base-uncased',\n","    'parsbert': 'HooshvareLab/bert-fa-zwnj-base',\n","}\n","\n","logger = logging.getLogger(__name__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FGZghD02Yzxv"},"outputs":[],"source":["def load_tokenizer(args):\n","    return MODEL_CLASSES[args.model_type][2].from_pretrained(args.model_name_or_path)\n","\n","def init_logger():\n","    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n","                        datefmt='%m/%d/%Y %H:%M:%S',\n","                        level=logging.INFO)\n","    \n","def label2index(label):\n","    if (label == 'neutral'): return 0\n","    elif (label == 'entails'): return 1\n","    # else: return 2\n","\n","def get_labels(dataset):\n","  train_label = list()\n","  for data in dataset:\n","      if (data['label'] != 'xx' and data['label'] != '-'): train_label.append(data['label'])\n","  \n","  return train_label\n","\n","def set_seed(args):\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    if not args.no_cuda and torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(args.seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dhLxqH9fr4jR"},"outputs":[],"source":["def prepare_dataset(data, args, tokenizer, padding='max_length'):\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    all_label = torch.tensor([label2index(x['label']) for x in data if  (x['label'] != 'xx' and x['label'] != '-')], dtype=torch.long).to(device)\n","    bert_input = tokenizer([x['premise'] for x in data if (x['label'] != 'xx' and x['label'] != '-')] , [x['hypothesis'] for x in data if (x['label'] != 'xx' and x['label'] != '-')], padding=padding, max_length=args.max_seq_len, truncation=True, return_tensors=\"pt\")\n","    if ('token_type_ids' in bert_input):\n","        dataset = TensorDataset(bert_input['input_ids'].to(device), bert_input['attention_mask'].to(device), bert_input['token_type_ids'].to(device), all_label)\n","    else:\n","        dataset = TensorDataset(bert_input['input_ids'].to(device), bert_input['attention_mask'].to(device), torch.zeros_like(bert_input['input_ids']).to(device), all_label)\n","\n","    return dataset"]},{"cell_type":"code","source":["#define\n","OUTPUT_DIM = 2\n","NUM_EPOCHS = 10\n","Prob_per_epoch = np.zeros((NUM_EPOCHS,len(dataset['train']),OUTPUT_DIM)) #2 labels\n","\n","class Trainer(object):\n","    def __init__(self, args, train_dataset=None, dev_dataset=None, test_dataset=None):\n","        self.args = args\n","        self.train_dataset = train_dataset\n","        self.dev_dataset = dev_dataset\n","        self.test_dataset = test_dataset\n","\n","        self.label_lst = get_labels(dataset['train'])\n","\n","        self.config_class, self.model_class, _ = MODEL_CLASSES[args.model_type]\n","        self.config = self.config_class.from_pretrained(args.model_name_or_path, finetuning_task=args.task)\n","        self.model = self.model_class.from_pretrained(args.model_name_or_path,\n","                                                      config=self.config,\n","                                                      args=args,\n","                                                      label_lst=self.label_lst)\n","                                                      \n","\n","        # GPU or CPU\n","        self.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n","        self.model.to(self.device)\n","\n","    def train(self):\n","        # train_sampler = RandomSampler(self.train_dataset)\n","        train_sampler = SequentialSampler(self.train_dataset)\n","        train_dataloader = DataLoader(self.train_dataset, sampler=train_sampler, batch_size=self.args.train_batch_size)\n","        if self.args.max_steps > 0:\n","            t_total = self.args.max_steps\n","            self.args.num_train_epochs = self.args.max_steps // (len(train_dataloader) // self.args.gradient_accumulation_steps) + 1\n","        else:\n","            t_total = len(train_dataloader) // self.args.gradient_accumulation_steps * self.args.num_train_epochs\n","\n","        # Prepare optimizer and schedule (linear warmup and decay)\n","        no_decay = ['bias', 'LayerNorm.weight']\n","        optimizer_grouped_parameters = [\n","            {'params': [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n","             'weight_decay': self.args.weight_decay},\n","            {'params': [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","        ]\n","        optimizer = AdamW(optimizer_grouped_parameters, lr=self.args.learning_rate, eps=self.args.adam_epsilon)\n","        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=t_total)\n","\n","        # Train!\n","        logger.info(\"***** Running training *****\")\n","        logger.info(\"  Num examples = %d\", len(self.train_dataset))\n","        logger.info(\"  Num Epochs = %d\", self.args.num_train_epochs)\n","        logger.info(\"  Total train batch size = %d\", self.args.train_batch_size)\n","        logger.info(\"  Gradient Accumulation steps = %d\", self.args.gradient_accumulation_steps)\n","        logger.info(\"  Total optimization steps = %d\", t_total)\n","        logger.info(\"  Logging steps = %d\", self.args.logging_steps)\n","        logger.info(\"  Save steps = %d\", self.args.save_steps)\n","\n","        global_step = 0\n","        tr_loss = 0.0\n","        self.model.zero_grad()\n","        best_acc = -1\n","       \n","        train_iterator = trange(int(self.args.num_train_epochs), desc=\"Epoch\")\n","        for epoch in train_iterator:\n","            label_preds = None\n","            out_label_ids = None\n","            # epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n","            stacked_train_preds = None\n","            for step, batch in enumerate(train_dataloader):\n","                self.model.train()\n","                inputs = {'input_ids': batch[0],\n","                          'attention_mask': batch[1],\n","                          'token_type_ids': batch[2],\n","                          'label_ids': batch[3]}\n","                outputs = self.model(**inputs)\n","                loss = outputs[0]\n","                label_logits = outputs[1]\n","\n","                # ############## Extracting samples'probability ##############\n","                # # Get the preds\n","                preds = nn.Softmax(dim=1)(label_logits) # convert to probability\n","\n","\n","                # # Move preds to the CPU\n","                train_preds = preds.detach().cpu().numpy()\n","                \n","                if stacked_train_preds is None:  # first batch\n","                    stacked_train_preds = train_preds\n","                else:\n","                  stacked_train_preds = np.vstack((stacked_train_preds, train_preds))\n","                \n","                # #######################  END ############################\n","\n","                 # label prediction\n","                if label_preds is None:\n","                    label_preds = label_logits.detach().cpu().numpy()\n","                    out_label_ids = inputs['label_ids'].detach().cpu().numpy()\n","                else:\n","                    label_preds = np.append(label_preds, label_logits.detach().cpu().numpy(), axis=0)\n","                    out_label_ids = np.append(out_label_ids, inputs['label_ids'].detach().cpu().numpy(), axis=0)\n","                \n","                if self.args.gradient_accumulation_steps > 1:\n","                    loss = loss / self.args.gradient_accumulation_steps\n","\n","                loss.backward()\n","\n","                tr_loss += loss.item()\n","                if (step + 1) % self.args.gradient_accumulation_steps == 0:\n","                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.max_grad_norm)\n","\n","                    optimizer.step()\n","                    scheduler.step()  # Update learning rate schedule\n","                    self.model.zero_grad()\n","                    global_step += 1\n","\n","                    if(global_step % 10 == 0): logger.info(\"Train loss = %.4f\", tr_loss / global_step)\n","                    if (self.args.logging_steps > 0 and global_step % self.args.logging_steps == 0):\n","                        results = self.evaluate(\"dev\")\n","                        if (float(results['accuracy']) > best_acc):\n","                        # if (float(results['slot_f1']) > best_f1):\n","                        #   best_f1 = float(results['slot_f1'])\n","                          best_acc = float(results['accuracy'])\n","                          logger.info(\"dev best_acc = %.4f\", best_acc * 100.0)\n","                          self.save_model()\n","\n","                   \n","            Prob_per_epoch[epoch][:,:] = stacked_train_preds\n","            \n","            if 0 < self.args.max_steps < global_step:\n","                train_iterator.close()\n","                break\n","            \n","            # label result\n","            label_preds = np.argmax(label_preds, axis=1)\n","            logger.info(\"train_acc = %.4f\", accuracy_score(out_label_ids, label_preds) * 100.0)\n","\n","\n","        return global_step, tr_loss / global_step\n","\n","    def evaluate(self, mode):\n","        if mode == 'test':\n","            dataset = self.test_dataset\n","        elif mode == 'dev':\n","            dataset = self.dev_dataset\n","        else:\n","            raise Exception(\"Only dev and test dataset available\")\n","\n","        eval_sampler = SequentialSampler(dataset)\n","        eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=self.args.eval_batch_size)\n","\n","        # Eval!\n","        logger.info(\"\\n***** Running evaluation on %s dataset *****\", mode)\n","        logger.info(\"  Num examples = %d\", len(dataset))\n","        logger.info(\"  Batch size = %d\", self.args.eval_batch_size)\n","        eval_loss = 0.0\n","        nb_eval_steps = 0\n","        label_preds = None\n","        out_label_ids = None\n","\n","        self.model.eval()\n","\n","        # for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","        for batch in eval_dataloader:\n","            batch = tuple(t.to(self.device) for t in batch)\n","            with torch.no_grad():\n","                inputs = {'input_ids': batch[0],\n","                          'attention_mask': batch[1],\n","                          'token_type_ids': batch[2],\n","                          'label_ids': batch[3]}\n","                outputs = self.model(**inputs)\n","                tmp_eval_loss, label_logits = outputs[:2]\n","                eval_loss += tmp_eval_loss.mean().item()\n","            \n","            nb_eval_steps += 1\n","\n","            # label prediction\n","            if label_preds is None:\n","                label_preds = label_logits.detach().cpu().numpy()\n","                out_label_ids = inputs['label_ids'].detach().cpu().numpy()\n","            else:\n","                label_preds = np.append(label_preds, label_logits.detach().cpu().numpy(), axis=0)\n","                out_label_ids = np.append(out_label_ids, inputs['label_ids'].detach().cpu().numpy(), axis=0)\n","\n","\n","        eval_loss = eval_loss / nb_eval_steps\n","        results = {\n","            \"loss\": eval_loss\n","        }\n","\n","        # label result\n","        label_preds = np.argmax(label_preds, axis=1)\n","        print(set(label_preds))\n","        total_result = accuracy_score(out_label_ids, label_preds)\n","        results.update({'accuracy' : total_result})\n","        logger.info(\"***** Eval results *****\")\n","        for key in sorted(results.keys()):\n","            logger.info(\"  %s = %.4f\", key if key != 'loss' else 'loss', float(results[key]) * 100.0 if key != 'loss' else float(results[key]))\n","\n","        if (mode == 'test'): print(classification_report(out_label_ids, label_preds, target_names=['neutral', 'entails'], digits = 4))\n","        return results\n","\n","    def save_model(self):\n","        # Save model checkpoint (Overwrite)\n","        if not os.path.exists(self.args.model_dir):\n","            os.makedirs(self.args.model_dir)\n","        model_to_save = self.model.module if hasattr(self.model, 'module') else self.model\n","        model_to_save.save_pretrained(self.args.model_dir)\n","\n","\n","    def load_model(self):\n","        # Check whether model exists\n","        if not os.path.exists(self.args.model_dir):\n","            raise Exception(\"Model doesn't exists! Train first!\")\n","\n","        try:\n","            self.model = self.model_class.from_pretrained(self.args.model_dir,\n","                                                          args=self.args,\n","                                                          label_lst=self.label_lst)\n","            self.model.to(self.device)\n","            logger.info(\"***** Model Loaded *****\")\n","        except:\n","            raise Exception(\"Some model files might be missing...\")\n"],"metadata":{"id":"q4AwhuNCSsEG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ydHioDjUJMeA"},"outputs":[],"source":["class dotdict(dict):\n","    \"\"\"dot.notation access to dictionary attributes\"\"\"\n","    __getattr__ = dict.get\n","    __setattr__ = dict.__setitem__\n","    __delattr__ = dict.__delitem__"]},{"cell_type":"markdown","metadata":{"id":"ErWVnaL9ahyP"},"source":["Using bert\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YMs8BTqnJOEf"},"outputs":[],"source":["args = dotdict(dict())\n","args.seed = 810197502\n","args.model_type = 'bert' \n","args.model_name_or_path = MODEL_PATH_MAP[args.model_type]\n","args.dropout_rate =  0.3\n","args.do_train = True\n","args.do_eval = True\n","args.train_batch_size = 32 \n","args.max_steps = -1\n","args.task = 'Textual_Entailement' \n","args.no_cuda = False\n","args.weight_decay = 0\n","args.num_train_epochs = NUM_EPOCHS\n","args.gradient_accumulation_steps = 1\n","args.learning_rate = 3e-5 \n","args.adam_epsilon = 1e-8\n","args.warmup_steps = 0 \n","args.logging_steps = 200\n","args.save_steps = 500\n","args.max_grad_norm = 1\n","args.eval_batch_size = 64\n","args.model_dir = \"Textual_Entailement_model_\" + args.model_type\n","args.max_seq_len = 128\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KeuvpC-ZNMRD","colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["8dd70cf61b354f519a6153319cb68a6f","a29add93b522478da7de3a994dadfa2f","41578e2ce36c4630a4a88b942b827b42","4269806b0df949c3980f7097af8bf45f","1ae20b9019834d3c8fe3ccf5b48f434f","614183020766488b859d63e022e57d07","94d704149b304961b4a0591d9a29a93d","7563fdaf2be8448eaeda8101281ee837","8a1b3386f2b7451ca5e07592921fb17a","e4ecbb0ec3a745e3b12efe4b23e68a02","b79e83ef8d5347e3b8ef20c9afd94bcd","5de6c0fbcc89499b8f8c77784d6027a6","fd003972d64749b7a62dfecd5b8bc009","840ffebd01c84cfb8e43ca339943384b","7e5d7c75991d490084a95254ea2762cc","911031d910d44a339e22a1ac8f168f3b","5373fcb4de7945968134dc3dacf03139","7a523a3f056c4dd8b3ffafc0155d248b","9e9bf8ee0eee4f6f9eae66c926ca375b","50be468b69d34abba5ea86b4033bfe22","45e5489c15a24240a08978da3eee5305","fa26fcfdd13547a8ad278aaf186947ca","c5a2fbce0a5747789d28ac77c7d5ac14","778bea3bf5544c119eb1dd563f364670","856d05e40cdf415cbdea7d449ae19bca","c5444b6fc37f4a828a7070a6cf0fede1","b7a479ccc3d1490d99e4b89f69312ea8","4219c848ee074e72a5798c058449d3d3","1c2ffa89b0684b2793455ff9bf7e1b90","8bf4c38aae384c0eac0ae1e35724cea4","2ffe6a7231d9456faf199478f1f4cf9b","d2bbe00cf48b4f48a9b0bdb6963dcde2","9f6456a9ae4c4e5ca42405d2b5e7f363"]},"executionInfo":{"status":"ok","timestamp":1659706576098,"user_tz":-120,"elapsed":1643,"user":{"displayName":"AmirHossein Dabiri Aghdam","userId":"03377875757005762455"}},"outputId":"c932eb11-7bb2-46cd-f3bf-e8a79a2144fd"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dd70cf61b354f519a6153319cb68a6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5de6c0fbcc89499b8f8c77784d6027a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5a2fbce0a5747789d28ac77c7d5ac14"}},"metadata":{}}],"source":["init_logger()\n","set_seed(args)\n","tokenizer = load_tokenizer(args)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RPUnSZlUq9kg"},"outputs":[],"source":["train_dataset = prepare_dataset(dataset['train'], args, tokenizer)\n","valid_dataset = prepare_dataset(dataset['validation'], args, tokenizer)\n","test_dataset = prepare_dataset(dataset['test'], args, tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3488,"status":"ok","timestamp":1659707369183,"user":{"displayName":"AmirHossein Dabiri Aghdam","userId":"03377875757005762455"},"user_tz":-120},"id":"AkmMPQ4ec7rw","outputId":"1d63c378-9688-4092-9e08-2ba1cdba007c"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BERT_Textual_Entailement: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BERT_Textual_Entailement from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BERT_Textual_Entailement from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BERT_Textual_Entailement were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['pooler.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.7.output.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'classifier.linear1.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.3.attention.self.value.weight', 'classifier.linear1.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.8.output.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.weight', 'classifier.linear2.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'classifier.linear2.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.1.attention.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["trainer = Trainer(args, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XoTrRNhZxRDU"},"outputs":[],"source":["gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"code","source":["#random\n","if args.do_train:\n","    # trainer.load_model()\n","    trainer.train()\n","\n","if args.do_eval:\n","    # trainer.load_model()\n","    trainer.evaluate(\"test\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lWTK330gRMWD","executionInfo":{"status":"ok","timestamp":1659707733827,"user_tz":-120,"elapsed":355233,"user":{"displayName":"AmirHossein Dabiri Aghdam","userId":"03377875757005762455"}},"outputId":"ded43792-3daf-4f42-d343-795c95280ab3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","08/05/2022 13:49:38 - INFO - __main__ -   ***** Running training *****\n","08/05/2022 13:49:38 - INFO - __main__ -     Num examples = 1523\n","08/05/2022 13:49:38 - INFO - __main__ -     Num Epochs = 10\n","08/05/2022 13:49:38 - INFO - __main__ -     Total train batch size = 32\n","08/05/2022 13:49:38 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/05/2022 13:49:38 - INFO - __main__ -     Total optimization steps = 480\n","08/05/2022 13:49:38 - INFO - __main__ -     Logging steps = 200\n","08/05/2022 13:49:38 - INFO - __main__ -     Save steps = 500\n","Epoch:   0%|          | 0/10 [00:00<?, ?it/s]08/05/2022 13:49:44 - INFO - __main__ -   Train loss = 0.6765\n","08/05/2022 13:49:51 - INFO - __main__ -   Train loss = 0.6531\n","08/05/2022 13:49:57 - INFO - __main__ -   Train loss = 0.6274\n","08/05/2022 13:50:03 - INFO - __main__ -   Train loss = 0.6012\n","08/05/2022 13:50:08 - INFO - __main__ -   train_acc = 69.7965\n","Epoch:  10%|█         | 1/10 [00:30<04:34, 30.52s/it]08/05/2022 13:50:10 - INFO - __main__ -   Train loss = 0.5768\n","08/05/2022 13:50:16 - INFO - __main__ -   Train loss = 0.5454\n","08/05/2022 13:50:23 - INFO - __main__ -   Train loss = 0.5149\n","08/05/2022 13:50:29 - INFO - __main__ -   Train loss = 0.4805\n","08/05/2022 13:50:36 - INFO - __main__ -   Train loss = 0.4540\n","08/05/2022 13:50:40 - INFO - __main__ -   train_acc = 91.9238\n","Epoch:  20%|██        | 2/10 [01:02<04:09, 31.20s/it]08/05/2022 13:50:43 - INFO - __main__ -   Train loss = 0.4266\n","08/05/2022 13:50:50 - INFO - __main__ -   Train loss = 0.4027\n","08/05/2022 13:50:56 - INFO - __main__ -   Train loss = 0.3809\n","08/05/2022 13:51:03 - INFO - __main__ -   Train loss = 0.3583\n","08/05/2022 13:51:09 - INFO - __main__ -   Train loss = 0.3404\n","08/05/2022 13:51:12 - INFO - __main__ -   train_acc = 96.9140\n","Epoch:  30%|███       | 3/10 [01:33<03:40, 31.46s/it]08/05/2022 13:51:16 - INFO - __main__ -   Train loss = 0.3242\n","08/05/2022 13:51:22 - INFO - __main__ -   Train loss = 0.3097\n","08/05/2022 13:51:29 - INFO - __main__ -   Train loss = 0.2944\n","08/05/2022 13:51:35 - INFO - __main__ -   Train loss = 0.2803\n","08/05/2022 13:51:42 - INFO - __main__ -   Train loss = 0.2687\n","08/05/2022 13:51:43 - INFO - __main__ -   train_acc = 98.5555\n","Epoch:  40%|████      | 4/10 [02:05<03:08, 31.38s/it]08/05/2022 13:51:48 - INFO - __main__ -   Train loss = 0.2563\n","08/05/2022 13:51:48 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 13:51:48 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 13:51:48 - INFO - __main__ -     Batch size = 64\n","08/05/2022 13:51:58 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 13:51:58 - INFO - __main__ -     accuracy = 88.7270\n","08/05/2022 13:51:58 - INFO - __main__ -     loss = 0.3694\n","08/05/2022 13:51:58 - INFO - __main__ -   dev best_acc = 88.7270\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 13:52:08 - INFO - __main__ -   Train loss = 0.2454\n","08/05/2022 13:52:14 - INFO - __main__ -   Train loss = 0.2353\n","08/05/2022 13:52:21 - INFO - __main__ -   Train loss = 0.2262\n","08/05/2022 13:52:27 - INFO - __main__ -   Train loss = 0.2172\n","08/05/2022 13:52:27 - INFO - __main__ -   train_acc = 99.6060\n","Epoch:  50%|█████     | 5/10 [02:49<02:59, 35.99s/it]08/05/2022 13:52:34 - INFO - __main__ -   Train loss = 0.2089\n","08/05/2022 13:52:41 - INFO - __main__ -   Train loss = 0.2013\n","08/05/2022 13:52:47 - INFO - __main__ -   Train loss = 0.1941\n","08/05/2022 13:52:54 - INFO - __main__ -   Train loss = 0.1877\n","08/05/2022 13:52:59 - INFO - __main__ -   train_acc = 99.8030\n","Epoch:  60%|██████    | 6/10 [03:20<02:17, 34.50s/it]08/05/2022 13:53:00 - INFO - __main__ -   Train loss = 0.1815\n","08/05/2022 13:53:07 - INFO - __main__ -   Train loss = 0.1758\n","08/05/2022 13:53:13 - INFO - __main__ -   Train loss = 0.1703\n","08/05/2022 13:53:20 - INFO - __main__ -   Train loss = 0.1651\n","08/05/2022 13:53:27 - INFO - __main__ -   Train loss = 0.1604\n","08/05/2022 13:53:30 - INFO - __main__ -   train_acc = 99.8687\n","Epoch:  70%|███████   | 7/10 [03:52<01:40, 33.46s/it]08/05/2022 13:53:33 - INFO - __main__ -   Train loss = 0.1558\n","08/05/2022 13:53:39 - INFO - __main__ -   Train loss = 0.1518\n","08/05/2022 13:53:46 - INFO - __main__ -   Train loss = 0.1477\n","08/05/2022 13:53:53 - INFO - __main__ -   Train loss = 0.1438\n","08/05/2022 13:53:59 - INFO - __main__ -   Train loss = 0.1402\n","08/05/2022 13:54:02 - INFO - __main__ -   train_acc = 99.8030\n","Epoch:  80%|████████  | 8/10 [04:23<01:05, 32.79s/it]08/05/2022 13:54:06 - INFO - __main__ -   Train loss = 0.1367\n","08/05/2022 13:54:12 - INFO - __main__ -   Train loss = 0.1335\n","08/05/2022 13:54:12 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 13:54:12 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 13:54:12 - INFO - __main__ -     Batch size = 64\n","08/05/2022 13:54:22 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 13:54:22 - INFO - __main__ -     accuracy = 90.7209\n","08/05/2022 13:54:22 - INFO - __main__ -     loss = 0.4396\n","08/05/2022 13:54:22 - INFO - __main__ -   dev best_acc = 90.7209\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 13:54:32 - INFO - __main__ -   Train loss = 0.1303\n","08/05/2022 13:54:38 - INFO - __main__ -   Train loss = 0.1275\n","08/05/2022 13:54:45 - INFO - __main__ -   Train loss = 0.1247\n","08/05/2022 13:54:46 - INFO - __main__ -   train_acc = 99.8030\n","Epoch:  90%|█████████ | 9/10 [05:07<00:36, 36.37s/it]08/05/2022 13:54:51 - INFO - __main__ -   Train loss = 0.1220\n","08/05/2022 13:54:58 - INFO - __main__ -   Train loss = 0.1194\n","08/05/2022 13:55:04 - INFO - __main__ -   Train loss = 0.1172\n","08/05/2022 13:55:11 - INFO - __main__ -   Train loss = 0.1149\n","08/05/2022 13:55:17 - INFO - __main__ -   Train loss = 0.1125\n","08/05/2022 13:55:17 - INFO - __main__ -   train_acc = 99.8030\n","Epoch: 100%|██████████| 10/10 [05:39<00:00, 33.93s/it]\n","08/05/2022 13:55:17 - INFO - __main__ -   \n","***** Running evaluation on test dataset *****\n","08/05/2022 13:55:17 - INFO - __main__ -     Num examples = 2126\n","08/05/2022 13:55:17 - INFO - __main__ -     Batch size = 64\n","08/05/2022 13:55:33 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 13:55:33 - INFO - __main__ -     accuracy = 88.8053\n","08/05/2022 13:55:33 - INFO - __main__ -     loss = 0.5249\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n","              precision    recall  f1-score   support\n","\n","     neutral     0.9105    0.9034    0.9070      1284\n","     entails     0.8545    0.8646    0.8595       842\n","\n","    accuracy                         0.8881      2126\n","   macro avg     0.8825    0.8840    0.8832      2126\n","weighted avg     0.8883    0.8881    0.8882      2126\n","\n"]}]},{"cell_type":"code","source":["#forgetable\n","if args.do_train:\n","    # trainer.load_model()\n","    trainer.train()\n","\n","if args.do_eval:\n","    # trainer.load_model()\n","    trainer.evaluate(\"test\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cipzpN81Ne23","executionInfo":{"status":"ok","timestamp":1659706957114,"user_tz":-120,"elapsed":353990,"user":{"displayName":"AmirHossein Dabiri Aghdam","userId":"03377875757005762455"}},"outputId":"2019dc03-5766-43c9-b00a-bd7bcf2837ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","08/05/2022 13:36:42 - INFO - __main__ -   ***** Running training *****\n","08/05/2022 13:36:42 - INFO - __main__ -     Num examples = 1523\n","08/05/2022 13:36:42 - INFO - __main__ -     Num Epochs = 10\n","08/05/2022 13:36:42 - INFO - __main__ -     Total train batch size = 32\n","08/05/2022 13:36:42 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/05/2022 13:36:42 - INFO - __main__ -     Total optimization steps = 480\n","08/05/2022 13:36:42 - INFO - __main__ -     Logging steps = 200\n","08/05/2022 13:36:42 - INFO - __main__ -     Save steps = 500\n","Epoch:   0%|          | 0/10 [00:00<?, ?it/s]08/05/2022 13:36:51 - INFO - __main__ -   Train loss = 0.6748\n","08/05/2022 13:36:57 - INFO - __main__ -   Train loss = 0.6960\n","08/05/2022 13:37:03 - INFO - __main__ -   Train loss = 0.6983\n","08/05/2022 13:37:09 - INFO - __main__ -   Train loss = 0.6980\n","08/05/2022 13:37:13 - INFO - __main__ -   train_acc = 51.1490\n","Epoch:  10%|█         | 1/10 [00:31<04:40, 31.18s/it]08/05/2022 13:37:15 - INFO - __main__ -   Train loss = 0.6996\n","08/05/2022 13:37:21 - INFO - __main__ -   Train loss = 0.6985\n","08/05/2022 13:37:27 - INFO - __main__ -   Train loss = 0.6979\n","08/05/2022 13:37:34 - INFO - __main__ -   Train loss = 0.6988\n","08/05/2022 13:37:40 - INFO - __main__ -   Train loss = 0.6983\n","08/05/2022 13:37:44 - INFO - __main__ -   train_acc = 48.9823\n","Epoch:  20%|██        | 2/10 [01:01<04:05, 30.64s/it]08/05/2022 13:37:46 - INFO - __main__ -   Train loss = 0.6973\n","08/05/2022 13:37:53 - INFO - __main__ -   Train loss = 0.6966\n","08/05/2022 13:37:59 - INFO - __main__ -   Train loss = 0.6971\n","08/05/2022 13:38:06 - INFO - __main__ -   Train loss = 0.6972\n","08/05/2022 13:38:12 - INFO - __main__ -   Train loss = 0.6969\n","08/05/2022 13:38:15 - INFO - __main__ -   train_acc = 51.9370\n","Epoch:  30%|███       | 3/10 [01:32<03:36, 30.87s/it]08/05/2022 13:38:19 - INFO - __main__ -   Train loss = 0.6966\n","08/05/2022 13:38:26 - INFO - __main__ -   Train loss = 0.6964\n","08/05/2022 13:38:32 - INFO - __main__ -   Train loss = 0.6955\n","08/05/2022 13:38:39 - INFO - __main__ -   Train loss = 0.6944\n","08/05/2022 13:38:46 - INFO - __main__ -   Train loss = 0.6933\n","08/05/2022 13:38:47 - INFO - __main__ -   train_acc = 56.9271\n","Epoch:  40%|████      | 4/10 [02:04<03:07, 31.22s/it]08/05/2022 13:38:52 - INFO - __main__ -   Train loss = 0.6915\n","08/05/2022 13:38:52 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 13:38:52 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 13:38:52 - INFO - __main__ -     Batch size = 64\n","08/05/2022 13:39:01 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 13:39:01 - INFO - __main__ -     accuracy = 55.8282\n","08/05/2022 13:39:01 - INFO - __main__ -     loss = 0.6815\n","08/05/2022 13:39:01 - INFO - __main__ -   dev best_acc = 55.8282\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 13:39:11 - INFO - __main__ -   Train loss = 0.6897\n","08/05/2022 13:39:18 - INFO - __main__ -   Train loss = 0.6869\n","08/05/2022 13:39:24 - INFO - __main__ -   Train loss = 0.6827\n","08/05/2022 13:39:30 - INFO - __main__ -   Train loss = 0.6789\n","08/05/2022 13:39:30 - INFO - __main__ -   train_acc = 69.9278\n","Epoch:  50%|█████     | 5/10 [02:48<02:58, 35.77s/it]08/05/2022 13:39:37 - INFO - __main__ -   Train loss = 0.6734\n","08/05/2022 13:39:44 - INFO - __main__ -   Train loss = 0.6682\n","08/05/2022 13:39:50 - INFO - __main__ -   Train loss = 0.6618\n","08/05/2022 13:39:57 - INFO - __main__ -   Train loss = 0.6555\n","08/05/2022 13:40:02 - INFO - __main__ -   train_acc = 79.6454\n","Epoch:  60%|██████    | 6/10 [03:19<02:17, 34.37s/it]08/05/2022 13:40:03 - INFO - __main__ -   Train loss = 0.6486\n","08/05/2022 13:40:10 - INFO - __main__ -   Train loss = 0.6421\n","08/05/2022 13:40:17 - INFO - __main__ -   Train loss = 0.6365\n","08/05/2022 13:40:23 - INFO - __main__ -   Train loss = 0.6277\n","08/05/2022 13:40:30 - INFO - __main__ -   Train loss = 0.6214\n","08/05/2022 13:40:34 - INFO - __main__ -   train_acc = 84.3729\n","Epoch:  70%|███████   | 7/10 [03:51<01:40, 33.40s/it]08/05/2022 13:40:36 - INFO - __main__ -   Train loss = 0.6139\n","08/05/2022 13:40:43 - INFO - __main__ -   Train loss = 0.6093\n","08/05/2022 13:40:49 - INFO - __main__ -   Train loss = 0.6011\n","08/05/2022 13:40:56 - INFO - __main__ -   Train loss = 0.5932\n","08/05/2022 13:41:02 - INFO - __main__ -   Train loss = 0.5858\n","08/05/2022 13:41:05 - INFO - __main__ -   train_acc = 88.9691\n","Epoch:  80%|████████  | 8/10 [04:22<01:05, 32.74s/it]08/05/2022 13:41:09 - INFO - __main__ -   Train loss = 0.5785\n","08/05/2022 13:41:15 - INFO - __main__ -   Train loss = 0.5741\n","08/05/2022 13:41:15 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 13:41:15 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 13:41:15 - INFO - __main__ -     Batch size = 64\n","08/05/2022 13:41:25 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 13:41:25 - INFO - __main__ -     accuracy = 66.9479\n","08/05/2022 13:41:25 - INFO - __main__ -     loss = 0.7457\n","08/05/2022 13:41:25 - INFO - __main__ -   dev best_acc = 66.9479\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 13:41:35 - INFO - __main__ -   Train loss = 0.5692\n","08/05/2022 13:41:41 - INFO - __main__ -   Train loss = 0.5618\n","08/05/2022 13:41:48 - INFO - __main__ -   Train loss = 0.5553\n","08/05/2022 13:41:49 - INFO - __main__ -   train_acc = 89.9540\n","Epoch:  90%|█████████ | 9/10 [05:06<00:36, 36.30s/it]08/05/2022 13:41:54 - INFO - __main__ -   Train loss = 0.5493\n","08/05/2022 13:42:01 - INFO - __main__ -   Train loss = 0.5454\n","08/05/2022 13:42:08 - INFO - __main__ -   Train loss = 0.5402\n","08/05/2022 13:42:14 - INFO - __main__ -   Train loss = 0.5355\n","08/05/2022 13:42:20 - INFO - __main__ -   Train loss = 0.5298\n","08/05/2022 13:42:20 - INFO - __main__ -   train_acc = 89.8227\n","Epoch: 100%|██████████| 10/10 [05:38<00:00, 33.82s/it]\n","08/05/2022 13:42:21 - INFO - __main__ -   \n","***** Running evaluation on test dataset *****\n","08/05/2022 13:42:21 - INFO - __main__ -     Num examples = 2126\n","08/05/2022 13:42:21 - INFO - __main__ -     Batch size = 64\n","08/05/2022 13:42:36 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 13:42:36 - INFO - __main__ -     accuracy = 76.9991\n","08/05/2022 13:42:36 - INFO - __main__ -     loss = 0.5208\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n","              precision    recall  f1-score   support\n","\n","     neutral     0.8142    0.8022    0.8082      1284\n","     entails     0.7050    0.7209    0.7129       842\n","\n","    accuracy                         0.7700      2126\n","   macro avg     0.7596    0.7615    0.7605      2126\n","weighted avg     0.7710    0.7700    0.7704      2126\n","\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7xNxHo9NNbuK","outputId":"8b18b396-3f78-4a86-f15e-fec49486fe33","executionInfo":{"status":"ok","timestamp":1659696586061,"user_tz":-120,"elapsed":5224932,"user":{"displayName":"AmirHossein Dabiri Aghdam","userId":"03377875757005762455"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","08/05/2022 09:22:40 - INFO - __main__ -   ***** Running training *****\n","08/05/2022 09:22:40 - INFO - __main__ -     Num examples = 23097\n","08/05/2022 09:22:40 - INFO - __main__ -     Num Epochs = 10\n","08/05/2022 09:22:40 - INFO - __main__ -     Total train batch size = 32\n","08/05/2022 09:22:40 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/05/2022 09:22:40 - INFO - __main__ -     Total optimization steps = 7220\n","08/05/2022 09:22:40 - INFO - __main__ -     Logging steps = 200\n","08/05/2022 09:22:40 - INFO - __main__ -     Save steps = 500\n","Epoch:   0%|          | 0/10 [00:00<?, ?it/s]08/05/2022 09:22:50 - INFO - __main__ -   Train loss = 0.6394\n","08/05/2022 09:22:56 - INFO - __main__ -   Train loss = 0.6090\n","08/05/2022 09:23:02 - INFO - __main__ -   Train loss = 0.6086\n","08/05/2022 09:23:08 - INFO - __main__ -   Train loss = 0.6018\n","08/05/2022 09:23:14 - INFO - __main__ -   Train loss = 0.5924\n","08/05/2022 09:23:21 - INFO - __main__ -   Train loss = 0.5888\n","08/05/2022 09:23:27 - INFO - __main__ -   Train loss = 0.5850\n","08/05/2022 09:23:33 - INFO - __main__ -   Train loss = 0.5965\n","08/05/2022 09:23:40 - INFO - __main__ -   Train loss = 0.5748\n","08/05/2022 09:23:46 - INFO - __main__ -   Train loss = 0.5943\n","08/05/2022 09:23:53 - INFO - __main__ -   Train loss = 0.5944\n","08/05/2022 09:24:00 - INFO - __main__ -   Train loss = 0.5870\n","08/05/2022 09:24:07 - INFO - __main__ -   Train loss = 0.5733\n","08/05/2022 09:24:14 - INFO - __main__ -   Train loss = 0.5762\n","08/05/2022 09:24:20 - INFO - __main__ -   Train loss = 0.5666\n","08/05/2022 09:24:27 - INFO - __main__ -   Train loss = 0.5590\n","08/05/2022 09:24:34 - INFO - __main__ -   Train loss = 0.5582\n","08/05/2022 09:24:41 - INFO - __main__ -   Train loss = 0.5561\n","08/05/2022 09:24:47 - INFO - __main__ -   Train loss = 0.5527\n","08/05/2022 09:24:54 - INFO - __main__ -   Train loss = 0.5461\n","08/05/2022 09:24:54 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 09:24:54 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 09:24:54 - INFO - __main__ -     Batch size = 64\n","08/05/2022 09:25:04 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 09:25:04 - INFO - __main__ -     accuracy = 86.0429\n","08/05/2022 09:25:04 - INFO - __main__ -     loss = 0.3496\n","08/05/2022 09:25:04 - INFO - __main__ -   dev best_acc = 86.0429\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 09:25:14 - INFO - __main__ -   Train loss = 0.5354\n","08/05/2022 09:25:20 - INFO - __main__ -   Train loss = 0.5292\n","08/05/2022 09:25:27 - INFO - __main__ -   Train loss = 0.5240\n","08/05/2022 09:25:34 - INFO - __main__ -   Train loss = 0.5169\n","08/05/2022 09:25:41 - INFO - __main__ -   Train loss = 0.5077\n","08/05/2022 09:25:47 - INFO - __main__ -   Train loss = 0.4998\n","08/05/2022 09:25:54 - INFO - __main__ -   Train loss = 0.4954\n","08/05/2022 09:26:01 - INFO - __main__ -   Train loss = 0.4889\n","08/05/2022 09:26:07 - INFO - __main__ -   Train loss = 0.4841\n","08/05/2022 09:26:14 - INFO - __main__ -   Train loss = 0.4772\n","08/05/2022 09:26:21 - INFO - __main__ -   Train loss = 0.4676\n","08/05/2022 09:26:28 - INFO - __main__ -   Train loss = 0.4668\n","08/05/2022 09:26:34 - INFO - __main__ -   Train loss = 0.4652\n","08/05/2022 09:26:41 - INFO - __main__ -   Train loss = 0.4614\n","08/05/2022 09:26:48 - INFO - __main__ -   Train loss = 0.4573\n","08/05/2022 09:26:54 - INFO - __main__ -   Train loss = 0.4507\n","08/05/2022 09:27:01 - INFO - __main__ -   Train loss = 0.4479\n","08/05/2022 09:27:08 - INFO - __main__ -   Train loss = 0.4424\n","08/05/2022 09:27:14 - INFO - __main__ -   Train loss = 0.4369\n","08/05/2022 09:27:21 - INFO - __main__ -   Train loss = 0.4337\n","08/05/2022 09:27:21 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 09:27:21 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 09:27:21 - INFO - __main__ -     Batch size = 64\n","08/05/2022 09:27:31 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 09:27:31 - INFO - __main__ -     accuracy = 92.0245\n","08/05/2022 09:27:31 - INFO - __main__ -     loss = 0.2345\n","08/05/2022 09:27:31 - INFO - __main__ -   dev best_acc = 92.0245\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 09:27:41 - INFO - __main__ -   Train loss = 0.4302\n","08/05/2022 09:27:48 - INFO - __main__ -   Train loss = 0.4268\n","08/05/2022 09:27:54 - INFO - __main__ -   Train loss = 0.4255\n","08/05/2022 09:28:01 - INFO - __main__ -   Train loss = 0.4216\n","08/05/2022 09:28:08 - INFO - __main__ -   Train loss = 0.4172\n","08/05/2022 09:28:14 - INFO - __main__ -   Train loss = 0.4126\n","08/05/2022 09:28:21 - INFO - __main__ -   Train loss = 0.4082\n","08/05/2022 09:28:28 - INFO - __main__ -   Train loss = 0.4063\n","08/05/2022 09:28:35 - INFO - __main__ -   Train loss = 0.4033\n","08/05/2022 09:28:41 - INFO - __main__ -   Train loss = 0.4003\n","08/05/2022 09:28:48 - INFO - __main__ -   Train loss = 0.3963\n","08/05/2022 09:28:55 - INFO - __main__ -   Train loss = 0.3926\n","08/05/2022 09:29:01 - INFO - __main__ -   Train loss = 0.3894\n","08/05/2022 09:29:08 - INFO - __main__ -   Train loss = 0.3851\n","08/05/2022 09:29:15 - INFO - __main__ -   Train loss = 0.3828\n","08/05/2022 09:29:22 - INFO - __main__ -   Train loss = 0.3814\n","08/05/2022 09:29:28 - INFO - __main__ -   Train loss = 0.3796\n","08/05/2022 09:29:35 - INFO - __main__ -   Train loss = 0.3771\n","08/05/2022 09:29:42 - INFO - __main__ -   Train loss = 0.3752\n","08/05/2022 09:29:48 - INFO - __main__ -   Train loss = 0.3747\n","08/05/2022 09:29:48 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 09:29:48 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 09:29:48 - INFO - __main__ -     Batch size = 64\n","08/05/2022 09:29:58 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 09:29:58 - INFO - __main__ -     accuracy = 91.1810\n","08/05/2022 09:29:58 - INFO - __main__ -     loss = 0.2300\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 09:30:05 - INFO - __main__ -   Train loss = 0.3718\n","08/05/2022 09:30:12 - INFO - __main__ -   Train loss = 0.3702\n","08/05/2022 09:30:18 - INFO - __main__ -   Train loss = 0.3678\n","08/05/2022 09:30:25 - INFO - __main__ -   Train loss = 0.3650\n","08/05/2022 09:30:32 - INFO - __main__ -   Train loss = 0.3624\n","08/05/2022 09:30:38 - INFO - __main__ -   Train loss = 0.3600\n","08/05/2022 09:30:45 - INFO - __main__ -   Train loss = 0.3592\n","08/05/2022 09:30:52 - INFO - __main__ -   Train loss = 0.3567\n","08/05/2022 09:30:58 - INFO - __main__ -   Train loss = 0.3552\n","08/05/2022 09:31:05 - INFO - __main__ -   Train loss = 0.3525\n","08/05/2022 09:31:12 - INFO - __main__ -   Train loss = 0.3529\n","08/05/2022 09:31:19 - INFO - __main__ -   Train loss = 0.3506\n","08/05/2022 09:31:20 - INFO - __main__ -   train_acc = 84.9244\n","Epoch:  10%|█         | 1/10 [08:39<1:17:54, 519.35s/it]08/05/2022 09:31:25 - INFO - __main__ -   Train loss = 0.3498\n","08/05/2022 09:31:32 - INFO - __main__ -   Train loss = 0.3487\n","08/05/2022 09:31:39 - INFO - __main__ -   Train loss = 0.3469\n","08/05/2022 09:31:45 - INFO - __main__ -   Train loss = 0.3449\n","08/05/2022 09:31:52 - INFO - __main__ -   Train loss = 0.3422\n","08/05/2022 09:31:59 - INFO - __main__ -   Train loss = 0.3400\n","08/05/2022 09:32:05 - INFO - __main__ -   Train loss = 0.3387\n","08/05/2022 09:32:12 - INFO - __main__ -   Train loss = 0.3365\n","08/05/2022 09:32:12 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 09:32:12 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 09:32:12 - INFO - __main__ -     Batch size = 64\n","08/05/2022 09:32:22 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 09:32:22 - INFO - __main__ -     accuracy = 93.6350\n","08/05/2022 09:32:22 - INFO - __main__ -     loss = 0.1810\n","08/05/2022 09:32:22 - INFO - __main__ -   dev best_acc = 93.6350\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 09:32:32 - INFO - __main__ -   Train loss = 0.3349\n","08/05/2022 09:32:39 - INFO - __main__ -   Train loss = 0.3338\n","08/05/2022 09:32:45 - INFO - __main__ -   Train loss = 0.3330\n","08/05/2022 09:32:52 - INFO - __main__ -   Train loss = 0.3311\n","08/05/2022 09:32:59 - INFO - __main__ -   Train loss = 0.3286\n","08/05/2022 09:33:05 - INFO - __main__ -   Train loss = 0.3282\n","08/05/2022 09:33:12 - INFO - __main__ -   Train loss = 0.3259\n","08/05/2022 09:33:19 - INFO - __main__ -   Train loss = 0.3241\n","08/05/2022 09:33:26 - INFO - __main__ -   Train loss = 0.3226\n","08/05/2022 09:33:32 - INFO - __main__ -   Train loss = 0.3218\n","08/05/2022 09:33:39 - INFO - __main__ -   Train loss = 0.3222\n","08/05/2022 09:33:46 - INFO - __main__ -   Train loss = 0.3212\n","08/05/2022 09:33:52 - INFO - __main__ -   Train loss = 0.3201\n","08/05/2022 09:33:59 - INFO - __main__ -   Train loss = 0.3192\n","08/05/2022 09:34:06 - INFO - __main__ -   Train loss = 0.3186\n","08/05/2022 09:34:12 - INFO - __main__ -   Train loss = 0.3177\n","08/05/2022 09:34:19 - INFO - __main__ -   Train loss = 0.3156\n","08/05/2022 09:34:26 - INFO - __main__ -   Train loss = 0.3137\n","08/05/2022 09:34:32 - INFO - __main__ -   Train loss = 0.3112\n","08/05/2022 09:34:39 - INFO - __main__ -   Train loss = 0.3102\n","08/05/2022 09:34:39 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 09:34:39 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 09:34:39 - INFO - __main__ -     Batch size = 64\n","08/05/2022 09:34:49 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 09:34:49 - INFO - __main__ -     accuracy = 91.2577\n","08/05/2022 09:34:49 - INFO - __main__ -     loss = 0.2994\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 09:34:56 - INFO - __main__ -   Train loss = 0.3087\n","08/05/2022 09:35:02 - INFO - __main__ -   Train loss = 0.3075\n","08/05/2022 09:35:09 - INFO - __main__ -   Train loss = 0.3052\n","08/05/2022 09:35:16 - INFO - __main__ -   Train loss = 0.3054\n","08/05/2022 09:35:23 - INFO - __main__ -   Train loss = 0.3039\n","08/05/2022 09:35:29 - INFO - __main__ -   Train loss = 0.3024\n","08/05/2022 09:35:36 - INFO - __main__ -   Train loss = 0.3002\n","08/05/2022 09:35:43 - INFO - __main__ -   Train loss = 0.2981\n","08/05/2022 09:35:49 - INFO - __main__ -   Train loss = 0.2966\n","08/05/2022 09:35:56 - INFO - __main__ -   Train loss = 0.2945\n","08/05/2022 09:36:03 - INFO - __main__ -   Train loss = 0.2927\n","08/05/2022 09:36:10 - INFO - __main__ -   Train loss = 0.2910\n","08/05/2022 09:36:16 - INFO - __main__ -   Train loss = 0.2897\n","08/05/2022 09:36:23 - INFO - __main__ -   Train loss = 0.2879\n","08/05/2022 09:36:30 - INFO - __main__ -   Train loss = 0.2874\n","08/05/2022 09:36:36 - INFO - __main__ -   Train loss = 0.2854\n","08/05/2022 09:36:43 - INFO - __main__ -   Train loss = 0.2835\n","08/05/2022 09:36:50 - INFO - __main__ -   Train loss = 0.2817\n","08/05/2022 09:36:57 - INFO - __main__ -   Train loss = 0.2804\n","08/05/2022 09:37:03 - INFO - __main__ -   Train loss = 0.2790\n","08/05/2022 09:37:03 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 09:37:03 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 09:37:03 - INFO - __main__ -     Batch size = 64\n","08/05/2022 09:37:13 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 09:37:13 - INFO - __main__ -     accuracy = 91.8712\n","08/05/2022 09:37:13 - INFO - __main__ -     loss = 0.2483\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 09:37:20 - INFO - __main__ -   Train loss = 0.2778\n","08/05/2022 09:37:27 - INFO - __main__ -   Train loss = 0.2766\n","08/05/2022 09:37:33 - INFO - __main__ -   Train loss = 0.2751\n","08/05/2022 09:37:40 - INFO - __main__ -   Train loss = 0.2736\n","08/05/2022 09:37:47 - INFO - __main__ -   Train loss = 0.2723\n","08/05/2022 09:37:53 - INFO - __main__ -   Train loss = 0.2705\n","08/05/2022 09:38:00 - INFO - __main__ -   Train loss = 0.2690\n","08/05/2022 09:38:07 - INFO - __main__ -   Train loss = 0.2687\n","08/05/2022 09:38:13 - INFO - __main__ -   Train loss = 0.2673\n","08/05/2022 09:38:20 - INFO - __main__ -   Train loss = 0.2658\n","08/05/2022 09:38:27 - INFO - __main__ -   Train loss = 0.2653\n","08/05/2022 09:38:33 - INFO - __main__ -   Train loss = 0.2641\n","08/05/2022 09:38:40 - INFO - __main__ -   Train loss = 0.2633\n","08/05/2022 09:38:47 - INFO - __main__ -   Train loss = 0.2620\n","08/05/2022 09:38:54 - INFO - __main__ -   Train loss = 0.2608\n","08/05/2022 09:39:00 - INFO - __main__ -   Train loss = 0.2603\n","08/05/2022 09:39:07 - INFO - __main__ -   Train loss = 0.2588\n","08/05/2022 09:39:14 - INFO - __main__ -   Train loss = 0.2583\n","08/05/2022 09:39:20 - INFO - __main__ -   Train loss = 0.2577\n","08/05/2022 09:39:27 - INFO - __main__ -   Train loss = 0.2565\n","08/05/2022 09:39:27 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 09:39:27 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 09:39:27 - INFO - __main__ -     Batch size = 64\n","08/05/2022 09:39:37 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 09:39:37 - INFO - __main__ -     accuracy = 91.4877\n","08/05/2022 09:39:37 - INFO - __main__ -     loss = 0.3080\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 09:39:44 - INFO - __main__ -   Train loss = 0.2550\n","08/05/2022 09:39:50 - INFO - __main__ -   Train loss = 0.2548\n","08/05/2022 09:39:57 - INFO - __main__ -   Train loss = 0.2546\n","08/05/2022 09:40:04 - INFO - __main__ -   Train loss = 0.2533\n","08/05/2022 09:40:06 - INFO - __main__ -   train_acc = 94.6270\n","Epoch:  20%|██        | 2/10 [17:25<1:09:49, 523.63s/it]08/05/2022 09:40:10 - INFO - __main__ -   Train loss = 0.2525\n","08/05/2022 09:40:17 - INFO - __main__ -   Train loss = 0.2516\n","08/05/2022 09:40:24 - INFO - __main__ -   Train loss = 0.2505\n","08/05/2022 09:40:31 - INFO - __main__ -   Train loss = 0.2495\n","08/05/2022 09:40:37 - INFO - __main__ -   Train loss = 0.2481\n","08/05/2022 09:40:44 - INFO - __main__ -   Train loss = 0.2466\n","08/05/2022 09:40:51 - INFO - __main__ -   Train loss = 0.2455\n","08/05/2022 09:40:57 - INFO - __main__ -   Train loss = 0.2445\n","08/05/2022 09:41:04 - INFO - __main__ -   Train loss = 0.2439\n","08/05/2022 09:41:11 - INFO - __main__ -   Train loss = 0.2431\n","08/05/2022 09:41:17 - INFO - __main__ -   Train loss = 0.2423\n","08/05/2022 09:41:24 - INFO - __main__ -   Train loss = 0.2417\n","08/05/2022 09:41:31 - INFO - __main__ -   Train loss = 0.2405\n","08/05/2022 09:41:38 - INFO - __main__ -   Train loss = 0.2394\n","08/05/2022 09:41:44 - INFO - __main__ -   Train loss = 0.2382\n","08/05/2022 09:41:51 - INFO - __main__ -   Train loss = 0.2372\n","08/05/2022 09:41:51 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 09:41:51 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 09:41:51 - INFO - __main__ -     Batch size = 64\n","08/05/2022 09:42:01 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 09:42:01 - INFO - __main__ -     accuracy = 93.5583\n","08/05/2022 09:42:01 - INFO - __main__ -     loss = 0.2216\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 09:42:08 - INFO - __main__ -   Train loss = 0.2364\n","08/05/2022 09:42:14 - INFO - __main__ -   Train loss = 0.2358\n","08/05/2022 09:42:21 - INFO - __main__ -   Train loss = 0.2351\n","08/05/2022 09:42:28 - INFO - __main__ -   Train loss = 0.2357\n","08/05/2022 09:42:34 - INFO - __main__ -   Train loss = 0.2351\n","08/05/2022 09:42:41 - INFO - __main__ -   Train loss = 0.2345\n","08/05/2022 09:42:48 - INFO - __main__ -   Train loss = 0.2338\n","08/05/2022 09:42:54 - INFO - __main__ -   Train loss = 0.2329\n","08/05/2022 09:43:01 - INFO - __main__ -   Train loss = 0.2318\n","08/05/2022 09:43:08 - INFO - __main__ -   Train loss = 0.2310\n","08/05/2022 09:43:15 - INFO - __main__ -   Train loss = 0.2299\n","08/05/2022 09:43:21 - INFO - __main__ -   Train loss = 0.2292\n","08/05/2022 09:43:28 - INFO - __main__ -   Train loss = 0.2288\n","08/05/2022 09:43:35 - INFO - __main__ -   Train loss = 0.2285\n","08/05/2022 09:43:41 - INFO - __main__ -   Train loss = 0.2274\n","08/05/2022 09:43:48 - INFO - __main__ -   Train loss = 0.2270\n","08/05/2022 09:43:55 - INFO - __main__ -   Train loss = 0.2265\n","08/05/2022 09:44:01 - INFO - __main__ -   Train loss = 0.2256\n","08/05/2022 09:44:08 - INFO - __main__ -   Train loss = 0.2247\n","08/05/2022 09:44:15 - INFO - __main__ -   Train loss = 0.2237\n","08/05/2022 09:44:15 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 09:44:15 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 09:44:15 - INFO - __main__ -     Batch size = 64\n","08/05/2022 09:44:25 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 09:44:25 - INFO - __main__ -     accuracy = 89.8773\n","08/05/2022 09:44:25 - INFO - __main__ -     loss = 0.4610\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 09:44:31 - INFO - __main__ -   Train loss = 0.2230\n","08/05/2022 09:44:38 - INFO - __main__ -   Train loss = 0.2220\n","08/05/2022 09:44:45 - INFO - __main__ -   Train loss = 0.2212\n","08/05/2022 09:44:52 - INFO - __main__ -   Train loss = 0.2202\n","08/05/2022 09:44:58 - INFO - __main__ -   Train loss = 0.2195\n","08/05/2022 09:45:05 - INFO - __main__ -   Train loss = 0.2188\n","08/05/2022 09:45:12 - INFO - __main__ -   Train loss = 0.2180\n","08/05/2022 09:45:18 - INFO - __main__ -   Train loss = 0.2172\n","08/05/2022 09:45:25 - INFO - __main__ -   Train loss = 0.2165\n","08/05/2022 09:45:32 - INFO - __main__ -   Train loss = 0.2155\n","08/05/2022 09:45:38 - INFO - __main__ -   Train loss = 0.2148\n","08/05/2022 09:45:45 - INFO - __main__ -   Train loss = 0.2140\n","08/05/2022 09:45:52 - INFO - __main__ -   Train loss = 0.2133\n","08/05/2022 09:45:59 - INFO - __main__ -   Train loss = 0.2127\n","08/05/2022 09:46:05 - INFO - __main__ -   Train loss = 0.2117\n","08/05/2022 09:46:12 - INFO - __main__ -   Train loss = 0.2108\n","08/05/2022 09:46:19 - INFO - __main__ -   Train loss = 0.2100\n","08/05/2022 09:46:25 - INFO - __main__ -   Train loss = 0.2092\n","08/05/2022 09:46:32 - INFO - __main__ -   Train loss = 0.2084\n","08/05/2022 09:46:39 - INFO - __main__ -   Train loss = 0.2077\n","08/05/2022 09:46:39 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 09:46:39 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 09:46:39 - INFO - __main__ -     Batch size = 64\n","08/05/2022 09:46:49 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 09:46:49 - INFO - __main__ -     accuracy = 93.9417\n","08/05/2022 09:46:49 - INFO - __main__ -     loss = 0.2265\n","08/05/2022 09:46:49 - INFO - __main__ -   dev best_acc = 93.9417\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 09:46:59 - INFO - __main__ -   Train loss = 0.2068\n","08/05/2022 09:47:05 - INFO - __main__ -   Train loss = 0.2060\n","08/05/2022 09:47:12 - INFO - __main__ -   Train loss = 0.2051\n","08/05/2022 09:47:19 - INFO - __main__ -   Train loss = 0.2043\n","08/05/2022 09:47:26 - INFO - __main__ -   Train loss = 0.2036\n","08/05/2022 09:47:32 - INFO - __main__ -   Train loss = 0.2031\n","08/05/2022 09:47:39 - INFO - __main__ -   Train loss = 0.2027\n","08/05/2022 09:47:46 - INFO - __main__ -   Train loss = 0.2024\n","08/05/2022 09:47:52 - INFO - __main__ -   Train loss = 0.2018\n","08/05/2022 09:47:59 - INFO - __main__ -   Train loss = 0.2012\n","08/05/2022 09:48:06 - INFO - __main__ -   Train loss = 0.2005\n","08/05/2022 09:48:13 - INFO - __main__ -   Train loss = 0.1998\n","08/05/2022 09:48:19 - INFO - __main__ -   Train loss = 0.1990\n","08/05/2022 09:48:26 - INFO - __main__ -   Train loss = 0.1982\n","08/05/2022 09:48:33 - INFO - __main__ -   Train loss = 0.1975\n","08/05/2022 09:48:39 - INFO - __main__ -   Train loss = 0.1968\n","08/05/2022 09:48:43 - INFO - __main__ -   train_acc = 97.5971\n","Epoch:  30%|███       | 3/10 [26:02<1:00:43, 520.56s/it]08/05/2022 09:48:46 - INFO - __main__ -   Train loss = 0.1963\n","08/05/2022 09:48:53 - INFO - __main__ -   Train loss = 0.1957\n","08/05/2022 09:48:59 - INFO - __main__ -   Train loss = 0.1952\n","08/05/2022 09:49:06 - INFO - __main__ -   Train loss = 0.1945\n","08/05/2022 09:49:06 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 09:49:06 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 09:49:06 - INFO - __main__ -     Batch size = 64\n","08/05/2022 09:49:16 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 09:49:16 - INFO - __main__ -     accuracy = 94.5552\n","08/05/2022 09:49:16 - INFO - __main__ -     loss = 0.2211\n","08/05/2022 09:49:16 - INFO - __main__ -   dev best_acc = 94.5552\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 09:49:26 - INFO - __main__ -   Train loss = 0.1937\n","08/05/2022 09:49:33 - INFO - __main__ -   Train loss = 0.1929\n","08/05/2022 09:49:39 - INFO - __main__ -   Train loss = 0.1923\n","08/05/2022 09:49:46 - INFO - __main__ -   Train loss = 0.1918\n","08/05/2022 09:49:53 - INFO - __main__ -   Train loss = 0.1912\n","08/05/2022 09:50:00 - INFO - __main__ -   Train loss = 0.1909\n","08/05/2022 09:50:06 - INFO - __main__ -   Train loss = 0.1903\n","08/05/2022 09:50:13 - INFO - __main__ -   Train loss = 0.1900\n","08/05/2022 09:50:20 - INFO - __main__ -   Train loss = 0.1893\n","08/05/2022 09:50:27 - INFO - __main__ -   Train loss = 0.1892\n","08/05/2022 09:50:33 - INFO - __main__ -   Train loss = 0.1884\n","08/05/2022 09:50:40 - INFO - __main__ -   Train loss = 0.1884\n","08/05/2022 09:50:47 - INFO - __main__ -   Train loss = 0.1881\n","08/05/2022 09:50:53 - INFO - __main__ -   Train loss = 0.1876\n","08/05/2022 09:51:00 - INFO - __main__ -   Train loss = 0.1872\n","08/05/2022 09:51:07 - INFO - __main__ -   Train loss = 0.1870\n","08/05/2022 09:51:13 - INFO - __main__ -   Train loss = 0.1865\n","08/05/2022 09:51:20 - INFO - __main__ -   Train loss = 0.1859\n","08/05/2022 09:51:27 - INFO - __main__ -   Train loss = 0.1853\n","08/05/2022 09:51:34 - INFO - __main__ -   Train loss = 0.1847\n","08/05/2022 09:51:34 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 09:51:34 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 09:51:34 - INFO - __main__ -     Batch size = 64\n","08/05/2022 09:51:44 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 09:51:44 - INFO - __main__ -     accuracy = 94.4785\n","08/05/2022 09:51:44 - INFO - __main__ -     loss = 0.2409\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 09:51:50 - INFO - __main__ -   Train loss = 0.1842\n","08/05/2022 09:51:57 - INFO - __main__ -   Train loss = 0.1835\n","08/05/2022 09:52:04 - INFO - __main__ -   Train loss = 0.1829\n","08/05/2022 09:52:11 - INFO - __main__ -   Train loss = 0.1824\n","08/05/2022 09:52:17 - INFO - __main__ -   Train loss = 0.1818\n","08/05/2022 09:52:24 - INFO - __main__ -   Train loss = 0.1815\n","08/05/2022 09:52:31 - INFO - __main__ -   Train loss = 0.1808\n","08/05/2022 09:52:37 - INFO - __main__ -   Train loss = 0.1809\n","08/05/2022 09:52:44 - INFO - __main__ -   Train loss = 0.1806\n","08/05/2022 09:52:51 - INFO - __main__ -   Train loss = 0.1803\n","08/05/2022 09:52:58 - INFO - __main__ -   Train loss = 0.1796\n","08/05/2022 09:53:04 - INFO - __main__ -   Train loss = 0.1790\n","08/05/2022 09:53:11 - INFO - __main__ -   Train loss = 0.1786\n","08/05/2022 09:53:18 - INFO - __main__ -   Train loss = 0.1781\n","08/05/2022 09:53:24 - INFO - __main__ -   Train loss = 0.1778\n","08/05/2022 09:53:31 - INFO - __main__ -   Train loss = 0.1772\n","08/05/2022 09:53:38 - INFO - __main__ -   Train loss = 0.1766\n","08/05/2022 09:53:44 - INFO - __main__ -   Train loss = 0.1761\n","08/05/2022 09:53:51 - INFO - __main__ -   Train loss = 0.1756\n","08/05/2022 09:53:58 - INFO - __main__ -   Train loss = 0.1752\n","08/05/2022 09:53:58 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 09:53:58 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 09:53:58 - INFO - __main__ -     Batch size = 64\n","08/05/2022 09:54:08 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 09:54:08 - INFO - __main__ -     accuracy = 93.7117\n","08/05/2022 09:54:08 - INFO - __main__ -     loss = 0.2584\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 09:54:14 - INFO - __main__ -   Train loss = 0.1746\n","08/05/2022 09:54:21 - INFO - __main__ -   Train loss = 0.1740\n","08/05/2022 09:54:28 - INFO - __main__ -   Train loss = 0.1736\n","08/05/2022 09:54:35 - INFO - __main__ -   Train loss = 0.1731\n","08/05/2022 09:54:41 - INFO - __main__ -   Train loss = 0.1726\n","08/05/2022 09:54:48 - INFO - __main__ -   Train loss = 0.1723\n","08/05/2022 09:54:55 - INFO - __main__ -   Train loss = 0.1717\n","08/05/2022 09:55:01 - INFO - __main__ -   Train loss = 0.1711\n","08/05/2022 09:55:08 - INFO - __main__ -   Train loss = 0.1706\n","08/05/2022 09:55:15 - INFO - __main__ -   Train loss = 0.1700\n","08/05/2022 09:55:22 - INFO - __main__ -   Train loss = 0.1696\n","08/05/2022 09:55:28 - INFO - __main__ -   Train loss = 0.1691\n","08/05/2022 09:55:35 - INFO - __main__ -   Train loss = 0.1685\n","08/05/2022 09:55:42 - INFO - __main__ -   Train loss = 0.1683\n","08/05/2022 09:55:48 - INFO - __main__ -   Train loss = 0.1678\n","08/05/2022 09:55:55 - INFO - __main__ -   Train loss = 0.1672\n","08/05/2022 09:56:02 - INFO - __main__ -   Train loss = 0.1671\n","08/05/2022 09:56:08 - INFO - __main__ -   Train loss = 0.1666\n","08/05/2022 09:56:15 - INFO - __main__ -   Train loss = 0.1663\n","08/05/2022 09:56:22 - INFO - __main__ -   Train loss = 0.1658\n","08/05/2022 09:56:22 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 09:56:22 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 09:56:22 - INFO - __main__ -     Batch size = 64\n","08/05/2022 09:56:32 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 09:56:32 - INFO - __main__ -     accuracy = 93.7883\n","08/05/2022 09:56:32 - INFO - __main__ -     loss = 0.2779\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 09:56:38 - INFO - __main__ -   Train loss = 0.1653\n","08/05/2022 09:56:45 - INFO - __main__ -   Train loss = 0.1648\n","08/05/2022 09:56:52 - INFO - __main__ -   Train loss = 0.1644\n","08/05/2022 09:56:59 - INFO - __main__ -   Train loss = 0.1641\n","08/05/2022 09:57:05 - INFO - __main__ -   Train loss = 0.1636\n","08/05/2022 09:57:12 - INFO - __main__ -   Train loss = 0.1634\n","08/05/2022 09:57:19 - INFO - __main__ -   Train loss = 0.1628\n","08/05/2022 09:57:25 - INFO - __main__ -   Train loss = 0.1626\n","08/05/2022 09:57:31 - INFO - __main__ -   train_acc = 98.4067\n","Epoch:  40%|████      | 4/10 [34:50<52:19, 523.25s/it]  08/05/2022 09:57:32 - INFO - __main__ -   Train loss = 0.1621\n","08/05/2022 09:57:39 - INFO - __main__ -   Train loss = 0.1616\n","08/05/2022 09:57:45 - INFO - __main__ -   Train loss = 0.1612\n","08/05/2022 09:57:52 - INFO - __main__ -   Train loss = 0.1607\n","08/05/2022 09:57:59 - INFO - __main__ -   Train loss = 0.1602\n","08/05/2022 09:58:06 - INFO - __main__ -   Train loss = 0.1596\n","08/05/2022 09:58:12 - INFO - __main__ -   Train loss = 0.1591\n","08/05/2022 09:58:19 - INFO - __main__ -   Train loss = 0.1590\n","08/05/2022 09:58:26 - INFO - __main__ -   Train loss = 0.1585\n","08/05/2022 09:58:32 - INFO - __main__ -   Train loss = 0.1581\n","08/05/2022 09:58:39 - INFO - __main__ -   Train loss = 0.1581\n","08/05/2022 09:58:46 - INFO - __main__ -   Train loss = 0.1579\n","08/05/2022 09:58:46 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 09:58:46 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 09:58:46 - INFO - __main__ -     Batch size = 64\n","08/05/2022 09:58:56 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 09:58:56 - INFO - __main__ -     accuracy = 92.9448\n","08/05/2022 09:58:56 - INFO - __main__ -     loss = 0.3076\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 09:59:02 - INFO - __main__ -   Train loss = 0.1575\n","08/05/2022 09:59:09 - INFO - __main__ -   Train loss = 0.1572\n","08/05/2022 09:59:16 - INFO - __main__ -   Train loss = 0.1567\n","08/05/2022 09:59:22 - INFO - __main__ -   Train loss = 0.1564\n","08/05/2022 09:59:29 - INFO - __main__ -   Train loss = 0.1562\n","08/05/2022 09:59:36 - INFO - __main__ -   Train loss = 0.1558\n","08/05/2022 09:59:42 - INFO - __main__ -   Train loss = 0.1553\n","08/05/2022 09:59:49 - INFO - __main__ -   Train loss = 0.1551\n","08/05/2022 09:59:56 - INFO - __main__ -   Train loss = 0.1548\n","08/05/2022 10:00:03 - INFO - __main__ -   Train loss = 0.1544\n","08/05/2022 10:00:09 - INFO - __main__ -   Train loss = 0.1540\n","08/05/2022 10:00:16 - INFO - __main__ -   Train loss = 0.1537\n","08/05/2022 10:00:23 - INFO - __main__ -   Train loss = 0.1534\n","08/05/2022 10:00:29 - INFO - __main__ -   Train loss = 0.1530\n","08/05/2022 10:00:36 - INFO - __main__ -   Train loss = 0.1526\n","08/05/2022 10:00:43 - INFO - __main__ -   Train loss = 0.1522\n","08/05/2022 10:00:50 - INFO - __main__ -   Train loss = 0.1518\n","08/05/2022 10:00:56 - INFO - __main__ -   Train loss = 0.1517\n","08/05/2022 10:01:03 - INFO - __main__ -   Train loss = 0.1514\n","08/05/2022 10:01:10 - INFO - __main__ -   Train loss = 0.1510\n","08/05/2022 10:01:10 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 10:01:10 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 10:01:10 - INFO - __main__ -     Batch size = 64\n","08/05/2022 10:01:20 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 10:01:20 - INFO - __main__ -     accuracy = 94.5552\n","08/05/2022 10:01:20 - INFO - __main__ -     loss = 0.2341\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 10:01:26 - INFO - __main__ -   Train loss = 0.1507\n","08/05/2022 10:01:33 - INFO - __main__ -   Train loss = 0.1503\n","08/05/2022 10:01:40 - INFO - __main__ -   Train loss = 0.1499\n","08/05/2022 10:01:46 - INFO - __main__ -   Train loss = 0.1494\n","08/05/2022 10:01:53 - INFO - __main__ -   Train loss = 0.1491\n","08/05/2022 10:02:00 - INFO - __main__ -   Train loss = 0.1487\n","08/05/2022 10:02:07 - INFO - __main__ -   Train loss = 0.1483\n","08/05/2022 10:02:13 - INFO - __main__ -   Train loss = 0.1480\n","08/05/2022 10:02:20 - INFO - __main__ -   Train loss = 0.1475\n","08/05/2022 10:02:27 - INFO - __main__ -   Train loss = 0.1473\n","08/05/2022 10:02:33 - INFO - __main__ -   Train loss = 0.1469\n","08/05/2022 10:02:40 - INFO - __main__ -   Train loss = 0.1466\n","08/05/2022 10:02:47 - INFO - __main__ -   Train loss = 0.1462\n","08/05/2022 10:02:53 - INFO - __main__ -   Train loss = 0.1459\n","08/05/2022 10:03:00 - INFO - __main__ -   Train loss = 0.1455\n","08/05/2022 10:03:07 - INFO - __main__ -   Train loss = 0.1451\n","08/05/2022 10:03:14 - INFO - __main__ -   Train loss = 0.1447\n","08/05/2022 10:03:20 - INFO - __main__ -   Train loss = 0.1444\n","08/05/2022 10:03:27 - INFO - __main__ -   Train loss = 0.1440\n","08/05/2022 10:03:34 - INFO - __main__ -   Train loss = 0.1436\n","08/05/2022 10:03:34 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 10:03:34 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 10:03:34 - INFO - __main__ -     Batch size = 64\n","08/05/2022 10:03:44 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 10:03:44 - INFO - __main__ -     accuracy = 92.8681\n","08/05/2022 10:03:44 - INFO - __main__ -     loss = 0.3493\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 10:03:50 - INFO - __main__ -   Train loss = 0.1432\n","08/05/2022 10:03:57 - INFO - __main__ -   Train loss = 0.1429\n","08/05/2022 10:04:04 - INFO - __main__ -   Train loss = 0.1425\n","08/05/2022 10:04:10 - INFO - __main__ -   Train loss = 0.1421\n","08/05/2022 10:04:17 - INFO - __main__ -   Train loss = 0.1418\n","08/05/2022 10:04:24 - INFO - __main__ -   Train loss = 0.1414\n","08/05/2022 10:04:30 - INFO - __main__ -   Train loss = 0.1411\n","08/05/2022 10:04:37 - INFO - __main__ -   Train loss = 0.1407\n","08/05/2022 10:04:44 - INFO - __main__ -   Train loss = 0.1404\n","08/05/2022 10:04:51 - INFO - __main__ -   Train loss = 0.1401\n","08/05/2022 10:04:57 - INFO - __main__ -   Train loss = 0.1397\n","08/05/2022 10:05:04 - INFO - __main__ -   Train loss = 0.1394\n","08/05/2022 10:05:11 - INFO - __main__ -   Train loss = 0.1390\n","08/05/2022 10:05:17 - INFO - __main__ -   Train loss = 0.1387\n","08/05/2022 10:05:24 - INFO - __main__ -   Train loss = 0.1383\n","08/05/2022 10:05:31 - INFO - __main__ -   Train loss = 0.1380\n","08/05/2022 10:05:38 - INFO - __main__ -   Train loss = 0.1376\n","08/05/2022 10:05:44 - INFO - __main__ -   Train loss = 0.1373\n","08/05/2022 10:05:51 - INFO - __main__ -   Train loss = 0.1369\n","08/05/2022 10:05:58 - INFO - __main__ -   Train loss = 0.1367\n","08/05/2022 10:05:58 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 10:05:58 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 10:05:58 - INFO - __main__ -     Batch size = 64\n","08/05/2022 10:06:08 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 10:06:08 - INFO - __main__ -     accuracy = 92.1779\n","08/05/2022 10:06:08 - INFO - __main__ -     loss = 0.3888\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 10:06:14 - INFO - __main__ -   Train loss = 0.1363\n","08/05/2022 10:06:14 - INFO - __main__ -   train_acc = 99.1298\n","Epoch:  50%|█████     | 5/10 [43:33<43:36, 523.37s/it]08/05/2022 10:06:21 - INFO - __main__ -   Train loss = 0.1360\n","08/05/2022 10:06:28 - INFO - __main__ -   Train loss = 0.1357\n","08/05/2022 10:06:34 - INFO - __main__ -   Train loss = 0.1353\n","08/05/2022 10:06:41 - INFO - __main__ -   Train loss = 0.1350\n","08/05/2022 10:06:48 - INFO - __main__ -   Train loss = 0.1347\n","08/05/2022 10:06:54 - INFO - __main__ -   Train loss = 0.1343\n","08/05/2022 10:07:01 - INFO - __main__ -   Train loss = 0.1342\n","08/05/2022 10:07:08 - INFO - __main__ -   Train loss = 0.1338\n","08/05/2022 10:07:15 - INFO - __main__ -   Train loss = 0.1335\n","08/05/2022 10:07:21 - INFO - __main__ -   Train loss = 0.1332\n","08/05/2022 10:07:28 - INFO - __main__ -   Train loss = 0.1331\n","08/05/2022 10:07:35 - INFO - __main__ -   Train loss = 0.1327\n","08/05/2022 10:07:41 - INFO - __main__ -   Train loss = 0.1324\n","08/05/2022 10:07:48 - INFO - __main__ -   Train loss = 0.1321\n","08/05/2022 10:07:55 - INFO - __main__ -   Train loss = 0.1317\n","08/05/2022 10:08:02 - INFO - __main__ -   Train loss = 0.1314\n","08/05/2022 10:08:08 - INFO - __main__ -   Train loss = 0.1311\n","08/05/2022 10:08:15 - INFO - __main__ -   Train loss = 0.1308\n","08/05/2022 10:08:22 - INFO - __main__ -   Train loss = 0.1305\n","08/05/2022 10:08:22 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 10:08:22 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 10:08:22 - INFO - __main__ -     Batch size = 64\n","08/05/2022 10:08:32 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 10:08:32 - INFO - __main__ -     accuracy = 92.9448\n","08/05/2022 10:08:32 - INFO - __main__ -     loss = 0.3696\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 10:08:38 - INFO - __main__ -   Train loss = 0.1303\n","08/05/2022 10:08:45 - INFO - __main__ -   Train loss = 0.1302\n","08/05/2022 10:08:52 - INFO - __main__ -   Train loss = 0.1300\n","08/05/2022 10:08:59 - INFO - __main__ -   Train loss = 0.1298\n","08/05/2022 10:09:05 - INFO - __main__ -   Train loss = 0.1294\n","08/05/2022 10:09:12 - INFO - __main__ -   Train loss = 0.1291\n","08/05/2022 10:09:19 - INFO - __main__ -   Train loss = 0.1288\n","08/05/2022 10:09:25 - INFO - __main__ -   Train loss = 0.1285\n","08/05/2022 10:09:32 - INFO - __main__ -   Train loss = 0.1282\n","08/05/2022 10:09:39 - INFO - __main__ -   Train loss = 0.1280\n","08/05/2022 10:09:45 - INFO - __main__ -   Train loss = 0.1278\n","08/05/2022 10:09:52 - INFO - __main__ -   Train loss = 0.1275\n","08/05/2022 10:09:59 - INFO - __main__ -   Train loss = 0.1273\n","08/05/2022 10:10:06 - INFO - __main__ -   Train loss = 0.1270\n","08/05/2022 10:10:12 - INFO - __main__ -   Train loss = 0.1267\n","08/05/2022 10:10:19 - INFO - __main__ -   Train loss = 0.1264\n","08/05/2022 10:10:26 - INFO - __main__ -   Train loss = 0.1261\n","08/05/2022 10:10:32 - INFO - __main__ -   Train loss = 0.1258\n","08/05/2022 10:10:39 - INFO - __main__ -   Train loss = 0.1255\n","08/05/2022 10:10:46 - INFO - __main__ -   Train loss = 0.1252\n","08/05/2022 10:10:46 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 10:10:46 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 10:10:46 - INFO - __main__ -     Batch size = 64\n","08/05/2022 10:10:56 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 10:10:56 - INFO - __main__ -     accuracy = 93.2515\n","08/05/2022 10:10:56 - INFO - __main__ -     loss = 0.3437\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 10:11:02 - INFO - __main__ -   Train loss = 0.1248\n","08/05/2022 10:11:09 - INFO - __main__ -   Train loss = 0.1246\n","08/05/2022 10:11:16 - INFO - __main__ -   Train loss = 0.1243\n","08/05/2022 10:11:23 - INFO - __main__ -   Train loss = 0.1241\n","08/05/2022 10:11:29 - INFO - __main__ -   Train loss = 0.1238\n","08/05/2022 10:11:36 - INFO - __main__ -   Train loss = 0.1235\n","08/05/2022 10:11:43 - INFO - __main__ -   Train loss = 0.1232\n","08/05/2022 10:11:49 - INFO - __main__ -   Train loss = 0.1229\n","08/05/2022 10:11:56 - INFO - __main__ -   Train loss = 0.1227\n","08/05/2022 10:12:03 - INFO - __main__ -   Train loss = 0.1224\n","08/05/2022 10:12:10 - INFO - __main__ -   Train loss = 0.1221\n","08/05/2022 10:12:16 - INFO - __main__ -   Train loss = 0.1218\n","08/05/2022 10:12:23 - INFO - __main__ -   Train loss = 0.1215\n","08/05/2022 10:12:30 - INFO - __main__ -   Train loss = 0.1212\n","08/05/2022 10:12:36 - INFO - __main__ -   Train loss = 0.1209\n","08/05/2022 10:12:43 - INFO - __main__ -   Train loss = 0.1207\n","08/05/2022 10:12:50 - INFO - __main__ -   Train loss = 0.1204\n","08/05/2022 10:12:57 - INFO - __main__ -   Train loss = 0.1201\n","08/05/2022 10:13:03 - INFO - __main__ -   Train loss = 0.1198\n","08/05/2022 10:13:10 - INFO - __main__ -   Train loss = 0.1195\n","08/05/2022 10:13:10 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 10:13:10 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 10:13:10 - INFO - __main__ -     Batch size = 64\n","08/05/2022 10:13:20 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 10:13:20 - INFO - __main__ -     accuracy = 92.9448\n","08/05/2022 10:13:20 - INFO - __main__ -     loss = 0.3910\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 10:13:27 - INFO - __main__ -   Train loss = 0.1192\n","08/05/2022 10:13:33 - INFO - __main__ -   Train loss = 0.1190\n","08/05/2022 10:13:40 - INFO - __main__ -   Train loss = 0.1187\n","08/05/2022 10:13:47 - INFO - __main__ -   Train loss = 0.1185\n","08/05/2022 10:13:54 - INFO - __main__ -   Train loss = 0.1182\n","08/05/2022 10:14:00 - INFO - __main__ -   Train loss = 0.1179\n","08/05/2022 10:14:07 - INFO - __main__ -   Train loss = 0.1177\n","08/05/2022 10:14:14 - INFO - __main__ -   Train loss = 0.1174\n","08/05/2022 10:14:20 - INFO - __main__ -   Train loss = 0.1171\n","08/05/2022 10:14:27 - INFO - __main__ -   Train loss = 0.1168\n","08/05/2022 10:14:34 - INFO - __main__ -   Train loss = 0.1166\n","08/05/2022 10:14:40 - INFO - __main__ -   Train loss = 0.1164\n","08/05/2022 10:14:47 - INFO - __main__ -   Train loss = 0.1162\n","08/05/2022 10:14:48 - INFO - __main__ -   train_acc = 99.6450\n","Epoch:  60%|██████    | 6/10 [52:07<34:40, 520.22s/it]08/05/2022 10:14:54 - INFO - __main__ -   Train loss = 0.1159\n","08/05/2022 10:15:00 - INFO - __main__ -   Train loss = 0.1157\n","08/05/2022 10:15:07 - INFO - __main__ -   Train loss = 0.1155\n","08/05/2022 10:15:14 - INFO - __main__ -   Train loss = 0.1152\n","08/05/2022 10:15:21 - INFO - __main__ -   Train loss = 0.1151\n","08/05/2022 10:15:27 - INFO - __main__ -   Train loss = 0.1148\n","08/05/2022 10:15:34 - INFO - __main__ -   Train loss = 0.1146\n","08/05/2022 10:15:34 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 10:15:34 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 10:15:34 - INFO - __main__ -     Batch size = 64\n","08/05/2022 10:15:44 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 10:15:44 - INFO - __main__ -     accuracy = 93.0982\n","08/05/2022 10:15:44 - INFO - __main__ -     loss = 0.3814\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 10:15:51 - INFO - __main__ -   Train loss = 0.1145\n","08/05/2022 10:15:57 - INFO - __main__ -   Train loss = 0.1142\n","08/05/2022 10:16:04 - INFO - __main__ -   Train loss = 0.1140\n","08/05/2022 10:16:11 - INFO - __main__ -   Train loss = 0.1138\n","08/05/2022 10:16:18 - INFO - __main__ -   Train loss = 0.1136\n","08/05/2022 10:16:24 - INFO - __main__ -   Train loss = 0.1133\n","08/05/2022 10:16:31 - INFO - __main__ -   Train loss = 0.1131\n","08/05/2022 10:16:38 - INFO - __main__ -   Train loss = 0.1128\n","08/05/2022 10:16:44 - INFO - __main__ -   Train loss = 0.1126\n","08/05/2022 10:16:51 - INFO - __main__ -   Train loss = 0.1123\n","08/05/2022 10:16:58 - INFO - __main__ -   Train loss = 0.1121\n","08/05/2022 10:17:04 - INFO - __main__ -   Train loss = 0.1118\n","08/05/2022 10:17:11 - INFO - __main__ -   Train loss = 0.1117\n","08/05/2022 10:17:18 - INFO - __main__ -   Train loss = 0.1114\n","08/05/2022 10:17:25 - INFO - __main__ -   Train loss = 0.1112\n","08/05/2022 10:17:31 - INFO - __main__ -   Train loss = 0.1110\n","08/05/2022 10:17:38 - INFO - __main__ -   Train loss = 0.1107\n","08/05/2022 10:17:45 - INFO - __main__ -   Train loss = 0.1105\n","08/05/2022 10:17:51 - INFO - __main__ -   Train loss = 0.1103\n","08/05/2022 10:17:58 - INFO - __main__ -   Train loss = 0.1101\n","08/05/2022 10:17:58 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 10:17:58 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 10:17:58 - INFO - __main__ -     Batch size = 64\n","08/05/2022 10:18:08 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 10:18:08 - INFO - __main__ -     accuracy = 93.5583\n","08/05/2022 10:18:08 - INFO - __main__ -     loss = 0.3649\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 10:18:15 - INFO - __main__ -   Train loss = 0.1099\n","08/05/2022 10:18:21 - INFO - __main__ -   Train loss = 0.1097\n","08/05/2022 10:18:28 - INFO - __main__ -   Train loss = 0.1094\n","08/05/2022 10:18:35 - INFO - __main__ -   Train loss = 0.1092\n","08/05/2022 10:18:42 - INFO - __main__ -   Train loss = 0.1090\n","08/05/2022 10:18:48 - INFO - __main__ -   Train loss = 0.1088\n","08/05/2022 10:18:55 - INFO - __main__ -   Train loss = 0.1086\n","08/05/2022 10:19:02 - INFO - __main__ -   Train loss = 0.1084\n","08/05/2022 10:19:08 - INFO - __main__ -   Train loss = 0.1081\n","08/05/2022 10:19:15 - INFO - __main__ -   Train loss = 0.1079\n","08/05/2022 10:19:22 - INFO - __main__ -   Train loss = 0.1077\n","08/05/2022 10:19:29 - INFO - __main__ -   Train loss = 0.1075\n","08/05/2022 10:19:35 - INFO - __main__ -   Train loss = 0.1073\n","08/05/2022 10:19:42 - INFO - __main__ -   Train loss = 0.1070\n","08/05/2022 10:19:49 - INFO - __main__ -   Train loss = 0.1068\n","08/05/2022 10:19:55 - INFO - __main__ -   Train loss = 0.1068\n","08/05/2022 10:20:02 - INFO - __main__ -   Train loss = 0.1066\n","08/05/2022 10:20:09 - INFO - __main__ -   Train loss = 0.1064\n","08/05/2022 10:20:16 - INFO - __main__ -   Train loss = 0.1062\n","08/05/2022 10:20:22 - INFO - __main__ -   Train loss = 0.1060\n","08/05/2022 10:20:22 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 10:20:22 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 10:20:22 - INFO - __main__ -     Batch size = 64\n","08/05/2022 10:20:32 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 10:20:32 - INFO - __main__ -     accuracy = 93.3282\n","08/05/2022 10:20:32 - INFO - __main__ -     loss = 0.3884\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 10:20:39 - INFO - __main__ -   Train loss = 0.1058\n","08/05/2022 10:20:46 - INFO - __main__ -   Train loss = 0.1056\n","08/05/2022 10:20:52 - INFO - __main__ -   Train loss = 0.1054\n","08/05/2022 10:20:59 - INFO - __main__ -   Train loss = 0.1052\n","08/05/2022 10:21:06 - INFO - __main__ -   Train loss = 0.1049\n","08/05/2022 10:21:13 - INFO - __main__ -   Train loss = 0.1047\n","08/05/2022 10:21:19 - INFO - __main__ -   Train loss = 0.1045\n","08/05/2022 10:21:26 - INFO - __main__ -   Train loss = 0.1043\n","08/05/2022 10:21:33 - INFO - __main__ -   Train loss = 0.1041\n","08/05/2022 10:21:39 - INFO - __main__ -   Train loss = 0.1039\n","08/05/2022 10:21:46 - INFO - __main__ -   Train loss = 0.1037\n","08/05/2022 10:21:53 - INFO - __main__ -   Train loss = 0.1035\n","08/05/2022 10:22:00 - INFO - __main__ -   Train loss = 0.1033\n","08/05/2022 10:22:06 - INFO - __main__ -   Train loss = 0.1032\n","08/05/2022 10:22:13 - INFO - __main__ -   Train loss = 0.1029\n","08/05/2022 10:22:20 - INFO - __main__ -   Train loss = 0.1027\n","08/05/2022 10:22:26 - INFO - __main__ -   Train loss = 0.1025\n","08/05/2022 10:22:33 - INFO - __main__ -   Train loss = 0.1023\n","08/05/2022 10:22:40 - INFO - __main__ -   Train loss = 0.1021\n","08/05/2022 10:22:47 - INFO - __main__ -   Train loss = 0.1019\n","08/05/2022 10:22:47 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 10:22:47 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 10:22:47 - INFO - __main__ -     Batch size = 64\n","08/05/2022 10:22:56 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 10:22:56 - INFO - __main__ -     accuracy = 93.2515\n","08/05/2022 10:22:56 - INFO - __main__ -     loss = 0.4273\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 10:23:03 - INFO - __main__ -   Train loss = 0.1017\n","08/05/2022 10:23:10 - INFO - __main__ -   Train loss = 0.1015\n","08/05/2022 10:23:17 - INFO - __main__ -   Train loss = 0.1013\n","08/05/2022 10:23:23 - INFO - __main__ -   Train loss = 0.1011\n","08/05/2022 10:23:30 - INFO - __main__ -   Train loss = 0.1009\n","08/05/2022 10:23:33 - INFO - __main__ -   train_acc = 99.8138\n","Epoch:  70%|███████   | 7/10 [1:00:52<26:04, 521.54s/it]08/05/2022 10:23:37 - INFO - __main__ -   Train loss = 0.1007\n","08/05/2022 10:23:43 - INFO - __main__ -   Train loss = 0.1006\n","08/05/2022 10:23:50 - INFO - __main__ -   Train loss = 0.1004\n","08/05/2022 10:23:57 - INFO - __main__ -   Train loss = 0.1002\n","08/05/2022 10:24:04 - INFO - __main__ -   Train loss = 0.1000\n","08/05/2022 10:24:10 - INFO - __main__ -   Train loss = 0.0999\n","08/05/2022 10:24:17 - INFO - __main__ -   Train loss = 0.0997\n","08/05/2022 10:24:24 - INFO - __main__ -   Train loss = 0.0996\n","08/05/2022 10:24:30 - INFO - __main__ -   Train loss = 0.0994\n","08/05/2022 10:24:37 - INFO - __main__ -   Train loss = 0.0992\n","08/05/2022 10:24:44 - INFO - __main__ -   Train loss = 0.0990\n","08/05/2022 10:24:50 - INFO - __main__ -   Train loss = 0.0988\n","08/05/2022 10:24:57 - INFO - __main__ -   Train loss = 0.0986\n","08/05/2022 10:25:04 - INFO - __main__ -   Train loss = 0.0985\n","08/05/2022 10:25:11 - INFO - __main__ -   Train loss = 0.0983\n","08/05/2022 10:25:11 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 10:25:11 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 10:25:11 - INFO - __main__ -     Batch size = 64\n","08/05/2022 10:25:21 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 10:25:21 - INFO - __main__ -     accuracy = 93.3282\n","08/05/2022 10:25:21 - INFO - __main__ -     loss = 0.4599\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 10:25:27 - INFO - __main__ -   Train loss = 0.0982\n","08/05/2022 10:25:34 - INFO - __main__ -   Train loss = 0.0980\n","08/05/2022 10:25:41 - INFO - __main__ -   Train loss = 0.0978\n","08/05/2022 10:25:47 - INFO - __main__ -   Train loss = 0.0977\n","08/05/2022 10:25:54 - INFO - __main__ -   Train loss = 0.0976\n","08/05/2022 10:26:01 - INFO - __main__ -   Train loss = 0.0974\n","08/05/2022 10:26:07 - INFO - __main__ -   Train loss = 0.0973\n","08/05/2022 10:26:14 - INFO - __main__ -   Train loss = 0.0971\n","08/05/2022 10:26:21 - INFO - __main__ -   Train loss = 0.0970\n","08/05/2022 10:26:27 - INFO - __main__ -   Train loss = 0.0968\n","08/05/2022 10:26:34 - INFO - __main__ -   Train loss = 0.0966\n","08/05/2022 10:26:41 - INFO - __main__ -   Train loss = 0.0965\n","08/05/2022 10:26:48 - INFO - __main__ -   Train loss = 0.0964\n","08/05/2022 10:26:54 - INFO - __main__ -   Train loss = 0.0962\n","08/05/2022 10:27:01 - INFO - __main__ -   Train loss = 0.0960\n","08/05/2022 10:27:08 - INFO - __main__ -   Train loss = 0.0958\n","08/05/2022 10:27:14 - INFO - __main__ -   Train loss = 0.0957\n","08/05/2022 10:27:21 - INFO - __main__ -   Train loss = 0.0955\n","08/05/2022 10:27:28 - INFO - __main__ -   Train loss = 0.0953\n","08/05/2022 10:27:34 - INFO - __main__ -   Train loss = 0.0952\n","08/05/2022 10:27:34 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 10:27:34 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 10:27:34 - INFO - __main__ -     Batch size = 64\n","08/05/2022 10:27:44 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 10:27:44 - INFO - __main__ -     accuracy = 93.6350\n","08/05/2022 10:27:44 - INFO - __main__ -     loss = 0.4183\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 10:27:51 - INFO - __main__ -   Train loss = 0.0950\n","08/05/2022 10:27:58 - INFO - __main__ -   Train loss = 0.0948\n","08/05/2022 10:28:04 - INFO - __main__ -   Train loss = 0.0947\n","08/05/2022 10:28:11 - INFO - __main__ -   Train loss = 0.0945\n","08/05/2022 10:28:18 - INFO - __main__ -   Train loss = 0.0943\n","08/05/2022 10:28:25 - INFO - __main__ -   Train loss = 0.0942\n","08/05/2022 10:28:31 - INFO - __main__ -   Train loss = 0.0940\n","08/05/2022 10:28:38 - INFO - __main__ -   Train loss = 0.0938\n","08/05/2022 10:28:45 - INFO - __main__ -   Train loss = 0.0937\n","08/05/2022 10:28:51 - INFO - __main__ -   Train loss = 0.0935\n","08/05/2022 10:28:58 - INFO - __main__ -   Train loss = 0.0933\n","08/05/2022 10:29:05 - INFO - __main__ -   Train loss = 0.0931\n","08/05/2022 10:29:11 - INFO - __main__ -   Train loss = 0.0930\n","08/05/2022 10:29:18 - INFO - __main__ -   Train loss = 0.0928\n","08/05/2022 10:29:25 - INFO - __main__ -   Train loss = 0.0927\n","08/05/2022 10:29:32 - INFO - __main__ -   Train loss = 0.0925\n","08/05/2022 10:29:38 - INFO - __main__ -   Train loss = 0.0924\n","08/05/2022 10:29:45 - INFO - __main__ -   Train loss = 0.0922\n","08/05/2022 10:29:52 - INFO - __main__ -   Train loss = 0.0920\n","08/05/2022 10:29:59 - INFO - __main__ -   Train loss = 0.0919\n","08/05/2022 10:29:59 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 10:29:59 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 10:29:59 - INFO - __main__ -     Batch size = 64\n","08/05/2022 10:30:08 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 10:30:08 - INFO - __main__ -     accuracy = 93.4816\n","08/05/2022 10:30:08 - INFO - __main__ -     loss = 0.4280\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 10:30:15 - INFO - __main__ -   Train loss = 0.0918\n","08/05/2022 10:30:22 - INFO - __main__ -   Train loss = 0.0916\n","08/05/2022 10:30:29 - INFO - __main__ -   Train loss = 0.0914\n","08/05/2022 10:30:35 - INFO - __main__ -   Train loss = 0.0913\n","08/05/2022 10:30:42 - INFO - __main__ -   Train loss = 0.0911\n","08/05/2022 10:30:49 - INFO - __main__ -   Train loss = 0.0910\n","08/05/2022 10:30:55 - INFO - __main__ -   Train loss = 0.0908\n","08/05/2022 10:31:02 - INFO - __main__ -   Train loss = 0.0907\n","08/05/2022 10:31:09 - INFO - __main__ -   Train loss = 0.0905\n","08/05/2022 10:31:16 - INFO - __main__ -   Train loss = 0.0904\n","08/05/2022 10:31:22 - INFO - __main__ -   Train loss = 0.0902\n","08/05/2022 10:31:29 - INFO - __main__ -   Train loss = 0.0901\n","08/05/2022 10:31:36 - INFO - __main__ -   Train loss = 0.0900\n","08/05/2022 10:31:42 - INFO - __main__ -   Train loss = 0.0899\n","08/05/2022 10:31:49 - INFO - __main__ -   Train loss = 0.0897\n","08/05/2022 10:31:56 - INFO - __main__ -   Train loss = 0.0895\n","08/05/2022 10:32:02 - INFO - __main__ -   Train loss = 0.0894\n","08/05/2022 10:32:06 - INFO - __main__ -   train_acc = 99.8311\n","Epoch:  80%|████████  | 8/10 [1:09:25<17:18, 519.04s/it]08/05/2022 10:32:09 - INFO - __main__ -   Train loss = 0.0892\n","08/05/2022 10:32:16 - INFO - __main__ -   Train loss = 0.0891\n","08/05/2022 10:32:22 - INFO - __main__ -   Train loss = 0.0889\n","08/05/2022 10:32:22 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 10:32:22 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 10:32:22 - INFO - __main__ -     Batch size = 64\n","08/05/2022 10:32:32 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 10:32:32 - INFO - __main__ -     accuracy = 93.7883\n","08/05/2022 10:32:32 - INFO - __main__ -     loss = 0.4159\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 10:32:39 - INFO - __main__ -   Train loss = 0.0888\n","08/05/2022 10:32:46 - INFO - __main__ -   Train loss = 0.0887\n","08/05/2022 10:32:52 - INFO - __main__ -   Train loss = 0.0885\n","08/05/2022 10:32:59 - INFO - __main__ -   Train loss = 0.0884\n","08/05/2022 10:33:06 - INFO - __main__ -   Train loss = 0.0883\n","08/05/2022 10:33:13 - INFO - __main__ -   Train loss = 0.0881\n","08/05/2022 10:33:19 - INFO - __main__ -   Train loss = 0.0880\n","08/05/2022 10:33:26 - INFO - __main__ -   Train loss = 0.0878\n","08/05/2022 10:33:33 - INFO - __main__ -   Train loss = 0.0877\n","08/05/2022 10:33:39 - INFO - __main__ -   Train loss = 0.0875\n","08/05/2022 10:33:46 - INFO - __main__ -   Train loss = 0.0874\n","08/05/2022 10:33:53 - INFO - __main__ -   Train loss = 0.0872\n","08/05/2022 10:34:00 - INFO - __main__ -   Train loss = 0.0871\n","08/05/2022 10:34:06 - INFO - __main__ -   Train loss = 0.0870\n","08/05/2022 10:34:13 - INFO - __main__ -   Train loss = 0.0868\n","08/05/2022 10:34:20 - INFO - __main__ -   Train loss = 0.0867\n","08/05/2022 10:34:26 - INFO - __main__ -   Train loss = 0.0866\n","08/05/2022 10:34:33 - INFO - __main__ -   Train loss = 0.0864\n","08/05/2022 10:34:40 - INFO - __main__ -   Train loss = 0.0863\n","08/05/2022 10:34:46 - INFO - __main__ -   Train loss = 0.0862\n","08/05/2022 10:34:46 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 10:34:46 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 10:34:46 - INFO - __main__ -     Batch size = 64\n","08/05/2022 10:34:56 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 10:34:56 - INFO - __main__ -     accuracy = 92.7914\n","08/05/2022 10:34:56 - INFO - __main__ -     loss = 0.4479\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 10:35:03 - INFO - __main__ -   Train loss = 0.0861\n","08/05/2022 10:35:10 - INFO - __main__ -   Train loss = 0.0860\n","08/05/2022 10:35:16 - INFO - __main__ -   Train loss = 0.0858\n","08/05/2022 10:35:23 - INFO - __main__ -   Train loss = 0.0857\n","08/05/2022 10:35:30 - INFO - __main__ -   Train loss = 0.0856\n","08/05/2022 10:35:37 - INFO - __main__ -   Train loss = 0.0854\n","08/05/2022 10:35:43 - INFO - __main__ -   Train loss = 0.0853\n","08/05/2022 10:35:50 - INFO - __main__ -   Train loss = 0.0851\n","08/05/2022 10:35:57 - INFO - __main__ -   Train loss = 0.0851\n","08/05/2022 10:36:03 - INFO - __main__ -   Train loss = 0.0850\n","08/05/2022 10:36:10 - INFO - __main__ -   Train loss = 0.0848\n","08/05/2022 10:36:17 - INFO - __main__ -   Train loss = 0.0847\n","08/05/2022 10:36:23 - INFO - __main__ -   Train loss = 0.0845\n","08/05/2022 10:36:30 - INFO - __main__ -   Train loss = 0.0844\n","08/05/2022 10:36:37 - INFO - __main__ -   Train loss = 0.0843\n","08/05/2022 10:36:43 - INFO - __main__ -   Train loss = 0.0841\n","08/05/2022 10:36:50 - INFO - __main__ -   Train loss = 0.0840\n","08/05/2022 10:36:57 - INFO - __main__ -   Train loss = 0.0839\n","08/05/2022 10:37:03 - INFO - __main__ -   Train loss = 0.0838\n","08/05/2022 10:37:10 - INFO - __main__ -   Train loss = 0.0837\n","08/05/2022 10:37:10 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 10:37:10 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 10:37:10 - INFO - __main__ -     Batch size = 64\n","08/05/2022 10:37:20 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 10:37:20 - INFO - __main__ -     accuracy = 93.4049\n","08/05/2022 10:37:20 - INFO - __main__ -     loss = 0.4454\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 10:37:27 - INFO - __main__ -   Train loss = 0.0835\n","08/05/2022 10:37:33 - INFO - __main__ -   Train loss = 0.0834\n","08/05/2022 10:37:40 - INFO - __main__ -   Train loss = 0.0833\n","08/05/2022 10:37:47 - INFO - __main__ -   Train loss = 0.0831\n","08/05/2022 10:37:53 - INFO - __main__ -   Train loss = 0.0830\n","08/05/2022 10:38:00 - INFO - __main__ -   Train loss = 0.0829\n","08/05/2022 10:38:07 - INFO - __main__ -   Train loss = 0.0827\n","08/05/2022 10:38:13 - INFO - __main__ -   Train loss = 0.0826\n","08/05/2022 10:38:20 - INFO - __main__ -   Train loss = 0.0825\n","08/05/2022 10:38:27 - INFO - __main__ -   Train loss = 0.0824\n","08/05/2022 10:38:33 - INFO - __main__ -   Train loss = 0.0822\n","08/05/2022 10:38:40 - INFO - __main__ -   Train loss = 0.0821\n","08/05/2022 10:38:47 - INFO - __main__ -   Train loss = 0.0820\n","08/05/2022 10:38:53 - INFO - __main__ -   Train loss = 0.0818\n","08/05/2022 10:39:00 - INFO - __main__ -   Train loss = 0.0817\n","08/05/2022 10:39:07 - INFO - __main__ -   Train loss = 0.0816\n","08/05/2022 10:39:13 - INFO - __main__ -   Train loss = 0.0815\n","08/05/2022 10:39:20 - INFO - __main__ -   Train loss = 0.0814\n","08/05/2022 10:39:27 - INFO - __main__ -   Train loss = 0.0813\n","08/05/2022 10:39:34 - INFO - __main__ -   Train loss = 0.0812\n","08/05/2022 10:39:34 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 10:39:34 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 10:39:34 - INFO - __main__ -     Batch size = 64\n","08/05/2022 10:39:43 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 10:39:43 - INFO - __main__ -     accuracy = 93.6350\n","08/05/2022 10:39:43 - INFO - __main__ -     loss = 0.4345\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 10:39:50 - INFO - __main__ -   Train loss = 0.0811\n","08/05/2022 10:39:57 - INFO - __main__ -   Train loss = 0.0809\n","08/05/2022 10:40:03 - INFO - __main__ -   Train loss = 0.0808\n","08/05/2022 10:40:10 - INFO - __main__ -   Train loss = 0.0807\n","08/05/2022 10:40:17 - INFO - __main__ -   Train loss = 0.0806\n","08/05/2022 10:40:23 - INFO - __main__ -   Train loss = 0.0804\n","08/05/2022 10:40:30 - INFO - __main__ -   Train loss = 0.0803\n","08/05/2022 10:40:37 - INFO - __main__ -   Train loss = 0.0802\n","08/05/2022 10:40:43 - INFO - __main__ -   Train loss = 0.0801\n","08/05/2022 10:40:49 - INFO - __main__ -   train_acc = 99.8701\n","Epoch:  90%|█████████ | 9/10 [1:18:08<08:40, 520.04s/it]08/05/2022 10:40:50 - INFO - __main__ -   Train loss = 0.0800\n","08/05/2022 10:40:57 - INFO - __main__ -   Train loss = 0.0798\n","08/05/2022 10:41:03 - INFO - __main__ -   Train loss = 0.0797\n","08/05/2022 10:41:10 - INFO - __main__ -   Train loss = 0.0796\n","08/05/2022 10:41:17 - INFO - __main__ -   Train loss = 0.0795\n","08/05/2022 10:41:23 - INFO - __main__ -   Train loss = 0.0793\n","08/05/2022 10:41:30 - INFO - __main__ -   Train loss = 0.0792\n","08/05/2022 10:41:37 - INFO - __main__ -   Train loss = 0.0791\n","08/05/2022 10:41:43 - INFO - __main__ -   Train loss = 0.0790\n","08/05/2022 10:41:50 - INFO - __main__ -   Train loss = 0.0789\n","08/05/2022 10:41:57 - INFO - __main__ -   Train loss = 0.0788\n","08/05/2022 10:41:57 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 10:41:57 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 10:41:57 - INFO - __main__ -     Batch size = 64\n","08/05/2022 10:42:06 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 10:42:06 - INFO - __main__ -     accuracy = 93.7117\n","08/05/2022 10:42:06 - INFO - __main__ -     loss = 0.4298\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 10:42:13 - INFO - __main__ -   Train loss = 0.0787\n","08/05/2022 10:42:20 - INFO - __main__ -   Train loss = 0.0785\n","08/05/2022 10:42:26 - INFO - __main__ -   Train loss = 0.0784\n","08/05/2022 10:42:33 - INFO - __main__ -   Train loss = 0.0783\n","08/05/2022 10:42:40 - INFO - __main__ -   Train loss = 0.0782\n","08/05/2022 10:42:46 - INFO - __main__ -   Train loss = 0.0781\n","08/05/2022 10:42:53 - INFO - __main__ -   Train loss = 0.0780\n","08/05/2022 10:43:00 - INFO - __main__ -   Train loss = 0.0779\n","08/05/2022 10:43:06 - INFO - __main__ -   Train loss = 0.0778\n","08/05/2022 10:43:13 - INFO - __main__ -   Train loss = 0.0776\n","08/05/2022 10:43:20 - INFO - __main__ -   Train loss = 0.0775\n","08/05/2022 10:43:26 - INFO - __main__ -   Train loss = 0.0774\n","08/05/2022 10:43:33 - INFO - __main__ -   Train loss = 0.0773\n","08/05/2022 10:43:40 - INFO - __main__ -   Train loss = 0.0772\n","08/05/2022 10:43:46 - INFO - __main__ -   Train loss = 0.0771\n","08/05/2022 10:43:53 - INFO - __main__ -   Train loss = 0.0770\n","08/05/2022 10:44:00 - INFO - __main__ -   Train loss = 0.0769\n","08/05/2022 10:44:06 - INFO - __main__ -   Train loss = 0.0768\n","08/05/2022 10:44:13 - INFO - __main__ -   Train loss = 0.0767\n","08/05/2022 10:44:20 - INFO - __main__ -   Train loss = 0.0765\n","08/05/2022 10:44:20 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 10:44:20 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 10:44:20 - INFO - __main__ -     Batch size = 64\n","08/05/2022 10:44:30 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 10:44:30 - INFO - __main__ -     accuracy = 93.4049\n","08/05/2022 10:44:30 - INFO - __main__ -     loss = 0.4463\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 10:44:36 - INFO - __main__ -   Train loss = 0.0764\n","08/05/2022 10:44:43 - INFO - __main__ -   Train loss = 0.0763\n","08/05/2022 10:44:50 - INFO - __main__ -   Train loss = 0.0762\n","08/05/2022 10:44:56 - INFO - __main__ -   Train loss = 0.0761\n","08/05/2022 10:45:03 - INFO - __main__ -   Train loss = 0.0760\n","08/05/2022 10:45:10 - INFO - __main__ -   Train loss = 0.0759\n","08/05/2022 10:45:16 - INFO - __main__ -   Train loss = 0.0758\n","08/05/2022 10:45:23 - INFO - __main__ -   Train loss = 0.0757\n","08/05/2022 10:45:30 - INFO - __main__ -   Train loss = 0.0756\n","08/05/2022 10:45:36 - INFO - __main__ -   Train loss = 0.0755\n","08/05/2022 10:45:43 - INFO - __main__ -   Train loss = 0.0754\n","08/05/2022 10:45:50 - INFO - __main__ -   Train loss = 0.0753\n","08/05/2022 10:45:56 - INFO - __main__ -   Train loss = 0.0752\n","08/05/2022 10:46:03 - INFO - __main__ -   Train loss = 0.0751\n","08/05/2022 10:46:10 - INFO - __main__ -   Train loss = 0.0749\n","08/05/2022 10:46:16 - INFO - __main__ -   Train loss = 0.0748\n","08/05/2022 10:46:23 - INFO - __main__ -   Train loss = 0.0747\n","08/05/2022 10:46:30 - INFO - __main__ -   Train loss = 0.0746\n","08/05/2022 10:46:36 - INFO - __main__ -   Train loss = 0.0745\n","08/05/2022 10:46:43 - INFO - __main__ -   Train loss = 0.0744\n","08/05/2022 10:46:43 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 10:46:43 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 10:46:43 - INFO - __main__ -     Batch size = 64\n","08/05/2022 10:46:53 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 10:46:53 - INFO - __main__ -     accuracy = 93.7883\n","08/05/2022 10:46:53 - INFO - __main__ -     loss = 0.4349\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 10:46:59 - INFO - __main__ -   Train loss = 0.0743\n","08/05/2022 10:47:06 - INFO - __main__ -   Train loss = 0.0742\n","08/05/2022 10:47:13 - INFO - __main__ -   Train loss = 0.0741\n","08/05/2022 10:47:19 - INFO - __main__ -   Train loss = 0.0740\n","08/05/2022 10:47:26 - INFO - __main__ -   Train loss = 0.0739\n","08/05/2022 10:47:33 - INFO - __main__ -   Train loss = 0.0738\n","08/05/2022 10:47:39 - INFO - __main__ -   Train loss = 0.0737\n","08/05/2022 10:47:46 - INFO - __main__ -   Train loss = 0.0736\n","08/05/2022 10:47:53 - INFO - __main__ -   Train loss = 0.0735\n","08/05/2022 10:47:59 - INFO - __main__ -   Train loss = 0.0734\n","08/05/2022 10:48:06 - INFO - __main__ -   Train loss = 0.0733\n","08/05/2022 10:48:13 - INFO - __main__ -   Train loss = 0.0732\n","08/05/2022 10:48:19 - INFO - __main__ -   Train loss = 0.0731\n","08/05/2022 10:48:26 - INFO - __main__ -   Train loss = 0.0730\n","08/05/2022 10:48:33 - INFO - __main__ -   Train loss = 0.0729\n","08/05/2022 10:48:39 - INFO - __main__ -   Train loss = 0.0728\n","08/05/2022 10:48:46 - INFO - __main__ -   Train loss = 0.0727\n","08/05/2022 10:48:53 - INFO - __main__ -   Train loss = 0.0726\n","08/05/2022 10:48:59 - INFO - __main__ -   Train loss = 0.0725\n","08/05/2022 10:49:06 - INFO - __main__ -   Train loss = 0.0724\n","08/05/2022 10:49:06 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 10:49:06 - INFO - __main__ -     Num examples = 1304\n","08/05/2022 10:49:06 - INFO - __main__ -     Batch size = 64\n","08/05/2022 10:49:16 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 10:49:16 - INFO - __main__ -     accuracy = 93.7883\n","08/05/2022 10:49:16 - INFO - __main__ -     loss = 0.4343\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 10:49:22 - INFO - __main__ -   Train loss = 0.0723\n","08/05/2022 10:49:29 - INFO - __main__ -   Train loss = 0.0722\n","08/05/2022 10:49:29 - INFO - __main__ -   train_acc = 99.9437\n","Epoch: 100%|██████████| 10/10 [1:26:48<00:00, 520.86s/it]\n","08/05/2022 10:49:29 - INFO - __main__ -   \n","***** Running evaluation on test dataset *****\n","08/05/2022 10:49:29 - INFO - __main__ -     Num examples = 2126\n","08/05/2022 10:49:29 - INFO - __main__ -     Batch size = 64\n","08/05/2022 10:49:45 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 10:49:45 - INFO - __main__ -     accuracy = 93.5089\n","08/05/2022 10:49:45 - INFO - __main__ -     loss = 0.5002\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n","              precision    recall  f1-score   support\n","\n","     neutral     0.9477    0.9447    0.9462      1284\n","     entails     0.9161    0.9204    0.9182       842\n","\n","    accuracy                         0.9351      2126\n","   macro avg     0.9319    0.9326    0.9322      2126\n","weighted avg     0.9351    0.9351    0.9351      2126\n","\n"]}],"source":["#all\n","if args.do_train:\n","    # trainer.load_model()\n","    trainer.train()\n","\n","if args.do_eval:\n","    # trainer.load_model()\n","    trainer.evaluate(\"test\")"]},{"cell_type":"code","source":["import pickle\n","with open(f'Entailment_prob_per_{NUM_EPOCHS}epochs.pkl', 'wb') as f:\n","    pickle.dump(Prob_per_epoch, f)"],"metadata":{"id":"yhWy7wbWTWhx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Prob_per_epoch.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o5APiWq0S_T-","executionInfo":{"status":"ok","timestamp":1659696670163,"user_tz":-120,"elapsed":4,"user":{"displayName":"AmirHossein Dabiri Aghdam","userId":"03377875757005762455"}},"outputId":"4aba87a2-d64b-421e-e6e1-ec0fa915387e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10, 23097, 2)"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["Prob_per_epoch[1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uWsPicO2TBH0","executionInfo":{"status":"ok","timestamp":1659696669527,"user_tz":-120,"elapsed":238,"user":{"displayName":"AmirHossein Dabiri Aghdam","userId":"03377875757005762455"}},"outputId":"87c154aa-b30d-4692-bfc7-369f6955bcbc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.0240559 , 0.97594404],\n","       [0.0307784 , 0.96922159],\n","       [0.98828053, 0.01171952],\n","       ...,\n","       [0.76659113, 0.23340888],\n","       [0.08912375, 0.91087621],\n","       [0.06238998, 0.93760997]])"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JIOazWmCumuf"},"outputs":[],"source":["# # Save the model to drive\n","# !cp /content/Textual_Entailement_model/pytorch_model.bin /content/drive/MyDrive/Textual_Entailement_model\n","# !cp /content/Textual_Entailement_model/config.json /content/drive/MyDrive/Textual_Entailement_model"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[{"file_id":"1UbAth0ImdSdle9I6aPfqUIg667sxTGf_","timestamp":1659397233383}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"f13b5142164b4713893b9ad346cac1a6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_07491f0f915a4dfb94a59f71beef609b","IPY_MODEL_a533081adb344ecb9ed11ad92435f90a","IPY_MODEL_418c3ec7af8b4b1caad2a879b7a83dca"],"layout":"IPY_MODEL_63c5350f33af45ada71f7a14ba0ba324"}},"07491f0f915a4dfb94a59f71beef609b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d2c8425505a74d30b20f79924fe363aa","placeholder":"​","style":"IPY_MODEL_f46eda44964640baa28ee568db4b7b42","value":"100%"}},"a533081adb344ecb9ed11ad92435f90a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_474f7378c06d413c9b1723473e9f5b41","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9bbd95b977ba4c5b8f2dbae61612ac09","value":3}},"418c3ec7af8b4b1caad2a879b7a83dca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1729294c101b4b528bc7dfefdb5d6450","placeholder":"​","style":"IPY_MODEL_3ea50171e2fd41d8bc6004cef5fbf619","value":" 3/3 [00:00&lt;00:00, 61.99it/s]"}},"63c5350f33af45ada71f7a14ba0ba324":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2c8425505a74d30b20f79924fe363aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f46eda44964640baa28ee568db4b7b42":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"474f7378c06d413c9b1723473e9f5b41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bbd95b977ba4c5b8f2dbae61612ac09":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1729294c101b4b528bc7dfefdb5d6450":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ea50171e2fd41d8bc6004cef5fbf619":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8dd70cf61b354f519a6153319cb68a6f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a29add93b522478da7de3a994dadfa2f","IPY_MODEL_41578e2ce36c4630a4a88b942b827b42","IPY_MODEL_4269806b0df949c3980f7097af8bf45f"],"layout":"IPY_MODEL_1ae20b9019834d3c8fe3ccf5b48f434f"}},"a29add93b522478da7de3a994dadfa2f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_614183020766488b859d63e022e57d07","placeholder":"​","style":"IPY_MODEL_94d704149b304961b4a0591d9a29a93d","value":"Downloading vocab.txt: 100%"}},"41578e2ce36c4630a4a88b942b827b42":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7563fdaf2be8448eaeda8101281ee837","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8a1b3386f2b7451ca5e07592921fb17a","value":231508}},"4269806b0df949c3980f7097af8bf45f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4ecbb0ec3a745e3b12efe4b23e68a02","placeholder":"​","style":"IPY_MODEL_b79e83ef8d5347e3b8ef20c9afd94bcd","value":" 226k/226k [00:00&lt;00:00, 8.36kB/s]"}},"1ae20b9019834d3c8fe3ccf5b48f434f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"614183020766488b859d63e022e57d07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94d704149b304961b4a0591d9a29a93d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7563fdaf2be8448eaeda8101281ee837":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a1b3386f2b7451ca5e07592921fb17a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e4ecbb0ec3a745e3b12efe4b23e68a02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b79e83ef8d5347e3b8ef20c9afd94bcd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5de6c0fbcc89499b8f8c77784d6027a6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fd003972d64749b7a62dfecd5b8bc009","IPY_MODEL_840ffebd01c84cfb8e43ca339943384b","IPY_MODEL_7e5d7c75991d490084a95254ea2762cc"],"layout":"IPY_MODEL_911031d910d44a339e22a1ac8f168f3b"}},"fd003972d64749b7a62dfecd5b8bc009":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5373fcb4de7945968134dc3dacf03139","placeholder":"​","style":"IPY_MODEL_7a523a3f056c4dd8b3ffafc0155d248b","value":"Downloading tokenizer_config.json: 100%"}},"840ffebd01c84cfb8e43ca339943384b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e9bf8ee0eee4f6f9eae66c926ca375b","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_50be468b69d34abba5ea86b4033bfe22","value":28}},"7e5d7c75991d490084a95254ea2762cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45e5489c15a24240a08978da3eee5305","placeholder":"​","style":"IPY_MODEL_fa26fcfdd13547a8ad278aaf186947ca","value":" 28.0/28.0 [00:00&lt;00:00, 388B/s]"}},"911031d910d44a339e22a1ac8f168f3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5373fcb4de7945968134dc3dacf03139":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a523a3f056c4dd8b3ffafc0155d248b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e9bf8ee0eee4f6f9eae66c926ca375b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50be468b69d34abba5ea86b4033bfe22":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"45e5489c15a24240a08978da3eee5305":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa26fcfdd13547a8ad278aaf186947ca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c5a2fbce0a5747789d28ac77c7d5ac14":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_778bea3bf5544c119eb1dd563f364670","IPY_MODEL_856d05e40cdf415cbdea7d449ae19bca","IPY_MODEL_c5444b6fc37f4a828a7070a6cf0fede1"],"layout":"IPY_MODEL_b7a479ccc3d1490d99e4b89f69312ea8"}},"778bea3bf5544c119eb1dd563f364670":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4219c848ee074e72a5798c058449d3d3","placeholder":"​","style":"IPY_MODEL_1c2ffa89b0684b2793455ff9bf7e1b90","value":"Downloading config.json: 100%"}},"856d05e40cdf415cbdea7d449ae19bca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8bf4c38aae384c0eac0ae1e35724cea4","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2ffe6a7231d9456faf199478f1f4cf9b","value":570}},"c5444b6fc37f4a828a7070a6cf0fede1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d2bbe00cf48b4f48a9b0bdb6963dcde2","placeholder":"​","style":"IPY_MODEL_9f6456a9ae4c4e5ca42405d2b5e7f363","value":" 570/570 [00:00&lt;00:00, 6.09kB/s]"}},"b7a479ccc3d1490d99e4b89f69312ea8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4219c848ee074e72a5798c058449d3d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c2ffa89b0684b2793455ff9bf7e1b90":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8bf4c38aae384c0eac0ae1e35724cea4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ffe6a7231d9456faf199478f1f4cf9b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d2bbe00cf48b4f48a9b0bdb6963dcde2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f6456a9ae4c4e5ca42405d2b5e7f363":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}