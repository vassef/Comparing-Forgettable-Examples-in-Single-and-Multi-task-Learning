{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N_4uNzDnM3qy","executionInfo":{"status":"ok","timestamp":1659892102700,"user_tz":-270,"elapsed":4071,"user":{"displayName":"shayan vassef","userId":"08909168472496340813"}},"outputId":"0fe60e93-ee2d-4e8d-8622-73589e2ba5a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y5wau-_3zTpo"},"outputs":[],"source":["!pip install -q datasets transformers sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_2_D7Xqzt4MN"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import datasets\n","from datasets import Dataset, DatasetDict\n","import os\n","import random\n","import regex as re\n","import logging\n","import gc\n","from tqdm import tqdm, trange\n","from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report\n","from datasets import load_dataset\n","import torch\n","import torch.nn as nn\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n","from transformers import XLMRobertaTokenizer\n","from transformers.models.xlm_roberta.modeling_xlm_roberta import  XLMRobertaModel, XLMRobertaConfig\n","from transformers import BertConfig, BertTokenizer, BertModel\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","plt.style.use('seaborn')"]},{"cell_type":"markdown","source":[""],"metadata":{"id":"HYmZcZKPC6zL"}},{"cell_type":"code","source":["#dataset = DatasetDict()\n","# dataset['train'] = load_dataset('glue', 'mrpc', split='train')\n","# dataset['test'] = load_dataset('glue', 'mrpc', split='test')"],"metadata":{"id":"eOR2zMPXy8RD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","dataset = load_dataset(\"Shayanvsf/US_Airline_Sentiment\")\n","dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":361,"referenced_widgets":["49078982e650422098f54e69d2ac509a","b9dce4e8b6294a669d7dda51956aea8f","12309434c9ac4454a6469befcd110532","21084e7aed8e436894be984ed723bae2","065b60661e1f448bb74a61e8ad96f6e8","22dca9f8a6284e0e8f037d135207ce51","40f9833f108d49b2936c865e03cb6037","9d3a99548f0b47398fc4d68e511fa9a2","8b44bf7dd9d74bba994917dc036c1157","ffd56f7b5ff84923907c9c8fd4341b54","e252fac95e4b49a49779c0246c7202c9"]},"id":"pGX_U0ulFc6K","executionInfo":{"status":"ok","timestamp":1659892113147,"user_tz":-270,"elapsed":5820,"user":{"displayName":"shayan vassef","userId":"08909168472496340813"}},"outputId":"4189615a-2162-4464-80f6-b36d07f6052f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["08/07/2022 17:08:31 - WARNING - datasets.builder -   Using custom data configuration Shayanvsf--US_Airline_Sentiment-a5a7209e33aa0ee7\n","08/07/2022 17:08:31 - WARNING - datasets.builder -   Reusing dataset parquet (/root/.cache/huggingface/datasets/Shayanvsf___parquet/Shayanvsf--US_Airline_Sentiment-a5a7209e33aa0ee7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49078982e650422098f54e69d2ac509a"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    validation: Dataset({\n","        features: ['airline_sentiment', 'airline_sentiment_confidence', 'negativereason_confidence', 'text'],\n","        num_rows: 2308\n","    })\n","    train: Dataset({\n","        features: ['airline_sentiment', 'airline_sentiment_confidence', 'negativereason_confidence', 'text'],\n","        num_rows: 8078\n","    })\n","    test: Dataset({\n","        features: ['airline_sentiment', 'airline_sentiment_confidence', 'negativereason_confidence', 'text'],\n","        num_rows: 1155\n","    })\n","})"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["df_train = pd.DataFrame.from_dict(dataset['train'])\n","df_train = df_train.sample(frac=1)\n","df_train_random = df_train[0:223]\n","dataset['train'] = Dataset.from_pandas(df_train_random )"],"metadata":{"id":"DFql99fCRDeu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from torch.utils.data import Subset\n","# dataset['train'] = dataset['train'].shuffle(seed=42)\n","# dataset['train'] = Subset(dataset[\"train\"], list(range(223)))"],"metadata":{"id":"VoTEhNUcS5D8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Paraphrase_Forgettables_Info = pd.read_csv(\"/content/drive/MyDrive/NLP Bachelors' Project/Sentiment_Forgettables_Info.csv\")\n","# Paraphrase_Forgettables_Info"],"metadata":{"id":"EnCg8yzCCvT7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dataset['train'] = Dataset.from_pandas(pd.DataFrame.from_dict([dataset['train'][idx] for idx in Paraphrase_Forgettables_Info['sample_index']]))\n","# dataset['train']"],"metadata":{"id":"LiKa3lDkDSfr"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":365},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1659892113149,"user":{"displayName":"shayan vassef","userId":"08909168472496340813"},"user_tz":-270},"id":"cDwxzAdY677O","outputId":"fc44c859-7e36-46d0-f0bc-d873b04f4ff0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 0.7927927927927928, 1: 0.21171171171171171}"]},"metadata":{},"execution_count":67},{"output_type":"display_data","data":{"text/plain":["<Figure size 576x396 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["freq_dict = dict()\n","for data in dataset['train']:\n","    if (data['airline_sentiment'] in freq_dict): freq_dict[data['airline_sentiment']] += 1\n","    else: freq_dict[data['airline_sentiment']] = 1\n","\n","for k,v in freq_dict.items():\n","    freq_dict[k] = v / (len(dataset['train']) - 1)\n","\n","sns.countplot(x=dataset['train']['airline_sentiment'])\n","freq_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":365},"executionInfo":{"elapsed":1173,"status":"ok","timestamp":1659892114312,"user":{"displayName":"shayan vassef","userId":"08909168472496340813"},"user_tz":-270},"id":"CErTCZLp6lAY","outputId":"ea2b2d75-0350-4a58-96f6-5b977694e7be"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 0.800520381613183, 1: 0.20121422376409367}"]},"metadata":{},"execution_count":68},{"output_type":"display_data","data":{"text/plain":["<Figure size 576x396 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["freq_dict = dict()\n","for data in dataset['test']:\n","    if (data['airline_sentiment'] in freq_dict): freq_dict[data['airline_sentiment']] += 1\n","    else: freq_dict[data['airline_sentiment']] = 1\n","\n","for k,v in freq_dict.items():\n","    freq_dict[k] = v / (len(dataset['test']) - 2)\n","\n","sns.countplot(x=dataset['test']['airline_sentiment'])\n","freq_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NrJ4LDo2AEoy"},"outputs":[],"source":["class Textual_EntailementClassifier(nn.Module):\n","    def __init__(self, input_dim, num_labels=3, dropout_rate=0.):\n","        super(Textual_EntailementClassifier, self).__init__()\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.linear1 = nn.Linear(input_dim, input_dim // 3)\n","        self.relu = nn.ReLU()\n","        self.linear2 = nn.Linear(input_dim // 3, num_labels)\n","\n","    def forward(self, x):\n","        x = self.dropout(x)\n","        x = self.linear1(x)\n","        x = self.relu(x)\n","        return self.linear2(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qX9IcSy-Yzhk"},"outputs":[],"source":["class BERT_Textual_Entailement(BertModel):\n","    def __init__(self, config, args, label_lst):\n","        super(BERT_Textual_Entailement, self).__init__(config)\n","        self.args = args\n","        self.num_labels = len(set(label_lst))\n","        self.bert = BertModel(config=config) # Load pretrained Bert\n","        self.classifier = Textual_EntailementClassifier(config.hidden_size, self.num_labels, args.dropout_rate)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids, label_ids):\n","        outputs = self.bert(input_ids, attention_mask, token_type_ids)  # sequence_output, pooled_output, (hidden_states), (attentions)\n","        pooled_output = outputs[1]  # [CLS]\n","        label_logits = self.classifier(pooled_output)\n","\n","        total_loss = 0\n","        # Softmax\n","        if label_ids is not None:\n","            loss = 0\n","            if self.num_labels == 1:\n","                loss_fct = nn.MSELoss()\n","                loss = loss_fct(label_logits.view(-1), label_ids.view(-1))\n","            else:\n","                loss_fct = nn.CrossEntropyLoss()\n","                loss = loss_fct(label_logits.view(-1, self.num_labels), label_ids.view(-1))\n","            total_loss += loss\n","\n","        outputs = (label_logits,) + outputs[2:]  # add hidden states and attention if they are here\n","        outputs = (total_loss,) + outputs\n","\n","        return outputs  # (loss), logits, (hidden_states), (attentions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TT9SxAJmDY8_"},"outputs":[],"source":["class ParsBERT_Textual_Entailement(BertModel):\n","    def __init__(self, config, args, label_lst):\n","        super(ParsBERT_Textual_Entailement, self).__init__(config)\n","        self.args = args\n","        self.num_labels = len(set(label_lst))\n","        self.parsbert = BertModel(config=config) # Load pretrained ParsBert\n","        self.classifier = Textual_EntailementClassifier(config.hidden_size, self.num_labels, args.dropout_rate)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids, label_ids):\n","        outputs = self.parsbert(input_ids, attention_mask, token_type_ids)  # sequence_output, pooled_output, (hidden_states), (attentions)\n","        pooled_output = outputs[1]  # [CLS]\n","        label_logits = self.classifier(pooled_output)\n","\n","        total_loss = 0\n","        # Softmax\n","        if label_ids is not None:\n","            loss = 0\n","            if self.num_labels == 1:\n","                loss_fct = nn.MSELoss()\n","                loss = loss_fct(label_logits.view(-1), label_ids.view(-1))\n","            else:\n","                loss_fct = nn.CrossEntropyLoss()\n","                loss = loss_fct(label_logits.view(-1, self.num_labels), label_ids.view(-1))\n","            total_loss += loss\n","\n","        outputs = (label_logits,) + outputs[2:]  # add hidden states and attention if they are here\n","        outputs = (total_loss,) + outputs\n","\n","        return outputs  # (loss), logits, (hidden_states), (attentions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xRlNFn16WA4M"},"outputs":[],"source":["MODEL_CLASSES = {\n","    'bert': (BertConfig, BERT_Textual_Entailement, BertTokenizer),\n","    'parsbert': (BertConfig, ParsBERT_Textual_Entailement, BertTokenizer),\n","}\n","\n","MODEL_PATH_MAP = {\n","    # 'XLMRoberta': 'xlm-roberta-base',\n","    'bert': 'bert-base-uncased',\n","    'parsbert': 'HooshvareLab/bert-fa-zwnj-base',\n","}\n","\n","logger = logging.getLogger(__name__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FGZghD02Yzxv"},"outputs":[],"source":["def load_tokenizer(args):\n","    return MODEL_CLASSES[args.model_type][2].from_pretrained(args.model_name_or_path)\n","\n","def init_logger():\n","    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n","                        datefmt='%m/%d/%Y %H:%M:%S',\n","                        level=logging.INFO)\n","    \n","def label2index(label):\n","    if (label == 0): return 0\n","    elif (label == 1): return 1\n","    # else: return 2\n","    else: return label\n","\n","def get_labels(dataset):\n","  train_label = list()\n","  for data in dataset:\n","      if (data['airline_sentiment'] != 'xx' and data['airline_sentiment'] != '-'): train_label.append(data['airline_sentiment'])\n","  \n","  return train_label\n","\n","def set_seed(args):\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    if not args.no_cuda and torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(args.seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dhLxqH9fr4jR"},"outputs":[],"source":["def prepare_dataset(data, args, tokenizer, padding='max_length'):\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    all_label = torch.tensor([label2index(x['airline_sentiment']) for x in data if  (x['airline_sentiment'] != 'xx' and x['airline_sentiment'] != '-')], dtype=torch.long).to(device)\n","    bert_input = tokenizer([x['text'] for x in data if (x['airline_sentiment'] != 'xx' and x['airline_sentiment'] != '-')] , padding=padding, max_length=args.max_seq_len, truncation=True, return_tensors=\"pt\")\n","    if ('token_type_ids' in bert_input):\n","        dataset = TensorDataset(bert_input['input_ids'].to(device), bert_input['attention_mask'].to(device), bert_input['token_type_ids'].to(device), all_label)\n","    else:\n","        dataset = TensorDataset(bert_input['input_ids'].to(device), bert_input['attention_mask'].to(device), torch.zeros_like(bert_input['input_ids']).to(device), all_label)\n","\n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3IEGTfNm1GZG"},"outputs":[],"source":["#define\n","OUTPUT_DIM = 2\n","NUM_EPOCHS = 10\n","Prob_per_epoch = np.zeros((NUM_EPOCHS,len(dataset['train']),OUTPUT_DIM)) #2 labels\n","\n","class Trainer(object):\n","    def __init__(self, args, train_dataset=None, dev_dataset=None, test_dataset=None):\n","        self.args = args\n","        self.train_dataset = train_dataset\n","        self.dev_dataset = dev_dataset\n","        self.test_dataset = test_dataset\n","\n","        self.label_lst = get_labels(dataset['train'])\n","\n","        self.config_class, self.model_class, _ = MODEL_CLASSES[args.model_type]\n","        self.config = self.config_class.from_pretrained(args.model_name_or_path, finetuning_task=args.task)\n","        self.model = self.model_class.from_pretrained(args.model_name_or_path,\n","                                                      config=self.config,\n","                                                      args=args,\n","                                                      label_lst=self.label_lst)\n","                                                      \n","\n","        # GPU or CPU\n","        self.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n","        self.model.to(self.device)\n","\n","    def train(self):\n","        # train_sampler = RandomSampler(self.train_dataset)\n","        train_sampler = SequentialSampler(self.train_dataset)\n","        train_dataloader = DataLoader(self.train_dataset, sampler=train_sampler, batch_size=self.args.train_batch_size)\n","        if self.args.max_steps > 0:\n","            t_total = self.args.max_steps\n","            self.args.num_train_epochs = self.args.max_steps // (len(train_dataloader) // self.args.gradient_accumulation_steps) + 1\n","        else:\n","            t_total = len(train_dataloader) // self.args.gradient_accumulation_steps * self.args.num_train_epochs\n","\n","        # Prepare optimizer and schedule (linear warmup and decay)\n","        no_decay = ['bias', 'LayerNorm.weight']\n","        optimizer_grouped_parameters = [\n","            {'params': [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n","             'weight_decay': self.args.weight_decay},\n","            {'params': [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","        ]\n","        optimizer = AdamW(optimizer_grouped_parameters, lr=self.args.learning_rate, eps=self.args.adam_epsilon)\n","        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=t_total)\n","\n","        # Train!\n","        logger.info(\"***** Running training *****\")\n","        logger.info(\"  Num examples = %d\", len(self.train_dataset))\n","        logger.info(\"  Num Epochs = %d\", self.args.num_train_epochs)\n","        logger.info(\"  Total train batch size = %d\", self.args.train_batch_size)\n","        logger.info(\"  Gradient Accumulation steps = %d\", self.args.gradient_accumulation_steps)\n","        logger.info(\"  Total optimization steps = %d\", t_total)\n","        logger.info(\"  Logging steps = %d\", self.args.logging_steps)\n","        logger.info(\"  Save steps = %d\", self.args.save_steps)\n","\n","        global_step = 0\n","        tr_loss = 0.0\n","        self.model.zero_grad()\n","        best_acc = -1\n","       \n","        train_iterator = trange(int(self.args.num_train_epochs), desc=\"Epoch\")\n","        for epoch in train_iterator:\n","            label_preds = None\n","            out_label_ids = None\n","            # epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n","            stacked_train_preds = None\n","            for step, batch in enumerate(train_dataloader):\n","                self.model.train()\n","                inputs = {'input_ids': batch[0],\n","                          'attention_mask': batch[1],\n","                          'token_type_ids': batch[2],\n","                          'label_ids': batch[3]}\n","                outputs = self.model(**inputs)\n","                loss = outputs[0]\n","                label_logits = outputs[1]\n","\n","                # ############## Extracting samples'probability ##############\n","                # # Get the preds\n","                preds = nn.Softmax(dim=1)(label_logits) # convert to probability\n","\n","\n","                # # Move preds to the CPU\n","                train_preds = preds.detach().cpu().numpy()\n","                \n","                if stacked_train_preds is None:  # first batch\n","                    stacked_train_preds = train_preds\n","                else:\n","                  stacked_train_preds = np.vstack((stacked_train_preds, train_preds))\n","                \n","                # #######################  END ############################\n","\n","                 # label prediction\n","                if label_preds is None:\n","                    label_preds = label_logits.detach().cpu().numpy()\n","                    out_label_ids = inputs['label_ids'].detach().cpu().numpy()\n","                else:\n","                    label_preds = np.append(label_preds, label_logits.detach().cpu().numpy(), axis=0)\n","                    out_label_ids = np.append(out_label_ids, inputs['label_ids'].detach().cpu().numpy(), axis=0)\n","                \n","                if self.args.gradient_accumulation_steps > 1:\n","                    loss = loss / self.args.gradient_accumulation_steps\n","\n","                loss.backward()\n","\n","                tr_loss += loss.item()\n","                if (step + 1) % self.args.gradient_accumulation_steps == 0:\n","                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.max_grad_norm)\n","\n","                    optimizer.step()\n","                    scheduler.step()  # Update learning rate schedule\n","                    self.model.zero_grad()\n","                    global_step += 1\n","\n","                    if(global_step % 10 == 0): logger.info(\"Train loss = %.4f\", tr_loss / global_step)\n","                    if (self.args.logging_steps > 0 and global_step % self.args.logging_steps == 0):\n","                        results = self.evaluate(\"dev\")\n","                        if (float(results['accuracy']) > best_acc):\n","                        # if (float(results['slot_f1']) > best_f1):\n","                        #   best_f1 = float(results['slot_f1'])\n","                          best_acc = float(results['accuracy'])\n","                          logger.info(\"dev best_acc = %.4f\", best_acc * 100.0)\n","                          self.save_model()\n","\n","                   \n","            Prob_per_epoch[epoch][:,:] = stacked_train_preds\n","            \n","            if 0 < self.args.max_steps < global_step:\n","                train_iterator.close()\n","                break\n","            \n","            # label result\n","            label_preds = np.argmax(label_preds, axis=1)\n","            logger.info(\"train_acc = %.4f\", accuracy_score(out_label_ids, label_preds) * 100.0)\n","\n","\n","        return global_step, tr_loss / global_step\n","\n","    def evaluate(self, mode):\n","        if mode == 'test':\n","            dataset = self.test_dataset\n","        elif mode == 'dev':\n","            dataset = self.dev_dataset\n","        else:\n","            raise Exception(\"Only dev and test dataset available\")\n","\n","        eval_sampler = SequentialSampler(dataset)\n","        eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=self.args.eval_batch_size)\n","\n","        # Eval!\n","        logger.info(\"\\n***** Running evaluation on %s dataset *****\", mode)\n","        logger.info(\"  Num examples = %d\", len(dataset))\n","        logger.info(\"  Batch size = %d\", self.args.eval_batch_size)\n","        eval_loss = 0.0\n","        nb_eval_steps = 0\n","        label_preds = None\n","        out_label_ids = None\n","\n","        self.model.eval()\n","\n","        # for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","        for batch in eval_dataloader:\n","            batch = tuple(t.to(self.device) for t in batch)\n","            with torch.no_grad():\n","                inputs = {'input_ids': batch[0],\n","                          'attention_mask': batch[1],\n","                          'token_type_ids': batch[2],\n","                          'label_ids': batch[3]}\n","                outputs = self.model(**inputs)\n","                tmp_eval_loss, label_logits = outputs[:2]\n","                eval_loss += tmp_eval_loss.mean().item()\n","            \n","            nb_eval_steps += 1\n","\n","            # label prediction\n","            if label_preds is None:\n","                label_preds = label_logits.detach().cpu().numpy()\n","                out_label_ids = inputs['label_ids'].detach().cpu().numpy()\n","            else:\n","                label_preds = np.append(label_preds, label_logits.detach().cpu().numpy(), axis=0)\n","                out_label_ids = np.append(out_label_ids, inputs['label_ids'].detach().cpu().numpy(), axis=0)\n","\n","\n","        eval_loss = eval_loss / nb_eval_steps\n","        results = {\n","            \"loss\": eval_loss\n","        }\n","\n","        # label result\n","        label_preds = np.argmax(label_preds, axis=1)\n","        print(set(label_preds))\n","        total_result = accuracy_score(out_label_ids, label_preds)\n","        results.update({'accuracy' : total_result})\n","        logger.info(\"***** Eval results *****\")\n","        for key in sorted(results.keys()):\n","            logger.info(\"  %s = %.4f\", key if key != 'loss' else 'loss', float(results[key]) * 100.0 if key != 'loss' else float(results[key]))\n","\n","        if (mode == 'test'): print(classification_report(out_label_ids, label_preds, target_names=['negative', 'positive'], digits = 4))\n","        return results\n","\n","    def save_model(self):\n","        # Save model checkpoint (Overwrite)\n","        if not os.path.exists(self.args.model_dir):\n","            os.makedirs(self.args.model_dir)\n","        model_to_save = self.model.module if hasattr(self.model, 'module') else self.model\n","        model_to_save.save_pretrained(self.args.model_dir)\n","\n","\n","    def load_model(self):\n","        # Check whether model exists\n","        if not os.path.exists(self.args.model_dir):\n","            raise Exception(\"Model doesn't exists! Train first!\")\n","\n","        try:\n","            self.model = self.model_class.from_pretrained(self.args.model_dir,\n","                                                          args=self.args,\n","                                                          label_lst=self.label_lst)\n","            self.model.to(self.device)\n","            logger.info(\"***** Model Loaded *****\")\n","        except:\n","            raise Exception(\"Some model files might be missing...\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ydHioDjUJMeA"},"outputs":[],"source":["class dotdict(dict):\n","    \"\"\"dot.notation access to dictionary attributes\"\"\"\n","    __getattr__ = dict.get\n","    __setattr__ = dict.__setitem__\n","    __delattr__ = dict.__delitem__"]},{"cell_type":"markdown","metadata":{"id":"ErWVnaL9ahyP"},"source":["Using bert\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YMs8BTqnJOEf"},"outputs":[],"source":["args = dotdict(dict())\n","args.seed = 810197502\n","args.model_type = 'bert' \n","args.model_name_or_path = MODEL_PATH_MAP[args.model_type]\n","args.dropout_rate =  0.3\n","args.do_train = True\n","args.do_eval = True\n","args.train_batch_size = 32 \n","args.max_steps = -1\n","args.task = 'Paraphrase' \n","args.no_cuda = False\n","args.weight_decay = 0\n","args.num_train_epochs = NUM_EPOCHS\n","args.gradient_accumulation_steps = 1\n","args.learning_rate = 3e-5 \n","args.adam_epsilon = 1e-8\n","args.warmup_steps = 0 \n","args.logging_steps = 200\n","args.save_steps = 500\n","args.max_grad_norm = 1\n","args.eval_batch_size = 64\n","args.model_dir = \"Paraphrase_model_\" + args.model_type\n","args.max_seq_len = 128\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KeuvpC-ZNMRD"},"outputs":[],"source":["init_logger()\n","set_seed(args)\n","tokenizer = load_tokenizer(args)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RPUnSZlUq9kg"},"outputs":[],"source":["train_dataset = prepare_dataset(dataset['train'], args, tokenizer)\n","valid_dataset = prepare_dataset(dataset['validation'], args, tokenizer)\n","test_dataset = prepare_dataset(dataset['test'], args, tokenizer)\n","#valid_dataset = test_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8053,"status":"ok","timestamp":1659892129460,"user":{"displayName":"shayan vassef","userId":"08909168472496340813"},"user_tz":-270},"id":"AkmMPQ4ec7rw","outputId":"29d2ebc7-2aa3-4f38-8202-a197fdb5f911"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BERT_Textual_Entailement: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BERT_Textual_Entailement from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BERT_Textual_Entailement from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BERT_Textual_Entailement were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['encoder.layer.7.intermediate.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.7.output.dense.bias', 'pooler.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'classifier.linear2.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.4.attention.output.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.0.attention.self.query.bias', 'pooler.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'classifier.linear1.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'classifier.linear2.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.2.output.dense.bias', 'classifier.linear1.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.7.intermediate.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.0.attention.self.query.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["trainer = Trainer(args, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XoTrRNhZxRDU"},"outputs":[],"source":["gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"code","source":["# all\n","if args.do_train:\n","    # trainer.load_model()\n","    trainer.train()\n","\n","if args.do_eval:\n","    # trainer.load_model()\n","    trainer.evaluate(\"test\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sPOneSsUTPrR","executionInfo":{"status":"ok","timestamp":1659831594612,"user_tz":-270,"elapsed":1921303,"user":{"displayName":"shayan vassef","userId":"08909168472496340813"}},"outputId":"0ee76bed-4b7f-4bb4-c449-421badd1cecd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","08/06/2022 23:47:52 - INFO - __main__ -   ***** Running training *****\n","08/06/2022 23:47:52 - INFO - __main__ -     Num examples = 8078\n","08/06/2022 23:47:52 - INFO - __main__ -     Num Epochs = 10\n","08/06/2022 23:47:52 - INFO - __main__ -     Total train batch size = 32\n","08/06/2022 23:47:52 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/06/2022 23:47:52 - INFO - __main__ -     Total optimization steps = 2530\n","08/06/2022 23:47:52 - INFO - __main__ -     Logging steps = 200\n","08/06/2022 23:47:52 - INFO - __main__ -     Save steps = 500\n","Epoch:   0%|          | 0/10 [00:00<?, ?it/s]08/06/2022 23:48:01 - INFO - __main__ -   Train loss = 0.6292\n","08/06/2022 23:48:07 - INFO - __main__ -   Train loss = 0.5790\n","08/06/2022 23:48:13 - INFO - __main__ -   Train loss = 0.5218\n","08/06/2022 23:48:19 - INFO - __main__ -   Train loss = 0.4681\n","08/06/2022 23:48:25 - INFO - __main__ -   Train loss = 0.4314\n","08/06/2022 23:48:31 - INFO - __main__ -   Train loss = 0.4019\n","08/06/2022 23:48:38 - INFO - __main__ -   Train loss = 0.3772\n","08/06/2022 23:48:44 - INFO - __main__ -   Train loss = 0.3546\n","08/06/2022 23:48:50 - INFO - __main__ -   Train loss = 0.3423\n","08/06/2022 23:48:57 - INFO - __main__ -   Train loss = 0.3290\n","08/06/2022 23:49:03 - INFO - __main__ -   Train loss = 0.3162\n","08/06/2022 23:49:10 - INFO - __main__ -   Train loss = 0.3047\n","08/06/2022 23:49:16 - INFO - __main__ -   Train loss = 0.2972\n","08/06/2022 23:49:23 - INFO - __main__ -   Train loss = 0.2863\n","08/06/2022 23:49:29 - INFO - __main__ -   Train loss = 0.2785\n","08/06/2022 23:49:36 - INFO - __main__ -   Train loss = 0.2733\n","08/06/2022 23:49:43 - INFO - __main__ -   Train loss = 0.2673\n","08/06/2022 23:49:50 - INFO - __main__ -   Train loss = 0.2638\n","08/06/2022 23:49:56 - INFO - __main__ -   Train loss = 0.2587\n","08/06/2022 23:50:03 - INFO - __main__ -   Train loss = 0.2526\n","08/06/2022 23:50:03 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/06/2022 23:50:03 - INFO - __main__ -     Num examples = 2308\n","08/06/2022 23:50:03 - INFO - __main__ -     Batch size = 64\n","08/06/2022 23:50:20 - INFO - __main__ -   ***** Eval results *****\n","08/06/2022 23:50:20 - INFO - __main__ -     accuracy = 94.7574\n","08/06/2022 23:50:20 - INFO - __main__ -     loss = 0.1560\n","08/06/2022 23:50:20 - INFO - __main__ -   dev best_acc = 94.7574\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/06/2022 23:50:30 - INFO - __main__ -   Train loss = 0.2520\n","08/06/2022 23:50:37 - INFO - __main__ -   Train loss = 0.2483\n","08/06/2022 23:50:43 - INFO - __main__ -   Train loss = 0.2439\n","08/06/2022 23:50:50 - INFO - __main__ -   Train loss = 0.2387\n","08/06/2022 23:50:57 - INFO - __main__ -   Train loss = 0.2343\n","08/06/2022 23:50:59 - INFO - __main__ -   train_acc = 91.7678\n","Epoch:  10%|█         | 1/10 [03:06<27:59, 186.56s/it]08/06/2022 23:51:03 - INFO - __main__ -   Train loss = 0.2322\n","08/06/2022 23:51:10 - INFO - __main__ -   Train loss = 0.2283\n","08/06/2022 23:51:17 - INFO - __main__ -   Train loss = 0.2249\n","08/06/2022 23:51:23 - INFO - __main__ -   Train loss = 0.2197\n","08/06/2022 23:51:30 - INFO - __main__ -   Train loss = 0.2156\n","08/06/2022 23:51:37 - INFO - __main__ -   Train loss = 0.2142\n","08/06/2022 23:51:43 - INFO - __main__ -   Train loss = 0.2120\n","08/06/2022 23:51:50 - INFO - __main__ -   Train loss = 0.2077\n","08/06/2022 23:51:57 - INFO - __main__ -   Train loss = 0.2068\n","08/06/2022 23:52:03 - INFO - __main__ -   Train loss = 0.2046\n","08/06/2022 23:52:10 - INFO - __main__ -   Train loss = 0.2016\n","08/06/2022 23:52:17 - INFO - __main__ -   Train loss = 0.1984\n","08/06/2022 23:52:24 - INFO - __main__ -   Train loss = 0.1946\n","08/06/2022 23:52:30 - INFO - __main__ -   Train loss = 0.1931\n","08/06/2022 23:52:37 - INFO - __main__ -   Train loss = 0.1907\n","08/06/2022 23:52:37 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/06/2022 23:52:37 - INFO - __main__ -     Num examples = 2308\n","08/06/2022 23:52:37 - INFO - __main__ -     Batch size = 64\n","08/06/2022 23:52:54 - INFO - __main__ -   ***** Eval results *****\n","08/06/2022 23:52:54 - INFO - __main__ -     accuracy = 93.5009\n","08/06/2022 23:52:54 - INFO - __main__ -     loss = 0.2742\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/06/2022 23:53:01 - INFO - __main__ -   Train loss = 0.1895\n","08/06/2022 23:53:08 - INFO - __main__ -   Train loss = 0.1875\n","08/06/2022 23:53:14 - INFO - __main__ -   Train loss = 0.1864\n","08/06/2022 23:53:21 - INFO - __main__ -   Train loss = 0.1845\n","08/06/2022 23:53:28 - INFO - __main__ -   Train loss = 0.1818\n","08/06/2022 23:53:35 - INFO - __main__ -   Train loss = 0.1790\n","08/06/2022 23:53:41 - INFO - __main__ -   Train loss = 0.1797\n","08/06/2022 23:53:48 - INFO - __main__ -   Train loss = 0.1774\n","08/06/2022 23:53:55 - INFO - __main__ -   Train loss = 0.1747\n","08/06/2022 23:54:02 - INFO - __main__ -   Train loss = 0.1721\n","08/06/2022 23:54:05 - INFO - __main__ -   train_acc = 96.5586\n","Epoch:  20%|██        | 2/10 [06:13<24:53, 186.63s/it]08/06/2022 23:54:08 - INFO - __main__ -   Train loss = 0.1708\n","08/06/2022 23:54:15 - INFO - __main__ -   Train loss = 0.1682\n","08/06/2022 23:54:21 - INFO - __main__ -   Train loss = 0.1661\n","08/06/2022 23:54:28 - INFO - __main__ -   Train loss = 0.1641\n","08/06/2022 23:54:35 - INFO - __main__ -   Train loss = 0.1617\n","08/06/2022 23:54:41 - INFO - __main__ -   Train loss = 0.1602\n","08/06/2022 23:54:48 - INFO - __main__ -   Train loss = 0.1598\n","08/06/2022 23:54:55 - INFO - __main__ -   Train loss = 0.1580\n","08/06/2022 23:55:01 - INFO - __main__ -   Train loss = 0.1570\n","08/06/2022 23:55:08 - INFO - __main__ -   Train loss = 0.1557\n","08/06/2022 23:55:08 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/06/2022 23:55:08 - INFO - __main__ -     Num examples = 2308\n","08/06/2022 23:55:08 - INFO - __main__ -     Batch size = 64\n","08/06/2022 23:55:26 - INFO - __main__ -   ***** Eval results *****\n","08/06/2022 23:55:26 - INFO - __main__ -     accuracy = 94.8007\n","08/06/2022 23:55:26 - INFO - __main__ -     loss = 0.1840\n","08/06/2022 23:55:26 - INFO - __main__ -   dev best_acc = 94.8007\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/06/2022 23:55:36 - INFO - __main__ -   Train loss = 0.1540\n","08/06/2022 23:55:42 - INFO - __main__ -   Train loss = 0.1520\n","08/06/2022 23:55:49 - INFO - __main__ -   Train loss = 0.1507\n","08/06/2022 23:55:56 - INFO - __main__ -   Train loss = 0.1487\n","08/06/2022 23:56:02 - INFO - __main__ -   Train loss = 0.1477\n","08/06/2022 23:56:09 - INFO - __main__ -   Train loss = 0.1466\n","08/06/2022 23:56:16 - INFO - __main__ -   Train loss = 0.1454\n","08/06/2022 23:56:22 - INFO - __main__ -   Train loss = 0.1446\n","08/06/2022 23:56:29 - INFO - __main__ -   Train loss = 0.1437\n","08/06/2022 23:56:36 - INFO - __main__ -   Train loss = 0.1421\n","08/06/2022 23:56:43 - INFO - __main__ -   Train loss = 0.1403\n","08/06/2022 23:56:49 - INFO - __main__ -   Train loss = 0.1403\n","08/06/2022 23:56:56 - INFO - __main__ -   Train loss = 0.1387\n","08/06/2022 23:57:03 - INFO - __main__ -   Train loss = 0.1369\n","08/06/2022 23:57:09 - INFO - __main__ -   Train loss = 0.1353\n","08/06/2022 23:57:15 - INFO - __main__ -   train_acc = 98.4526\n","Epoch:  30%|███       | 3/10 [09:23<21:56, 188.06s/it]08/06/2022 23:57:16 - INFO - __main__ -   Train loss = 0.1341\n","08/06/2022 23:57:22 - INFO - __main__ -   Train loss = 0.1329\n","08/06/2022 23:57:29 - INFO - __main__ -   Train loss = 0.1317\n","08/06/2022 23:57:36 - INFO - __main__ -   Train loss = 0.1301\n","08/06/2022 23:57:42 - INFO - __main__ -   Train loss = 0.1285\n","08/06/2022 23:57:42 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/06/2022 23:57:42 - INFO - __main__ -     Num examples = 2308\n","08/06/2022 23:57:42 - INFO - __main__ -     Batch size = 64\n","08/06/2022 23:58:00 - INFO - __main__ -   ***** Eval results *****\n","08/06/2022 23:58:00 - INFO - __main__ -     accuracy = 94.6274\n","08/06/2022 23:58:00 - INFO - __main__ -     loss = 0.2539\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/06/2022 23:58:07 - INFO - __main__ -   Train loss = 0.1274\n","08/06/2022 23:58:13 - INFO - __main__ -   Train loss = 0.1273\n","08/06/2022 23:58:20 - INFO - __main__ -   Train loss = 0.1262\n","08/06/2022 23:58:27 - INFO - __main__ -   Train loss = 0.1249\n","08/06/2022 23:58:33 - INFO - __main__ -   Train loss = 0.1244\n","08/06/2022 23:58:40 - INFO - __main__ -   Train loss = 0.1232\n","08/06/2022 23:58:47 - INFO - __main__ -   Train loss = 0.1222\n","08/06/2022 23:58:53 - INFO - __main__ -   Train loss = 0.1211\n","08/06/2022 23:59:00 - INFO - __main__ -   Train loss = 0.1200\n","08/06/2022 23:59:07 - INFO - __main__ -   Train loss = 0.1193\n","08/06/2022 23:59:13 - INFO - __main__ -   Train loss = 0.1184\n","08/06/2022 23:59:20 - INFO - __main__ -   Train loss = 0.1174\n","08/06/2022 23:59:27 - INFO - __main__ -   Train loss = 0.1165\n","08/06/2022 23:59:33 - INFO - __main__ -   Train loss = 0.1161\n","08/06/2022 23:59:40 - INFO - __main__ -   Train loss = 0.1154\n","08/06/2022 23:59:47 - INFO - __main__ -   Train loss = 0.1143\n","08/06/2022 23:59:53 - INFO - __main__ -   Train loss = 0.1139\n","08/07/2022 00:00:00 - INFO - __main__ -   Train loss = 0.1130\n","08/07/2022 00:00:07 - INFO - __main__ -   Train loss = 0.1120\n","08/07/2022 00:00:14 - INFO - __main__ -   Train loss = 0.1109\n","08/07/2022 00:00:14 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/07/2022 00:00:14 - INFO - __main__ -     Num examples = 2308\n","08/07/2022 00:00:14 - INFO - __main__ -     Batch size = 64\n","08/07/2022 00:00:31 - INFO - __main__ -   ***** Eval results *****\n","08/07/2022 00:00:31 - INFO - __main__ -     accuracy = 94.7574\n","08/07/2022 00:00:31 - INFO - __main__ -     loss = 0.2366\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/07/2022 00:00:38 - INFO - __main__ -   Train loss = 0.1098\n","08/07/2022 00:00:39 - INFO - __main__ -   train_acc = 99.2201\n","Epoch:  40%|████      | 4/10 [12:46<19:25, 194.28s/it]08/07/2022 00:00:44 - INFO - __main__ -   Train loss = 0.1089\n","08/07/2022 00:00:51 - INFO - __main__ -   Train loss = 0.1079\n","08/07/2022 00:00:57 - INFO - __main__ -   Train loss = 0.1072\n","08/07/2022 00:01:04 - INFO - __main__ -   Train loss = 0.1062\n","08/07/2022 00:01:11 - INFO - __main__ -   Train loss = 0.1052\n","08/07/2022 00:01:18 - INFO - __main__ -   Train loss = 0.1049\n","08/07/2022 00:01:24 - INFO - __main__ -   Train loss = 0.1044\n","08/07/2022 00:01:31 - INFO - __main__ -   Train loss = 0.1035\n","08/07/2022 00:01:38 - INFO - __main__ -   Train loss = 0.1029\n","08/07/2022 00:01:44 - INFO - __main__ -   Train loss = 0.1020\n","08/07/2022 00:01:51 - INFO - __main__ -   Train loss = 0.1013\n","08/07/2022 00:01:58 - INFO - __main__ -   Train loss = 0.1007\n","08/07/2022 00:02:04 - INFO - __main__ -   Train loss = 0.0999\n","08/07/2022 00:02:11 - INFO - __main__ -   Train loss = 0.0991\n","08/07/2022 00:02:18 - INFO - __main__ -   Train loss = 0.0982\n","08/07/2022 00:02:25 - INFO - __main__ -   Train loss = 0.0975\n","08/07/2022 00:02:31 - INFO - __main__ -   Train loss = 0.0969\n","08/07/2022 00:02:38 - INFO - __main__ -   Train loss = 0.0970\n","08/07/2022 00:02:45 - INFO - __main__ -   Train loss = 0.0964\n","08/07/2022 00:02:45 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/07/2022 00:02:45 - INFO - __main__ -     Num examples = 2308\n","08/07/2022 00:02:45 - INFO - __main__ -     Batch size = 64\n","08/07/2022 00:03:02 - INFO - __main__ -   ***** Eval results *****\n","08/07/2022 00:03:02 - INFO - __main__ -     accuracy = 95.1040\n","08/07/2022 00:03:02 - INFO - __main__ -     loss = 0.2338\n","08/07/2022 00:03:02 - INFO - __main__ -   dev best_acc = 95.1040\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/07/2022 00:03:12 - INFO - __main__ -   Train loss = 0.0957\n","08/07/2022 00:03:19 - INFO - __main__ -   Train loss = 0.0949\n","08/07/2022 00:03:25 - INFO - __main__ -   Train loss = 0.0946\n","08/07/2022 00:03:32 - INFO - __main__ -   Train loss = 0.0940\n","08/07/2022 00:03:39 - INFO - __main__ -   Train loss = 0.0934\n","08/07/2022 00:03:46 - INFO - __main__ -   Train loss = 0.0927\n","08/07/2022 00:03:49 - INFO - __main__ -   train_acc = 99.5667\n","Epoch:  50%|█████     | 5/10 [15:56<16:03, 192.67s/it]08/07/2022 00:03:52 - INFO - __main__ -   Train loss = 0.0921\n","08/07/2022 00:03:59 - INFO - __main__ -   Train loss = 0.0915\n","08/07/2022 00:04:05 - INFO - __main__ -   Train loss = 0.0909\n","08/07/2022 00:04:12 - INFO - __main__ -   Train loss = 0.0903\n","08/07/2022 00:04:19 - INFO - __main__ -   Train loss = 0.0898\n","08/07/2022 00:04:25 - INFO - __main__ -   Train loss = 0.0892\n","08/07/2022 00:04:32 - INFO - __main__ -   Train loss = 0.0892\n","08/07/2022 00:04:39 - INFO - __main__ -   Train loss = 0.0886\n","08/07/2022 00:04:45 - INFO - __main__ -   Train loss = 0.0882\n","08/07/2022 00:04:52 - INFO - __main__ -   Train loss = 0.0876\n","08/07/2022 00:04:59 - INFO - __main__ -   Train loss = 0.0869\n","08/07/2022 00:05:05 - INFO - __main__ -   Train loss = 0.0864\n","08/07/2022 00:05:12 - INFO - __main__ -   Train loss = 0.0858\n","08/07/2022 00:05:19 - INFO - __main__ -   Train loss = 0.0852\n","08/07/2022 00:05:19 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/07/2022 00:05:19 - INFO - __main__ -     Num examples = 2308\n","08/07/2022 00:05:19 - INFO - __main__ -     Batch size = 64\n","08/07/2022 00:05:36 - INFO - __main__ -   ***** Eval results *****\n","08/07/2022 00:05:36 - INFO - __main__ -     accuracy = 94.4107\n","08/07/2022 00:05:36 - INFO - __main__ -     loss = 0.2589\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/07/2022 00:05:43 - INFO - __main__ -   Train loss = 0.0847\n","08/07/2022 00:05:50 - INFO - __main__ -   Train loss = 0.0841\n","08/07/2022 00:05:56 - INFO - __main__ -   Train loss = 0.0837\n","08/07/2022 00:06:03 - INFO - __main__ -   Train loss = 0.0834\n","08/07/2022 00:06:10 - INFO - __main__ -   Train loss = 0.0831\n","08/07/2022 00:06:16 - INFO - __main__ -   Train loss = 0.0827\n","08/07/2022 00:06:23 - INFO - __main__ -   Train loss = 0.0821\n","08/07/2022 00:06:30 - INFO - __main__ -   Train loss = 0.0819\n","08/07/2022 00:06:36 - INFO - __main__ -   Train loss = 0.0813\n","08/07/2022 00:06:43 - INFO - __main__ -   Train loss = 0.0808\n","08/07/2022 00:06:50 - INFO - __main__ -   Train loss = 0.0803\n","08/07/2022 00:06:55 - INFO - __main__ -   train_acc = 99.6905\n","Epoch:  60%|██████    | 6/10 [19:02<12:41, 190.49s/it]08/07/2022 00:06:56 - INFO - __main__ -   Train loss = 0.0798\n","08/07/2022 00:07:03 - INFO - __main__ -   Train loss = 0.0792\n","08/07/2022 00:07:10 - INFO - __main__ -   Train loss = 0.0787\n","08/07/2022 00:07:16 - INFO - __main__ -   Train loss = 0.0782\n","08/07/2022 00:07:23 - INFO - __main__ -   Train loss = 0.0777\n","08/07/2022 00:07:30 - INFO - __main__ -   Train loss = 0.0773\n","08/07/2022 00:07:36 - INFO - __main__ -   Train loss = 0.0772\n","08/07/2022 00:07:43 - INFO - __main__ -   Train loss = 0.0768\n","08/07/2022 00:07:50 - INFO - __main__ -   Train loss = 0.0763\n","08/07/2022 00:07:50 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/07/2022 00:07:50 - INFO - __main__ -     Num examples = 2308\n","08/07/2022 00:07:50 - INFO - __main__ -     Batch size = 64\n","08/07/2022 00:08:07 - INFO - __main__ -   ***** Eval results *****\n","08/07/2022 00:08:07 - INFO - __main__ -     accuracy = 94.7140\n","08/07/2022 00:08:07 - INFO - __main__ -     loss = 0.2638\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/07/2022 00:08:14 - INFO - __main__ -   Train loss = 0.0761\n","08/07/2022 00:08:21 - INFO - __main__ -   Train loss = 0.0756\n","08/07/2022 00:08:27 - INFO - __main__ -   Train loss = 0.0752\n","08/07/2022 00:08:34 - INFO - __main__ -   Train loss = 0.0748\n","08/07/2022 00:08:41 - INFO - __main__ -   Train loss = 0.0744\n","08/07/2022 00:08:47 - INFO - __main__ -   Train loss = 0.0739\n","08/07/2022 00:08:54 - INFO - __main__ -   Train loss = 0.0735\n","08/07/2022 00:09:01 - INFO - __main__ -   Train loss = 0.0732\n","08/07/2022 00:09:07 - INFO - __main__ -   Train loss = 0.0729\n","08/07/2022 00:09:14 - INFO - __main__ -   Train loss = 0.0727\n","08/07/2022 00:09:21 - INFO - __main__ -   Train loss = 0.0724\n","08/07/2022 00:09:27 - INFO - __main__ -   Train loss = 0.0720\n","08/07/2022 00:09:34 - INFO - __main__ -   Train loss = 0.0717\n","08/07/2022 00:09:41 - INFO - __main__ -   Train loss = 0.0713\n","08/07/2022 00:09:47 - INFO - __main__ -   Train loss = 0.0709\n","08/07/2022 00:09:54 - INFO - __main__ -   Train loss = 0.0705\n","08/07/2022 00:10:01 - INFO - __main__ -   Train loss = 0.0702\n","08/07/2022 00:10:01 - INFO - __main__ -   train_acc = 99.7524\n","Epoch:  70%|███████   | 7/10 [22:09<09:27, 189.15s/it]08/07/2022 00:10:07 - INFO - __main__ -   Train loss = 0.0698\n","08/07/2022 00:10:14 - INFO - __main__ -   Train loss = 0.0695\n","08/07/2022 00:10:21 - INFO - __main__ -   Train loss = 0.0691\n","08/07/2022 00:10:21 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/07/2022 00:10:21 - INFO - __main__ -     Num examples = 2308\n","08/07/2022 00:10:21 - INFO - __main__ -     Batch size = 64\n","08/07/2022 00:10:38 - INFO - __main__ -   ***** Eval results *****\n","08/07/2022 00:10:38 - INFO - __main__ -     accuracy = 95.4506\n","08/07/2022 00:10:38 - INFO - __main__ -     loss = 0.2548\n","08/07/2022 00:10:38 - INFO - __main__ -   dev best_acc = 95.4506\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/07/2022 00:10:48 - INFO - __main__ -   Train loss = 0.0687\n","08/07/2022 00:10:55 - INFO - __main__ -   Train loss = 0.0683\n","08/07/2022 00:11:01 - INFO - __main__ -   Train loss = 0.0681\n","08/07/2022 00:11:08 - INFO - __main__ -   Train loss = 0.0678\n","08/07/2022 00:11:15 - INFO - __main__ -   Train loss = 0.0675\n","08/07/2022 00:11:21 - INFO - __main__ -   Train loss = 0.0672\n","08/07/2022 00:11:28 - INFO - __main__ -   Train loss = 0.0668\n","08/07/2022 00:11:35 - INFO - __main__ -   Train loss = 0.0665\n","08/07/2022 00:11:42 - INFO - __main__ -   Train loss = 0.0662\n","08/07/2022 00:11:48 - INFO - __main__ -   Train loss = 0.0658\n","08/07/2022 00:11:55 - INFO - __main__ -   Train loss = 0.0655\n","08/07/2022 00:12:02 - INFO - __main__ -   Train loss = 0.0652\n","08/07/2022 00:12:08 - INFO - __main__ -   Train loss = 0.0649\n","08/07/2022 00:12:15 - INFO - __main__ -   Train loss = 0.0646\n","08/07/2022 00:12:22 - INFO - __main__ -   Train loss = 0.0644\n","08/07/2022 00:12:28 - INFO - __main__ -   Train loss = 0.0642\n","08/07/2022 00:12:35 - INFO - __main__ -   Train loss = 0.0639\n","08/07/2022 00:12:42 - INFO - __main__ -   Train loss = 0.0636\n","08/07/2022 00:12:48 - INFO - __main__ -   Train loss = 0.0633\n","08/07/2022 00:12:55 - INFO - __main__ -   Train loss = 0.0630\n","08/07/2022 00:12:55 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/07/2022 00:12:55 - INFO - __main__ -     Num examples = 2308\n","08/07/2022 00:12:55 - INFO - __main__ -     Batch size = 64\n","08/07/2022 00:13:13 - INFO - __main__ -   ***** Eval results *****\n","08/07/2022 00:13:13 - INFO - __main__ -     accuracy = 95.4506\n","08/07/2022 00:13:13 - INFO - __main__ -     loss = 0.2589\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/07/2022 00:13:19 - INFO - __main__ -   Train loss = 0.0627\n","08/07/2022 00:13:26 - INFO - __main__ -   Train loss = 0.0624\n","08/07/2022 00:13:28 - INFO - __main__ -   train_acc = 99.8391\n","Epoch:  80%|████████  | 8/10 [25:36<06:29, 194.85s/it]08/07/2022 00:13:32 - INFO - __main__ -   Train loss = 0.0621\n","08/07/2022 00:13:39 - INFO - __main__ -   Train loss = 0.0618\n","08/07/2022 00:13:46 - INFO - __main__ -   Train loss = 0.0615\n","08/07/2022 00:13:52 - INFO - __main__ -   Train loss = 0.0612\n","08/07/2022 00:13:59 - INFO - __main__ -   Train loss = 0.0609\n","08/07/2022 00:14:06 - INFO - __main__ -   Train loss = 0.0606\n","08/07/2022 00:14:12 - INFO - __main__ -   Train loss = 0.0604\n","08/07/2022 00:14:19 - INFO - __main__ -   Train loss = 0.0603\n","08/07/2022 00:14:26 - INFO - __main__ -   Train loss = 0.0601\n","08/07/2022 00:14:32 - INFO - __main__ -   Train loss = 0.0598\n","08/07/2022 00:14:39 - INFO - __main__ -   Train loss = 0.0595\n","08/07/2022 00:14:46 - INFO - __main__ -   Train loss = 0.0592\n","08/07/2022 00:14:53 - INFO - __main__ -   Train loss = 0.0590\n","08/07/2022 00:14:59 - INFO - __main__ -   Train loss = 0.0587\n","08/07/2022 00:15:06 - INFO - __main__ -   Train loss = 0.0584\n","08/07/2022 00:15:13 - INFO - __main__ -   Train loss = 0.0582\n","08/07/2022 00:15:19 - INFO - __main__ -   Train loss = 0.0579\n","08/07/2022 00:15:26 - INFO - __main__ -   Train loss = 0.0577\n","08/07/2022 00:15:26 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/07/2022 00:15:26 - INFO - __main__ -     Num examples = 2308\n","08/07/2022 00:15:26 - INFO - __main__ -     Batch size = 64\n","08/07/2022 00:15:43 - INFO - __main__ -   ***** Eval results *****\n","08/07/2022 00:15:43 - INFO - __main__ -     accuracy = 95.3640\n","08/07/2022 00:15:43 - INFO - __main__ -     loss = 0.2707\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/07/2022 00:15:50 - INFO - __main__ -   Train loss = 0.0575\n","08/07/2022 00:15:57 - INFO - __main__ -   Train loss = 0.0572\n","08/07/2022 00:16:04 - INFO - __main__ -   Train loss = 0.0570\n","08/07/2022 00:16:10 - INFO - __main__ -   Train loss = 0.0567\n","08/07/2022 00:16:17 - INFO - __main__ -   Train loss = 0.0565\n","08/07/2022 00:16:24 - INFO - __main__ -   Train loss = 0.0562\n","08/07/2022 00:16:30 - INFO - __main__ -   Train loss = 0.0560\n","08/07/2022 00:16:35 - INFO - __main__ -   train_acc = 99.9133\n","Epoch:  90%|█████████ | 9/10 [28:42<03:12, 192.18s/it]08/07/2022 00:16:37 - INFO - __main__ -   Train loss = 0.0557\n","08/07/2022 00:16:43 - INFO - __main__ -   Train loss = 0.0555\n","08/07/2022 00:16:50 - INFO - __main__ -   Train loss = 0.0553\n","08/07/2022 00:16:57 - INFO - __main__ -   Train loss = 0.0550\n","08/07/2022 00:17:03 - INFO - __main__ -   Train loss = 0.0548\n","08/07/2022 00:17:10 - INFO - __main__ -   Train loss = 0.0546\n","08/07/2022 00:17:17 - INFO - __main__ -   Train loss = 0.0544\n","08/07/2022 00:17:23 - INFO - __main__ -   Train loss = 0.0542\n","08/07/2022 00:17:30 - INFO - __main__ -   Train loss = 0.0539\n","08/07/2022 00:17:37 - INFO - __main__ -   Train loss = 0.0538\n","08/07/2022 00:17:43 - INFO - __main__ -   Train loss = 0.0536\n","08/07/2022 00:17:50 - INFO - __main__ -   Train loss = 0.0534\n","08/07/2022 00:17:57 - INFO - __main__ -   Train loss = 0.0531\n","08/07/2022 00:17:57 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/07/2022 00:17:57 - INFO - __main__ -     Num examples = 2308\n","08/07/2022 00:17:57 - INFO - __main__ -     Batch size = 64\n","08/07/2022 00:18:14 - INFO - __main__ -   ***** Eval results *****\n","08/07/2022 00:18:14 - INFO - __main__ -     accuracy = 95.5373\n","08/07/2022 00:18:14 - INFO - __main__ -     loss = 0.2755\n","08/07/2022 00:18:14 - INFO - __main__ -   dev best_acc = 95.5373\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/07/2022 00:18:24 - INFO - __main__ -   Train loss = 0.0529\n","08/07/2022 00:18:31 - INFO - __main__ -   Train loss = 0.0527\n","08/07/2022 00:18:37 - INFO - __main__ -   Train loss = 0.0525\n","08/07/2022 00:18:44 - INFO - __main__ -   Train loss = 0.0523\n","08/07/2022 00:18:51 - INFO - __main__ -   Train loss = 0.0521\n","08/07/2022 00:18:58 - INFO - __main__ -   Train loss = 0.0519\n","08/07/2022 00:19:04 - INFO - __main__ -   Train loss = 0.0517\n","08/07/2022 00:19:11 - INFO - __main__ -   Train loss = 0.0515\n","08/07/2022 00:19:18 - INFO - __main__ -   Train loss = 0.0513\n","08/07/2022 00:19:24 - INFO - __main__ -   Train loss = 0.0511\n","08/07/2022 00:19:31 - INFO - __main__ -   Train loss = 0.0509\n","08/07/2022 00:19:38 - INFO - __main__ -   Train loss = 0.0507\n","08/07/2022 00:19:44 - INFO - __main__ -   Train loss = 0.0505\n","08/07/2022 00:19:44 - INFO - __main__ -   train_acc = 99.9381\n","Epoch: 100%|██████████| 10/10 [31:52<00:00, 191.21s/it]\n","08/07/2022 00:19:44 - INFO - __main__ -   \n","***** Running evaluation on test dataset *****\n","08/07/2022 00:19:44 - INFO - __main__ -     Num examples = 1155\n","08/07/2022 00:19:44 - INFO - __main__ -     Batch size = 64\n","08/07/2022 00:19:53 - INFO - __main__ -   ***** Eval results *****\n","08/07/2022 00:19:53 - INFO - __main__ -     accuracy = 95.2381\n","08/07/2022 00:19:53 - INFO - __main__ -     loss = 0.2783\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n","              precision    recall  f1-score   support\n","\n","    negative     0.9707    0.9697    0.9702       923\n","    positive     0.8798    0.8836    0.8817       232\n","\n","    accuracy                         0.9524      1155\n","   macro avg     0.9253    0.9266    0.9260      1155\n","weighted avg     0.9525    0.9524    0.9524      1155\n","\n"]}]},{"cell_type":"code","source":["# random\n","if args.do_train:\n","    # trainer.load_model()\n","    trainer.train()\n","\n","if args.do_eval:\n","    # trainer.load_model()\n","    trainer.evaluate(\"test\")"],"metadata":{"id":"dwoRzR6xOggq","executionInfo":{"status":"ok","timestamp":1659892194843,"user_tz":-270,"elapsed":54912,"user":{"displayName":"shayan vassef","userId":"08909168472496340813"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f0ea5781-1b68-40f2-e49f-625b9d68a87d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","08/07/2022 17:08:59 - INFO - __main__ -   ***** Running training *****\n","08/07/2022 17:08:59 - INFO - __main__ -     Num examples = 223\n","08/07/2022 17:08:59 - INFO - __main__ -     Num Epochs = 10\n","08/07/2022 17:08:59 - INFO - __main__ -     Total train batch size = 32\n","08/07/2022 17:08:59 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/07/2022 17:08:59 - INFO - __main__ -     Total optimization steps = 70\n","08/07/2022 17:08:59 - INFO - __main__ -     Logging steps = 200\n","08/07/2022 17:08:59 - INFO - __main__ -     Save steps = 500\n","Epoch:   0%|          | 0/10 [00:00<?, ?it/s]08/07/2022 17:09:03 - INFO - __main__ -   train_acc = 63.6771\n","Epoch:  10%|█         | 1/10 [00:04<00:40,  4.47s/it]08/07/2022 17:09:05 - INFO - __main__ -   Train loss = 0.6318\n","08/07/2022 17:09:08 - INFO - __main__ -   train_acc = 79.3722\n","Epoch:  20%|██        | 2/10 [00:08<00:35,  4.47s/it]08/07/2022 17:09:12 - INFO - __main__ -   Train loss = 0.5686\n","08/07/2022 17:09:12 - INFO - __main__ -   train_acc = 86.9955\n","Epoch:  30%|███       | 3/10 [00:13<00:31,  4.49s/it]08/07/2022 17:09:17 - INFO - __main__ -   train_acc = 90.1345\n","Epoch:  40%|████      | 4/10 [00:17<00:27,  4.50s/it]08/07/2022 17:09:18 - INFO - __main__ -   Train loss = 0.5136\n","08/07/2022 17:09:21 - INFO - __main__ -   train_acc = 93.2735\n","Epoch:  50%|█████     | 5/10 [00:22<00:22,  4.52s/it]08/07/2022 17:09:25 - INFO - __main__ -   Train loss = 0.4615\n","08/07/2022 17:09:26 - INFO - __main__ -   train_acc = 95.9641\n","Epoch:  60%|██████    | 6/10 [00:27<00:18,  4.53s/it]08/07/2022 17:09:31 - INFO - __main__ -   train_acc = 97.3094\n","Epoch:  70%|███████   | 7/10 [00:31<00:13,  4.56s/it]08/07/2022 17:09:31 - INFO - __main__ -   Train loss = 0.4165\n","08/07/2022 17:09:35 - INFO - __main__ -   train_acc = 97.7578\n","Epoch:  80%|████████  | 8/10 [00:36<00:09,  4.59s/it]08/07/2022 17:09:38 - INFO - __main__ -   Train loss = 0.3799\n","08/07/2022 17:09:40 - INFO - __main__ -   train_acc = 98.6547\n","Epoch:  90%|█████████ | 9/10 [00:41<00:04,  4.62s/it]08/07/2022 17:09:45 - INFO - __main__ -   Train loss = 0.3509\n","08/07/2022 17:09:45 - INFO - __main__ -   train_acc = 98.6547\n","Epoch: 100%|██████████| 10/10 [00:45<00:00,  4.57s/it]\n","08/07/2022 17:09:45 - INFO - __main__ -   \n","***** Running evaluation on test dataset *****\n","08/07/2022 17:09:45 - INFO - __main__ -     Num examples = 1155\n","08/07/2022 17:09:45 - INFO - __main__ -     Batch size = 64\n","08/07/2022 17:09:53 - INFO - __main__ -   ***** Eval results *****\n","08/07/2022 17:09:53 - INFO - __main__ -     accuracy = 89.3506\n","08/07/2022 17:09:53 - INFO - __main__ -     loss = 0.2970\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n","              precision    recall  f1-score   support\n","\n","    negative     0.9545    0.9101    0.9318       923\n","    positive     0.6982    0.8276    0.7574       232\n","\n","    accuracy                         0.8935      1155\n","   macro avg     0.8264    0.8688    0.8446      1155\n","weighted avg     0.9031    0.8935    0.8968      1155\n","\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7xNxHo9NNbuK","executionInfo":{"status":"ok","timestamp":1659891343736,"user_tz":-270,"elapsed":53638,"user":{"displayName":"shayan vassef","userId":"08909168472496340813"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"720a4da2-3473-4815-bf96-8be8fa64a535"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","08/07/2022 16:54:49 - INFO - __main__ -   ***** Running training *****\n","08/07/2022 16:54:49 - INFO - __main__ -     Num examples = 223\n","08/07/2022 16:54:49 - INFO - __main__ -     Num Epochs = 10\n","08/07/2022 16:54:49 - INFO - __main__ -     Total train batch size = 32\n","08/07/2022 16:54:49 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/07/2022 16:54:49 - INFO - __main__ -     Total optimization steps = 70\n","08/07/2022 16:54:49 - INFO - __main__ -     Logging steps = 200\n","08/07/2022 16:54:49 - INFO - __main__ -     Save steps = 500\n","Epoch:   0%|          | 0/10 [00:00<?, ?it/s]08/07/2022 16:54:56 - INFO - __main__ -   train_acc = 50.2242\n","Epoch:  10%|█         | 1/10 [00:06<01:02,  6.90s/it]08/07/2022 16:54:57 - INFO - __main__ -   Train loss = 0.6934\n","08/07/2022 16:55:00 - INFO - __main__ -   train_acc = 56.5022\n","Epoch:  20%|██        | 2/10 [00:11<00:42,  5.29s/it]08/07/2022 16:55:03 - INFO - __main__ -   Train loss = 0.6902\n","08/07/2022 16:55:04 - INFO - __main__ -   train_acc = 55.1570\n","Epoch:  30%|███       | 3/10 [00:15<00:33,  4.78s/it]08/07/2022 16:55:08 - INFO - __main__ -   train_acc = 57.3991\n","Epoch:  40%|████      | 4/10 [00:19<00:27,  4.55s/it]08/07/2022 16:55:09 - INFO - __main__ -   Train loss = 0.6848\n","08/07/2022 16:55:12 - INFO - __main__ -   train_acc = 69.5067\n","Epoch:  50%|█████     | 5/10 [00:23<00:22,  4.43s/it]08/07/2022 16:55:15 - INFO - __main__ -   Train loss = 0.6738\n","08/07/2022 16:55:17 - INFO - __main__ -   train_acc = 80.2691\n","Epoch:  60%|██████    | 6/10 [00:27<00:17,  4.36s/it]08/07/2022 16:55:21 - INFO - __main__ -   train_acc = 84.7534\n","Epoch:  70%|███████   | 7/10 [00:32<00:12,  4.32s/it]08/07/2022 16:55:21 - INFO - __main__ -   Train loss = 0.6554\n","08/07/2022 16:55:25 - INFO - __main__ -   train_acc = 92.3767\n","Epoch:  80%|████████  | 8/10 [00:36<00:08,  4.31s/it]08/07/2022 16:55:28 - INFO - __main__ -   Train loss = 0.6313\n","08/07/2022 16:55:29 - INFO - __main__ -   train_acc = 93.2735\n","Epoch:  90%|█████████ | 9/10 [00:40<00:04,  4.30s/it]08/07/2022 16:55:34 - INFO - __main__ -   Train loss = 0.6091\n","08/07/2022 16:55:34 - INFO - __main__ -   train_acc = 94.1704\n","Epoch: 100%|██████████| 10/10 [00:45<00:00,  4.50s/it]\n","08/07/2022 16:55:34 - INFO - __main__ -   \n","***** Running evaluation on test dataset *****\n","08/07/2022 16:55:34 - INFO - __main__ -     Num examples = 1155\n","08/07/2022 16:55:34 - INFO - __main__ -     Batch size = 64\n","08/07/2022 16:55:42 - INFO - __main__ -   ***** Eval results *****\n","08/07/2022 16:55:42 - INFO - __main__ -     accuracy = 73.6797\n","08/07/2022 16:55:42 - INFO - __main__ -     loss = 0.5839\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n","              precision    recall  f1-score   support\n","\n","    negative     0.9110    0.7432    0.8186       923\n","    positive     0.4104    0.7112    0.5205       232\n","\n","    accuracy                         0.7368      1155\n","   macro avg     0.6607    0.7272    0.6696      1155\n","weighted avg     0.8105    0.7368    0.7587      1155\n","\n"]}],"source":["# forgetable\n","if args.do_train:\n","    # trainer.load_model()\n","    trainer.train()\n","\n","if args.do_eval:\n","    # trainer.load_model()\n","    trainer.evaluate(\"test\")"]},{"cell_type":"code","source":["Prob_per_epoch.shape"],"metadata":{"id":"MdLTY5tCHZJ1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659831687799,"user_tz":-270,"elapsed":550,"user":{"displayName":"shayan vassef","userId":"08909168472496340813"}},"outputId":"c0ab9f3a-b733-4d4d-d5cc-234bd019566f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10, 8078, 2)"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["import pickle\n","with open(f'Sentiment_prob_per_{NUM_EPOCHS}epochs.pkl', 'wb') as f:\n","    data = pickle.dump(Prob_per_epoch, f)"],"metadata":{"id":"mTIZuWT-LWt5"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JIOazWmCumuf"},"outputs":[],"source":["# # Save the model to drive\n","# !cp /content/Textual_Entailement_model/pytorch_model.bin /content/drive/MyDrive/Textual_Entailement_model\n","# !cp /content/Textual_Entailement_model/config.json /content/drive/MyDrive/Textual_Entailement_model"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Sentiment.ipynb","provenance":[{"file_id":"1aHGZF4tFjZ90ShjxbGyGDrwi7cD5IbED","timestamp":1659828712786},{"file_id":"15DcvKQd-hFQsyTMehaz2aTR23NjhlO_U","timestamp":1659430721406},{"file_id":"1UbAth0ImdSdle9I6aPfqUIg667sxTGf_","timestamp":1659397233383}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"49078982e650422098f54e69d2ac509a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b9dce4e8b6294a669d7dda51956aea8f","IPY_MODEL_12309434c9ac4454a6469befcd110532","IPY_MODEL_21084e7aed8e436894be984ed723bae2"],"layout":"IPY_MODEL_065b60661e1f448bb74a61e8ad96f6e8"}},"b9dce4e8b6294a669d7dda51956aea8f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_22dca9f8a6284e0e8f037d135207ce51","placeholder":"​","style":"IPY_MODEL_40f9833f108d49b2936c865e03cb6037","value":"100%"}},"12309434c9ac4454a6469befcd110532":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d3a99548f0b47398fc4d68e511fa9a2","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8b44bf7dd9d74bba994917dc036c1157","value":3}},"21084e7aed8e436894be984ed723bae2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ffd56f7b5ff84923907c9c8fd4341b54","placeholder":"​","style":"IPY_MODEL_e252fac95e4b49a49779c0246c7202c9","value":" 3/3 [00:00&lt;00:00, 77.63it/s]"}},"065b60661e1f448bb74a61e8ad96f6e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22dca9f8a6284e0e8f037d135207ce51":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40f9833f108d49b2936c865e03cb6037":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9d3a99548f0b47398fc4d68e511fa9a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b44bf7dd9d74bba994917dc036c1157":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ffd56f7b5ff84923907c9c8fd4341b54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e252fac95e4b49a49779c0246c7202c9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}