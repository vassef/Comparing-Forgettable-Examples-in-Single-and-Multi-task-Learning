{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Y5wau-_3zTpo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659707890854,"user_tz":-120,"elapsed":22324,"user":{"displayName":"AmirHossein Dabiri Aghdam","userId":"03377875757005762455"}},"outputId":"55105595-6ba5-47fe-951b-67eb528fd84a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 365 kB 10.3 MB/s \n","\u001b[K     |████████████████████████████████| 4.7 MB 42.9 MB/s \n","\u001b[K     |████████████████████████████████| 1.2 MB 44.5 MB/s \n","\u001b[K     |████████████████████████████████| 101 kB 7.3 MB/s \n","\u001b[K     |████████████████████████████████| 212 kB 21.2 MB/s \n","\u001b[K     |████████████████████████████████| 141 kB 9.1 MB/s \n","\u001b[K     |████████████████████████████████| 115 kB 8.8 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 53.1 MB/s \n","\u001b[K     |████████████████████████████████| 127 kB 9.2 MB/s \n","\u001b[K     |████████████████████████████████| 6.6 MB 11.3 MB/s \n","\u001b[?25h"]}],"source":["!pip install -q datasets transformers sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_2_D7Xqzt4MN"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import datasets\n","from datasets import Dataset, DatasetDict\n","import os\n","import random\n","import regex as re\n","import logging\n","import gc\n","from tqdm import tqdm, trange\n","from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report\n","from datasets import load_dataset\n","import torch\n","import torch.nn as nn\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n","from transformers import XLMRobertaTokenizer\n","from transformers.models.xlm_roberta.modeling_xlm_roberta import  XLMRobertaModel, XLMRobertaConfig\n","from transformers import BertConfig, BertTokenizer, BertModel\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","plt.style.use('seaborn')"]},{"cell_type":"markdown","source":[""],"metadata":{"id":"HYmZcZKPC6zL"}},{"cell_type":"code","source":["dataset = DatasetDict()\n","dataset['train'] = load_dataset('glue', 'mrpc', split='train')\n","dataset['test'] = load_dataset('glue', 'mrpc', split='test')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eOR2zMPXy8RD","executionInfo":{"status":"ok","timestamp":1659708109499,"user_tz":-120,"elapsed":1137,"user":{"displayName":"AmirHossein Dabiri Aghdam","userId":"03377875757005762455"}},"outputId":"87771136-f2b9-4fa7-8b93-06c2be47fe86"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["08/05/2022 14:01:48 - WARNING - datasets.builder -   Reusing dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n","08/05/2022 14:01:48 - WARNING - datasets.builder -   Reusing dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"]}]},{"cell_type":"code","source":["dataset['train']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pGX_U0ulFc6K","executionInfo":{"status":"ok","timestamp":1659708123261,"user_tz":-120,"elapsed":221,"user":{"displayName":"AmirHossein Dabiri Aghdam","userId":"03377875757005762455"}},"outputId":"b29f4881-1487-499e-9565-ea7236dd2408"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['sentence1', 'sentence2', 'label', 'idx'],\n","    num_rows: 3668\n","})"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["# from torch.utils.data import Subset\n","# dataset['train'] = dataset['train'].shuffle(seed=42)\n","# dataset['train'] = Subset(dataset[\"train\"], list(range(380)))"],"metadata":{"id":"VoTEhNUcS5D8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Paraphrase_Forgettables_Info = pd.read_csv('/content/Paraphrase_Forgettables_Info.csv')\n","Paraphrase_Forgettables_Info"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"EnCg8yzCCvT7","executionInfo":{"status":"ok","timestamp":1659704935922,"user_tz":-120,"elapsed":1336,"user":{"displayName":"AmirHossein Dabiri Aghdam","userId":"03377875757005762455"}},"outputId":"175ecb21-f169-4de0-d473-0136961c4e17"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     Unnamed: 0  sample_index  forgetting_events   True_label_indexes\n","0             0             1                  1        [0 5 6 7 8 9]\n","1             1             5                  2        [0 4 6 7 8 9]\n","2             2             6                  1    [1 2 4 5 6 7 8 9]\n","3             3             9                  2    [0 2 4 5 6 7 8 9]\n","4             4            24                  1    [0 3 4 5 6 7 8 9]\n","..          ...           ...                ...                  ...\n","375         375          3570                  1    [1 3 4 5 6 7 8 9]\n","376         376          3603                  1  [0 2 3 4 5 6 7 8 9]\n","377         377          3624                  1  [0 1 2 3 4 6 7 8 9]\n","378         378          3651                  1  [0 1 3 4 5 6 7 8 9]\n","379         379          3662                  1  [0 1 2 3 4 6 7 8 9]\n","\n","[380 rows x 4 columns]"],"text/html":["\n","  <div id=\"df-e322a95f-4902-4d3a-80ac-cfaa6853691c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>sample_index</th>\n","      <th>forgetting_events</th>\n","      <th>True_label_indexes</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>[0 5 6 7 8 9]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>[0 4 6 7 8 9]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>[1 2 4 5 6 7 8 9]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>9</td>\n","      <td>2</td>\n","      <td>[0 2 4 5 6 7 8 9]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>24</td>\n","      <td>1</td>\n","      <td>[0 3 4 5 6 7 8 9]</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>375</th>\n","      <td>375</td>\n","      <td>3570</td>\n","      <td>1</td>\n","      <td>[1 3 4 5 6 7 8 9]</td>\n","    </tr>\n","    <tr>\n","      <th>376</th>\n","      <td>376</td>\n","      <td>3603</td>\n","      <td>1</td>\n","      <td>[0 2 3 4 5 6 7 8 9]</td>\n","    </tr>\n","    <tr>\n","      <th>377</th>\n","      <td>377</td>\n","      <td>3624</td>\n","      <td>1</td>\n","      <td>[0 1 2 3 4 6 7 8 9]</td>\n","    </tr>\n","    <tr>\n","      <th>378</th>\n","      <td>378</td>\n","      <td>3651</td>\n","      <td>1</td>\n","      <td>[0 1 3 4 5 6 7 8 9]</td>\n","    </tr>\n","    <tr>\n","      <th>379</th>\n","      <td>379</td>\n","      <td>3662</td>\n","      <td>1</td>\n","      <td>[0 1 2 3 4 6 7 8 9]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>380 rows × 4 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e322a95f-4902-4d3a-80ac-cfaa6853691c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e322a95f-4902-4d3a-80ac-cfaa6853691c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e322a95f-4902-4d3a-80ac-cfaa6853691c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["dataset['train'] = Dataset.from_pandas(pd.DataFrame.from_dict([dataset['train'][idx] for idx in Paraphrase_Forgettables_Info['sample_index']]))\n","dataset['train']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LiKa3lDkDSfr","executionInfo":{"status":"ok","timestamp":1659704935923,"user_tz":-120,"elapsed":12,"user":{"displayName":"AmirHossein Dabiri Aghdam","userId":"03377875757005762455"}},"outputId":"65d0f93f-e74e-43da-c770-54562e2a0b57"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['sentence1', 'sentence2', 'label', 'idx'],\n","    num_rows: 380\n","})"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":364},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1659704935924,"user":{"displayName":"AmirHossein Dabiri Aghdam","userId":"03377875757005762455"},"user_tz":-120},"id":"cDwxzAdY677O","outputId":"c7e58c7d-366a-488b-f26e-8de813f3293d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 0.40105540897097625, 1: 0.6015831134564644}"]},"metadata":{},"execution_count":11},{"output_type":"display_data","data":{"text/plain":["<Figure size 576x396 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["freq_dict = dict()\n","for data in dataset['train']:\n","    if (data['label'] in freq_dict): freq_dict[data['label']] += 1\n","    else: freq_dict[data['label']] = 1\n","\n","for k,v in freq_dict.items():\n","    freq_dict[k] = v / (len(dataset['train']) - 1)\n","\n","sns.countplot(x=dataset['train']['label'])\n","freq_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1659704936800,"user":{"displayName":"AmirHossein Dabiri Aghdam","userId":"03377875757005762455"},"user_tz":-120},"id":"CErTCZLp6lAY","outputId":"93640426-1a6b-4c44-9e9f-45dfe5344e2c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 0.3354614045269878, 1: 0.665699361578642}"]},"metadata":{},"execution_count":14},{"output_type":"display_data","data":{"text/plain":["<Figure size 576x396 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["freq_dict = dict()\n","for data in dataset['test']:\n","    if (data['label'] in freq_dict): freq_dict[data['label']] += 1\n","    else: freq_dict[data['label']] = 1\n","\n","for k,v in freq_dict.items():\n","    freq_dict[k] = v / (len(dataset['test']) - 2)\n","\n","sns.countplot(x=dataset['test']['label'])\n","freq_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NrJ4LDo2AEoy"},"outputs":[],"source":["class Textual_EntailementClassifier(nn.Module):\n","    def __init__(self, input_dim, num_labels=3, dropout_rate=0.):\n","        super(Textual_EntailementClassifier, self).__init__()\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.linear1 = nn.Linear(input_dim, input_dim // 3)\n","        self.relu = nn.ReLU()\n","        self.linear2 = nn.Linear(input_dim // 3, num_labels)\n","\n","    def forward(self, x):\n","        x = self.dropout(x)\n","        x = self.linear1(x)\n","        x = self.relu(x)\n","        return self.linear2(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qX9IcSy-Yzhk"},"outputs":[],"source":["class BERT_Textual_Entailement(BertModel):\n","    def __init__(self, config, args, label_lst):\n","        super(BERT_Textual_Entailement, self).__init__(config)\n","        self.args = args\n","        self.num_labels = len(set(label_lst))\n","        self.bert = BertModel(config=config) # Load pretrained Bert\n","        self.classifier = Textual_EntailementClassifier(config.hidden_size, self.num_labels, args.dropout_rate)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids, label_ids):\n","        outputs = self.bert(input_ids, attention_mask, token_type_ids)  # sequence_output, pooled_output, (hidden_states), (attentions)\n","        pooled_output = outputs[1]  # [CLS]\n","        label_logits = self.classifier(pooled_output)\n","\n","        total_loss = 0\n","        # Softmax\n","        if label_ids is not None:\n","            loss = 0\n","            if self.num_labels == 1:\n","                loss_fct = nn.MSELoss()\n","                loss = loss_fct(label_logits.view(-1), label_ids.view(-1))\n","            else:\n","                loss_fct = nn.CrossEntropyLoss()\n","                loss = loss_fct(label_logits.view(-1, self.num_labels), label_ids.view(-1))\n","            total_loss += loss\n","\n","        outputs = (label_logits,) + outputs[2:]  # add hidden states and attention if they are here\n","        outputs = (total_loss,) + outputs\n","\n","        return outputs  # (loss), logits, (hidden_states), (attentions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TT9SxAJmDY8_"},"outputs":[],"source":["class ParsBERT_Textual_Entailement(BertModel):\n","    def __init__(self, config, args, label_lst):\n","        super(ParsBERT_Textual_Entailement, self).__init__(config)\n","        self.args = args\n","        self.num_labels = len(set(label_lst))\n","        self.parsbert = BertModel(config=config) # Load pretrained ParsBert\n","        self.classifier = Textual_EntailementClassifier(config.hidden_size, self.num_labels, args.dropout_rate)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids, label_ids):\n","        outputs = self.parsbert(input_ids, attention_mask, token_type_ids)  # sequence_output, pooled_output, (hidden_states), (attentions)\n","        pooled_output = outputs[1]  # [CLS]\n","        label_logits = self.classifier(pooled_output)\n","\n","        total_loss = 0\n","        # Softmax\n","        if label_ids is not None:\n","            loss = 0\n","            if self.num_labels == 1:\n","                loss_fct = nn.MSELoss()\n","                loss = loss_fct(label_logits.view(-1), label_ids.view(-1))\n","            else:\n","                loss_fct = nn.CrossEntropyLoss()\n","                loss = loss_fct(label_logits.view(-1, self.num_labels), label_ids.view(-1))\n","            total_loss += loss\n","\n","        outputs = (label_logits,) + outputs[2:]  # add hidden states and attention if they are here\n","        outputs = (total_loss,) + outputs\n","\n","        return outputs  # (loss), logits, (hidden_states), (attentions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xRlNFn16WA4M"},"outputs":[],"source":["MODEL_CLASSES = {\n","    'bert': (BertConfig, BERT_Textual_Entailement, BertTokenizer),\n","    'parsbert': (BertConfig, ParsBERT_Textual_Entailement, BertTokenizer),\n","}\n","\n","MODEL_PATH_MAP = {\n","    # 'XLMRoberta': 'xlm-roberta-base',\n","    'bert': 'bert-base-uncased',\n","    'parsbert': 'HooshvareLab/bert-fa-zwnj-base',\n","}\n","\n","logger = logging.getLogger(__name__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FGZghD02Yzxv"},"outputs":[],"source":["def load_tokenizer(args):\n","    return MODEL_CLASSES[args.model_type][2].from_pretrained(args.model_name_or_path)\n","\n","def init_logger():\n","    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n","                        datefmt='%m/%d/%Y %H:%M:%S',\n","                        level=logging.INFO)\n","    \n","def label2index(label):\n","    if (label == 'neutral'): return 0\n","    elif (label == 'entails'): return 1\n","    # else: return 2\n","    else: return label\n","\n","def get_labels(dataset):\n","  train_label = list()\n","  for data in dataset:\n","      if (data['label'] != 'xx' and data['label'] != '-'): train_label.append(data['label'])\n","  \n","  return train_label\n","\n","def set_seed(args):\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    if not args.no_cuda and torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(args.seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dhLxqH9fr4jR"},"outputs":[],"source":["def prepare_dataset(data, args, tokenizer, padding='max_length'):\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    all_label = torch.tensor([label2index(x['label']) for x in data if  (x['label'] != 'xx' and x['label'] != '-')], dtype=torch.long).to(device)\n","    bert_input = tokenizer([x['sentence1'] for x in data if (x['label'] != 'xx' and x['label'] != '-')] , [x['sentence2'] for x in data if (x['label'] != 'xx' and x['label'] != '-')], padding=padding, max_length=args.max_seq_len, truncation=True, return_tensors=\"pt\")\n","    if ('token_type_ids' in bert_input):\n","        dataset = TensorDataset(bert_input['input_ids'].to(device), bert_input['attention_mask'].to(device), bert_input['token_type_ids'].to(device), all_label)\n","    else:\n","        dataset = TensorDataset(bert_input['input_ids'].to(device), bert_input['attention_mask'].to(device), torch.zeros_like(bert_input['input_ids']).to(device), all_label)\n","\n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3IEGTfNm1GZG"},"outputs":[],"source":["#define\n","OUTPUT_DIM = 2\n","NUM_EPOCHS = 10\n","Prob_per_epoch = np.zeros((NUM_EPOCHS,len(dataset['train']),OUTPUT_DIM)) #2 labels\n","\n","class Trainer(object):\n","    def __init__(self, args, train_dataset=None, dev_dataset=None, test_dataset=None):\n","        self.args = args\n","        self.train_dataset = train_dataset\n","        self.dev_dataset = dev_dataset\n","        self.test_dataset = test_dataset\n","\n","        self.label_lst = get_labels(dataset['train'])\n","\n","        self.config_class, self.model_class, _ = MODEL_CLASSES[args.model_type]\n","        self.config = self.config_class.from_pretrained(args.model_name_or_path, finetuning_task=args.task)\n","        self.model = self.model_class.from_pretrained(args.model_name_or_path,\n","                                                      config=self.config,\n","                                                      args=args,\n","                                                      label_lst=self.label_lst)\n","                                                      \n","\n","        # GPU or CPU\n","        self.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n","        self.model.to(self.device)\n","\n","    def train(self):\n","        # train_sampler = RandomSampler(self.train_dataset)\n","        train_sampler = SequentialSampler(self.train_dataset)\n","        train_dataloader = DataLoader(self.train_dataset, sampler=train_sampler, batch_size=self.args.train_batch_size)\n","        if self.args.max_steps > 0:\n","            t_total = self.args.max_steps\n","            self.args.num_train_epochs = self.args.max_steps // (len(train_dataloader) // self.args.gradient_accumulation_steps) + 1\n","        else:\n","            t_total = len(train_dataloader) // self.args.gradient_accumulation_steps * self.args.num_train_epochs\n","\n","        # Prepare optimizer and schedule (linear warmup and decay)\n","        no_decay = ['bias', 'LayerNorm.weight']\n","        optimizer_grouped_parameters = [\n","            {'params': [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n","             'weight_decay': self.args.weight_decay},\n","            {'params': [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","        ]\n","        optimizer = AdamW(optimizer_grouped_parameters, lr=self.args.learning_rate, eps=self.args.adam_epsilon)\n","        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=t_total)\n","\n","        # Train!\n","        logger.info(\"***** Running training *****\")\n","        logger.info(\"  Num examples = %d\", len(self.train_dataset))\n","        logger.info(\"  Num Epochs = %d\", self.args.num_train_epochs)\n","        logger.info(\"  Total train batch size = %d\", self.args.train_batch_size)\n","        logger.info(\"  Gradient Accumulation steps = %d\", self.args.gradient_accumulation_steps)\n","        logger.info(\"  Total optimization steps = %d\", t_total)\n","        logger.info(\"  Logging steps = %d\", self.args.logging_steps)\n","        logger.info(\"  Save steps = %d\", self.args.save_steps)\n","\n","        global_step = 0\n","        tr_loss = 0.0\n","        self.model.zero_grad()\n","        best_acc = -1\n","       \n","        train_iterator = trange(int(self.args.num_train_epochs), desc=\"Epoch\")\n","        for epoch in train_iterator:\n","            label_preds = None\n","            out_label_ids = None\n","            # epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n","            stacked_train_preds = None\n","            for step, batch in enumerate(train_dataloader):\n","                self.model.train()\n","                inputs = {'input_ids': batch[0],\n","                          'attention_mask': batch[1],\n","                          'token_type_ids': batch[2],\n","                          'label_ids': batch[3]}\n","                outputs = self.model(**inputs)\n","                loss = outputs[0]\n","                label_logits = outputs[1]\n","\n","                # ############## Extracting samples'probability ##############\n","                # # Get the preds\n","                preds = nn.Softmax(dim=1)(label_logits) # convert to probability\n","\n","\n","                # # Move preds to the CPU\n","                train_preds = preds.detach().cpu().numpy()\n","                \n","                if stacked_train_preds is None:  # first batch\n","                    stacked_train_preds = train_preds\n","                else:\n","                  stacked_train_preds = np.vstack((stacked_train_preds, train_preds))\n","                \n","                # #######################  END ############################\n","\n","                 # label prediction\n","                if label_preds is None:\n","                    label_preds = label_logits.detach().cpu().numpy()\n","                    out_label_ids = inputs['label_ids'].detach().cpu().numpy()\n","                else:\n","                    label_preds = np.append(label_preds, label_logits.detach().cpu().numpy(), axis=0)\n","                    out_label_ids = np.append(out_label_ids, inputs['label_ids'].detach().cpu().numpy(), axis=0)\n","                \n","                if self.args.gradient_accumulation_steps > 1:\n","                    loss = loss / self.args.gradient_accumulation_steps\n","\n","                loss.backward()\n","\n","                tr_loss += loss.item()\n","                if (step + 1) % self.args.gradient_accumulation_steps == 0:\n","                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.max_grad_norm)\n","\n","                    optimizer.step()\n","                    scheduler.step()  # Update learning rate schedule\n","                    self.model.zero_grad()\n","                    global_step += 1\n","\n","                    if(global_step % 10 == 0): logger.info(\"Train loss = %.4f\", tr_loss / global_step)\n","                    if (self.args.logging_steps > 0 and global_step % self.args.logging_steps == 0):\n","                        results = self.evaluate(\"dev\")\n","                        if (float(results['accuracy']) > best_acc):\n","                        # if (float(results['slot_f1']) > best_f1):\n","                        #   best_f1 = float(results['slot_f1'])\n","                          best_acc = float(results['accuracy'])\n","                          logger.info(\"dev best_acc = %.4f\", best_acc * 100.0)\n","                          self.save_model()\n","\n","                   \n","            Prob_per_epoch[epoch][:,:] = stacked_train_preds\n","            \n","            if 0 < self.args.max_steps < global_step:\n","                train_iterator.close()\n","                break\n","            \n","            # label result\n","            label_preds = np.argmax(label_preds, axis=1)\n","            logger.info(\"train_acc = %.4f\", accuracy_score(out_label_ids, label_preds) * 100.0)\n","\n","\n","        return global_step, tr_loss / global_step\n","\n","    def evaluate(self, mode):\n","        if mode == 'test':\n","            dataset = self.test_dataset\n","        elif mode == 'dev':\n","            dataset = self.dev_dataset\n","        else:\n","            raise Exception(\"Only dev and test dataset available\")\n","\n","        eval_sampler = SequentialSampler(dataset)\n","        eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=self.args.eval_batch_size)\n","\n","        # Eval!\n","        logger.info(\"\\n***** Running evaluation on %s dataset *****\", mode)\n","        logger.info(\"  Num examples = %d\", len(dataset))\n","        logger.info(\"  Batch size = %d\", self.args.eval_batch_size)\n","        eval_loss = 0.0\n","        nb_eval_steps = 0\n","        label_preds = None\n","        out_label_ids = None\n","\n","        self.model.eval()\n","\n","        # for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","        for batch in eval_dataloader:\n","            batch = tuple(t.to(self.device) for t in batch)\n","            with torch.no_grad():\n","                inputs = {'input_ids': batch[0],\n","                          'attention_mask': batch[1],\n","                          'token_type_ids': batch[2],\n","                          'label_ids': batch[3]}\n","                outputs = self.model(**inputs)\n","                tmp_eval_loss, label_logits = outputs[:2]\n","                eval_loss += tmp_eval_loss.mean().item()\n","            \n","            nb_eval_steps += 1\n","\n","            # label prediction\n","            if label_preds is None:\n","                label_preds = label_logits.detach().cpu().numpy()\n","                out_label_ids = inputs['label_ids'].detach().cpu().numpy()\n","            else:\n","                label_preds = np.append(label_preds, label_logits.detach().cpu().numpy(), axis=0)\n","                out_label_ids = np.append(out_label_ids, inputs['label_ids'].detach().cpu().numpy(), axis=0)\n","\n","\n","        eval_loss = eval_loss / nb_eval_steps\n","        results = {\n","            \"loss\": eval_loss\n","        }\n","\n","        # label result\n","        label_preds = np.argmax(label_preds, axis=1)\n","        print(set(label_preds))\n","        total_result = accuracy_score(out_label_ids, label_preds)\n","        results.update({'accuracy' : total_result})\n","        logger.info(\"***** Eval results *****\")\n","        for key in sorted(results.keys()):\n","            logger.info(\"  %s = %.4f\", key if key != 'loss' else 'loss', float(results[key]) * 100.0 if key != 'loss' else float(results[key]))\n","\n","        if (mode == 'test'): print(classification_report(out_label_ids, label_preds, target_names=['neutral', 'entails'], digits = 4))\n","        return results\n","\n","    def save_model(self):\n","        # Save model checkpoint (Overwrite)\n","        if not os.path.exists(self.args.model_dir):\n","            os.makedirs(self.args.model_dir)\n","        model_to_save = self.model.module if hasattr(self.model, 'module') else self.model\n","        model_to_save.save_pretrained(self.args.model_dir)\n","\n","\n","    def load_model(self):\n","        # Check whether model exists\n","        if not os.path.exists(self.args.model_dir):\n","            raise Exception(\"Model doesn't exists! Train first!\")\n","\n","        try:\n","            self.model = self.model_class.from_pretrained(self.args.model_dir,\n","                                                          args=self.args,\n","                                                          label_lst=self.label_lst)\n","            self.model.to(self.device)\n","            logger.info(\"***** Model Loaded *****\")\n","        except:\n","            raise Exception(\"Some model files might be missing...\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ydHioDjUJMeA"},"outputs":[],"source":["class dotdict(dict):\n","    \"\"\"dot.notation access to dictionary attributes\"\"\"\n","    __getattr__ = dict.get\n","    __setattr__ = dict.__setitem__\n","    __delattr__ = dict.__delitem__"]},{"cell_type":"markdown","metadata":{"id":"ErWVnaL9ahyP"},"source":["Using bert\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YMs8BTqnJOEf"},"outputs":[],"source":["args = dotdict(dict())\n","args.seed = 810197502\n","args.model_type = 'bert' \n","args.model_name_or_path = MODEL_PATH_MAP[args.model_type]\n","args.dropout_rate =  0.3\n","args.do_train = True\n","args.do_eval = True\n","args.train_batch_size = 32 \n","args.max_steps = -1\n","args.task = 'Paraphrase' \n","args.no_cuda = False\n","args.weight_decay = 0\n","args.num_train_epochs = NUM_EPOCHS\n","args.gradient_accumulation_steps = 1\n","args.learning_rate = 3e-5 \n","args.adam_epsilon = 1e-8\n","args.warmup_steps = 0 \n","args.logging_steps = 200\n","args.save_steps = 500\n","args.max_grad_norm = 1\n","args.eval_batch_size = 64\n","args.model_dir = \"Paraphrase_model_\" + args.model_type\n","args.max_seq_len = 128\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KeuvpC-ZNMRD"},"outputs":[],"source":["init_logger()\n","set_seed(args)\n","tokenizer = load_tokenizer(args)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RPUnSZlUq9kg"},"outputs":[],"source":["train_dataset = prepare_dataset(dataset['train'], args, tokenizer)\n","# valid_dataset = prepare_dataset(dataset['validation'], args, tokenizer)\n","test_dataset = prepare_dataset(dataset['test'], args, tokenizer)\n","valid_dataset = test_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3106,"status":"ok","timestamp":1659708296751,"user":{"displayName":"AmirHossein Dabiri Aghdam","userId":"03377875757005762455"},"user_tz":-120},"id":"AkmMPQ4ec7rw","outputId":"64ae2971-d8e8-4955-af76-5405596cc49c"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BERT_Textual_Entailement: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BERT_Textual_Entailement from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BERT_Textual_Entailement from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BERT_Textual_Entailement were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['encoder.layer.5.attention.self.key.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'classifier.linear2.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'classifier.linear2.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'pooler.dense.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.5.intermediate.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.7.attention.self.query.weight', 'classifier.linear1.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.3.output.dense.weight', 'pooler.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.bias', 'classifier.linear1.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.4.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.attention.self.key.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["trainer = Trainer(args, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XoTrRNhZxRDU"},"outputs":[],"source":["gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"code","source":["# all\n","if args.do_train:\n","    # trainer.load_model()\n","    trainer.train()\n","\n","if args.do_eval:\n","    # trainer.load_model()\n","    trainer.evaluate(\"test\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sPOneSsUTPrR","executionInfo":{"status":"ok","timestamp":1659709149500,"user_tz":-120,"elapsed":852533,"user":{"displayName":"AmirHossein Dabiri Aghdam","userId":"03377875757005762455"}},"outputId":"0ad02320-cf8a-40ed-8b8f-0036d81d6adc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","08/05/2022 14:04:56 - INFO - __main__ -   ***** Running training *****\n","08/05/2022 14:04:56 - INFO - __main__ -     Num examples = 3668\n","08/05/2022 14:04:56 - INFO - __main__ -     Num Epochs = 10\n","08/05/2022 14:04:56 - INFO - __main__ -     Total train batch size = 32\n","08/05/2022 14:04:56 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/05/2022 14:04:56 - INFO - __main__ -     Total optimization steps = 1150\n","08/05/2022 14:04:56 - INFO - __main__ -     Logging steps = 200\n","08/05/2022 14:04:56 - INFO - __main__ -     Save steps = 500\n","Epoch:   0%|          | 0/10 [00:00<?, ?it/s]08/05/2022 14:05:03 - INFO - __main__ -   Train loss = 0.6627\n","08/05/2022 14:05:10 - INFO - __main__ -   Train loss = 0.6572\n","08/05/2022 14:05:17 - INFO - __main__ -   Train loss = 0.6481\n","08/05/2022 14:05:24 - INFO - __main__ -   Train loss = 0.6336\n","08/05/2022 14:05:30 - INFO - __main__ -   Train loss = 0.6248\n","08/05/2022 14:05:37 - INFO - __main__ -   Train loss = 0.6119\n","08/05/2022 14:05:44 - INFO - __main__ -   Train loss = 0.6065\n","08/05/2022 14:05:51 - INFO - __main__ -   Train loss = 0.5972\n","08/05/2022 14:05:57 - INFO - __main__ -   Train loss = 0.5819\n","08/05/2022 14:06:04 - INFO - __main__ -   Train loss = 0.5721\n","08/05/2022 14:06:11 - INFO - __main__ -   Train loss = 0.5611\n","08/05/2022 14:06:14 - INFO - __main__ -   train_acc = 71.6467\n","Epoch:  10%|█         | 1/10 [01:17<11:37, 77.51s/it]08/05/2022 14:06:17 - INFO - __main__ -   Train loss = 0.5556\n","08/05/2022 14:06:24 - INFO - __main__ -   Train loss = 0.5443\n","08/05/2022 14:06:30 - INFO - __main__ -   Train loss = 0.5356\n","08/05/2022 14:06:37 - INFO - __main__ -   Train loss = 0.5262\n","08/05/2022 14:06:44 - INFO - __main__ -   Train loss = 0.5141\n","08/05/2022 14:06:51 - INFO - __main__ -   Train loss = 0.5036\n","08/05/2022 14:06:57 - INFO - __main__ -   Train loss = 0.4923\n","08/05/2022 14:07:04 - INFO - __main__ -   Train loss = 0.4851\n","08/05/2022 14:07:11 - INFO - __main__ -   Train loss = 0.4772\n","08/05/2022 14:07:11 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 14:07:11 - INFO - __main__ -     Num examples = 1725\n","08/05/2022 14:07:11 - INFO - __main__ -     Batch size = 64\n","08/05/2022 14:07:24 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 14:07:24 - INFO - __main__ -     accuracy = 83.8841\n","08/05/2022 14:07:24 - INFO - __main__ -     loss = 0.4203\n","08/05/2022 14:07:24 - INFO - __main__ -   dev best_acc = 83.8841\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 14:07:33 - INFO - __main__ -   Train loss = 0.4688\n","08/05/2022 14:07:40 - INFO - __main__ -   Train loss = 0.4588\n","08/05/2022 14:07:47 - INFO - __main__ -   Train loss = 0.4485\n","08/05/2022 14:07:47 - INFO - __main__ -   train_acc = 86.8321\n","Epoch:  20%|██        | 2/10 [02:50<11:32, 86.54s/it]08/05/2022 14:07:53 - INFO - __main__ -   Train loss = 0.4391\n","08/05/2022 14:08:00 - INFO - __main__ -   Train loss = 0.4304\n","08/05/2022 14:08:07 - INFO - __main__ -   Train loss = 0.4211\n","08/05/2022 14:08:13 - INFO - __main__ -   Train loss = 0.4119\n","08/05/2022 14:08:20 - INFO - __main__ -   Train loss = 0.4075\n","08/05/2022 14:08:27 - INFO - __main__ -   Train loss = 0.4002\n","08/05/2022 14:08:33 - INFO - __main__ -   Train loss = 0.3927\n","08/05/2022 14:08:40 - INFO - __main__ -   Train loss = 0.3876\n","08/05/2022 14:08:47 - INFO - __main__ -   Train loss = 0.3801\n","08/05/2022 14:08:54 - INFO - __main__ -   Train loss = 0.3733\n","08/05/2022 14:09:00 - INFO - __main__ -   Train loss = 0.3649\n","08/05/2022 14:09:03 - INFO - __main__ -   train_acc = 93.7568\n","Epoch:  30%|███       | 3/10 [04:07<09:34, 82.12s/it]08/05/2022 14:09:07 - INFO - __main__ -   Train loss = 0.3592\n","08/05/2022 14:09:13 - INFO - __main__ -   Train loss = 0.3531\n","08/05/2022 14:09:20 - INFO - __main__ -   Train loss = 0.3461\n","08/05/2022 14:09:27 - INFO - __main__ -   Train loss = 0.3395\n","08/05/2022 14:09:33 - INFO - __main__ -   Train loss = 0.3343\n","08/05/2022 14:09:40 - INFO - __main__ -   Train loss = 0.3281\n","08/05/2022 14:09:40 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 14:09:40 - INFO - __main__ -     Num examples = 1725\n","08/05/2022 14:09:40 - INFO - __main__ -     Batch size = 64\n","08/05/2022 14:09:53 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 14:09:53 - INFO - __main__ -     accuracy = 83.7681\n","08/05/2022 14:09:53 - INFO - __main__ -     loss = 0.5208\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 14:10:00 - INFO - __main__ -   Train loss = 0.3226\n","08/05/2022 14:10:06 - INFO - __main__ -   Train loss = 0.3178\n","08/05/2022 14:10:13 - INFO - __main__ -   Train loss = 0.3128\n","08/05/2022 14:10:20 - INFO - __main__ -   Train loss = 0.3076\n","08/05/2022 14:10:27 - INFO - __main__ -   Train loss = 0.3024\n","08/05/2022 14:10:33 - INFO - __main__ -   Train loss = 0.2974\n","08/05/2022 14:10:33 - INFO - __main__ -   train_acc = 96.9466\n","Epoch:  40%|████      | 4/10 [05:36<08:30, 85.08s/it]08/05/2022 14:10:40 - INFO - __main__ -   Train loss = 0.2928\n","08/05/2022 14:10:46 - INFO - __main__ -   Train loss = 0.2891\n","08/05/2022 14:10:53 - INFO - __main__ -   Train loss = 0.2840\n","08/05/2022 14:11:00 - INFO - __main__ -   Train loss = 0.2792\n","08/05/2022 14:11:06 - INFO - __main__ -   Train loss = 0.2757\n","08/05/2022 14:11:13 - INFO - __main__ -   Train loss = 0.2709\n","08/05/2022 14:11:20 - INFO - __main__ -   Train loss = 0.2675\n","08/05/2022 14:11:27 - INFO - __main__ -   Train loss = 0.2645\n","08/05/2022 14:11:33 - INFO - __main__ -   Train loss = 0.2610\n","08/05/2022 14:11:40 - INFO - __main__ -   Train loss = 0.2578\n","08/05/2022 14:11:47 - INFO - __main__ -   Train loss = 0.2535\n","08/05/2022 14:11:50 - INFO - __main__ -   train_acc = 98.1189\n","Epoch:  50%|█████     | 5/10 [06:53<06:50, 82.05s/it]08/05/2022 14:11:53 - INFO - __main__ -   Train loss = 0.2504\n","08/05/2022 14:12:00 - INFO - __main__ -   Train loss = 0.2469\n","08/05/2022 14:12:06 - INFO - __main__ -   Train loss = 0.2429\n","08/05/2022 14:12:06 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 14:12:06 - INFO - __main__ -     Num examples = 1725\n","08/05/2022 14:12:06 - INFO - __main__ -     Batch size = 64\n","08/05/2022 14:12:19 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 14:12:19 - INFO - __main__ -     accuracy = 84.1739\n","08/05/2022 14:12:19 - INFO - __main__ -     loss = 0.6898\n","08/05/2022 14:12:19 - INFO - __main__ -   dev best_acc = 84.1739\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 14:12:29 - INFO - __main__ -   Train loss = 0.2395\n","08/05/2022 14:12:36 - INFO - __main__ -   Train loss = 0.2366\n","08/05/2022 14:12:43 - INFO - __main__ -   Train loss = 0.2332\n","08/05/2022 14:12:49 - INFO - __main__ -   Train loss = 0.2303\n","08/05/2022 14:12:56 - INFO - __main__ -   Train loss = 0.2272\n","08/05/2022 14:13:03 - INFO - __main__ -   Train loss = 0.2247\n","08/05/2022 14:13:09 - INFO - __main__ -   Train loss = 0.2220\n","08/05/2022 14:13:16 - INFO - __main__ -   Train loss = 0.2188\n","08/05/2022 14:13:23 - INFO - __main__ -   Train loss = 0.2160\n","08/05/2022 14:13:23 - INFO - __main__ -   train_acc = 99.1276\n","Epoch:  60%|██████    | 6/10 [08:26<05:43, 85.75s/it]08/05/2022 14:13:29 - INFO - __main__ -   Train loss = 0.2132\n","08/05/2022 14:13:36 - INFO - __main__ -   Train loss = 0.2109\n","08/05/2022 14:13:43 - INFO - __main__ -   Train loss = 0.2082\n","08/05/2022 14:13:49 - INFO - __main__ -   Train loss = 0.2059\n","08/05/2022 14:13:56 - INFO - __main__ -   Train loss = 0.2038\n","08/05/2022 14:14:03 - INFO - __main__ -   Train loss = 0.2016\n","08/05/2022 14:14:09 - INFO - __main__ -   Train loss = 0.1996\n","08/05/2022 14:14:16 - INFO - __main__ -   Train loss = 0.1972\n","08/05/2022 14:14:23 - INFO - __main__ -   Train loss = 0.1951\n","08/05/2022 14:14:30 - INFO - __main__ -   Train loss = 0.1930\n","08/05/2022 14:14:36 - INFO - __main__ -   Train loss = 0.1906\n","08/05/2022 14:14:36 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 14:14:36 - INFO - __main__ -     Num examples = 1725\n","08/05/2022 14:14:36 - INFO - __main__ -     Batch size = 64\n","08/05/2022 14:14:49 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 14:14:49 - INFO - __main__ -     accuracy = 83.9420\n","08/05/2022 14:14:49 - INFO - __main__ -     loss = 0.7772\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 14:14:52 - INFO - __main__ -   train_acc = 99.3184\n","Epoch:  70%|███████   | 7/10 [09:56<04:21, 87.01s/it]08/05/2022 14:14:56 - INFO - __main__ -   Train loss = 0.1885\n","08/05/2022 14:15:02 - INFO - __main__ -   Train loss = 0.1868\n","08/05/2022 14:15:09 - INFO - __main__ -   Train loss = 0.1845\n","08/05/2022 14:15:16 - INFO - __main__ -   Train loss = 0.1826\n","08/05/2022 14:15:22 - INFO - __main__ -   Train loss = 0.1807\n","08/05/2022 14:15:29 - INFO - __main__ -   Train loss = 0.1787\n","08/05/2022 14:15:36 - INFO - __main__ -   Train loss = 0.1768\n","08/05/2022 14:15:42 - INFO - __main__ -   Train loss = 0.1750\n","08/05/2022 14:15:49 - INFO - __main__ -   Train loss = 0.1734\n","08/05/2022 14:15:56 - INFO - __main__ -   Train loss = 0.1717\n","08/05/2022 14:16:03 - INFO - __main__ -   Train loss = 0.1699\n","08/05/2022 14:16:09 - INFO - __main__ -   Train loss = 0.1681\n","08/05/2022 14:16:09 - INFO - __main__ -   train_acc = 99.5911\n","Epoch:  80%|████████  | 8/10 [11:12<02:47, 83.78s/it]08/05/2022 14:16:16 - INFO - __main__ -   Train loss = 0.1663\n","08/05/2022 14:16:22 - INFO - __main__ -   Train loss = 0.1650\n","08/05/2022 14:16:29 - INFO - __main__ -   Train loss = 0.1633\n","08/05/2022 14:16:36 - INFO - __main__ -   Train loss = 0.1618\n","08/05/2022 14:16:43 - INFO - __main__ -   Train loss = 0.1605\n","08/05/2022 14:16:49 - INFO - __main__ -   Train loss = 0.1589\n","08/05/2022 14:16:56 - INFO - __main__ -   Train loss = 0.1573\n","08/05/2022 14:17:03 - INFO - __main__ -   Train loss = 0.1559\n","08/05/2022 14:17:03 - INFO - __main__ -   \n","***** Running evaluation on dev dataset *****\n","08/05/2022 14:17:03 - INFO - __main__ -     Num examples = 1725\n","08/05/2022 14:17:03 - INFO - __main__ -     Batch size = 64\n","08/05/2022 14:17:15 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 14:17:15 - INFO - __main__ -     accuracy = 83.7101\n","08/05/2022 14:17:15 - INFO - __main__ -     loss = 0.8258\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n"]},{"output_type":"stream","name":"stderr","text":["08/05/2022 14:17:22 - INFO - __main__ -   Train loss = 0.1546\n","08/05/2022 14:17:29 - INFO - __main__ -   Train loss = 0.1531\n","08/05/2022 14:17:36 - INFO - __main__ -   Train loss = 0.1516\n","08/05/2022 14:17:39 - INFO - __main__ -   train_acc = 99.7001\n","Epoch:  90%|█████████ | 9/10 [12:42<01:25, 85.60s/it]08/05/2022 14:17:42 - INFO - __main__ -   Train loss = 0.1502\n","08/05/2022 14:17:49 - INFO - __main__ -   Train loss = 0.1490\n","08/05/2022 14:17:55 - INFO - __main__ -   Train loss = 0.1478\n","08/05/2022 14:18:02 - INFO - __main__ -   Train loss = 0.1465\n","08/05/2022 14:18:09 - INFO - __main__ -   Train loss = 0.1454\n","08/05/2022 14:18:15 - INFO - __main__ -   Train loss = 0.1440\n","08/05/2022 14:18:22 - INFO - __main__ -   Train loss = 0.1428\n","08/05/2022 14:18:29 - INFO - __main__ -   Train loss = 0.1415\n","08/05/2022 14:18:36 - INFO - __main__ -   Train loss = 0.1404\n","08/05/2022 14:18:42 - INFO - __main__ -   Train loss = 0.1392\n","08/05/2022 14:18:49 - INFO - __main__ -   Train loss = 0.1380\n","08/05/2022 14:18:55 - INFO - __main__ -   Train loss = 0.1368\n","08/05/2022 14:18:55 - INFO - __main__ -   train_acc = 99.8092\n","Epoch: 100%|██████████| 10/10 [13:59<00:00, 83.94s/it]\n","08/05/2022 14:18:56 - INFO - __main__ -   \n","***** Running evaluation on test dataset *****\n","08/05/2022 14:18:56 - INFO - __main__ -     Num examples = 1725\n","08/05/2022 14:18:56 - INFO - __main__ -     Batch size = 64\n","08/05/2022 14:19:08 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 14:19:08 - INFO - __main__ -     accuracy = 83.8841\n","08/05/2022 14:19:08 - INFO - __main__ -     loss = 0.8311\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n","              precision    recall  f1-score   support\n","\n","     neutral     0.7941    0.7007    0.7445       578\n","     entails     0.8576    0.9085    0.8823      1147\n","\n","    accuracy                         0.8388      1725\n","   macro avg     0.8259    0.8046    0.8134      1725\n","weighted avg     0.8363    0.8388    0.8361      1725\n","\n"]}]},{"cell_type":"code","source":["# random\n","if args.do_train:\n","    # trainer.load_model()\n","    trainer.train()\n","\n","if args.do_eval:\n","    # trainer.load_model()\n","    trainer.evaluate(\"test\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dwoRzR6xOggq","executionInfo":{"status":"ok","timestamp":1659708070205,"user_tz":-120,"elapsed":94870,"user":{"displayName":"AmirHossein Dabiri Aghdam","userId":"03377875757005762455"}},"outputId":"13378814-fc5b-4a9e-dfcf-baeccd982b21"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","08/05/2022 13:59:34 - INFO - __main__ -   ***** Running training *****\n","08/05/2022 13:59:34 - INFO - __main__ -     Num examples = 380\n","08/05/2022 13:59:34 - INFO - __main__ -     Num Epochs = 10\n","08/05/2022 13:59:34 - INFO - __main__ -     Total train batch size = 32\n","08/05/2022 13:59:34 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/05/2022 13:59:34 - INFO - __main__ -     Total optimization steps = 120\n","08/05/2022 13:59:34 - INFO - __main__ -     Logging steps = 200\n","08/05/2022 13:59:34 - INFO - __main__ -     Save steps = 500\n","Epoch:   0%|          | 0/10 [00:00<?, ?it/s]08/05/2022 13:59:43 - INFO - __main__ -   Train loss = 0.6657\n","08/05/2022 13:59:44 - INFO - __main__ -   train_acc = 62.3684\n","Epoch:  10%|█         | 1/10 [00:09<01:28,  9.87s/it]08/05/2022 13:59:50 - INFO - __main__ -   Train loss = 0.6588\n","08/05/2022 13:59:52 - INFO - __main__ -   train_acc = 64.2105\n","Epoch:  20%|██        | 2/10 [00:17<01:08,  8.59s/it]08/05/2022 13:59:56 - INFO - __main__ -   Train loss = 0.6402\n","08/05/2022 14:00:00 - INFO - __main__ -   train_acc = 73.6842\n","Epoch:  30%|███       | 3/10 [00:25<00:57,  8.24s/it]08/05/2022 14:00:03 - INFO - __main__ -   Train loss = 0.6065\n","08/05/2022 14:00:08 - INFO - __main__ -   train_acc = 91.0526\n","Epoch:  40%|████      | 4/10 [00:33<00:48,  8.13s/it]08/05/2022 14:00:09 - INFO - __main__ -   Train loss = 0.5641\n","08/05/2022 14:00:16 - INFO - __main__ -   Train loss = 0.5185\n","08/05/2022 14:00:16 - INFO - __main__ -   train_acc = 94.4737\n","Epoch:  50%|█████     | 5/10 [00:41<00:40,  8.10s/it]08/05/2022 14:00:23 - INFO - __main__ -   Train loss = 0.4765\n","08/05/2022 14:00:24 - INFO - __main__ -   train_acc = 97.6316\n","Epoch:  60%|██████    | 6/10 [00:49<00:32,  8.13s/it]08/05/2022 14:00:30 - INFO - __main__ -   Train loss = 0.4378\n","08/05/2022 14:00:32 - INFO - __main__ -   train_acc = 98.6842\n","Epoch:  70%|███████   | 7/10 [00:57<00:24,  8.15s/it]08/05/2022 14:00:36 - INFO - __main__ -   Train loss = 0.4044\n","08/05/2022 14:00:40 - INFO - __main__ -   train_acc = 99.4737\n","Epoch:  80%|████████  | 8/10 [01:05<00:16,  8.15s/it]08/05/2022 14:00:43 - INFO - __main__ -   Train loss = 0.3749\n","08/05/2022 14:00:48 - INFO - __main__ -   train_acc = 99.4737\n","Epoch:  90%|█████████ | 9/10 [01:13<00:08,  8.12s/it]08/05/2022 14:00:50 - INFO - __main__ -   Train loss = 0.3501\n","08/05/2022 14:00:56 - INFO - __main__ -   Train loss = 0.3290\n","08/05/2022 14:00:56 - INFO - __main__ -   train_acc = 99.4737\n","Epoch: 100%|██████████| 10/10 [01:21<00:00,  8.20s/it]\n","08/05/2022 14:00:56 - INFO - __main__ -   \n","***** Running evaluation on test dataset *****\n","08/05/2022 14:00:56 - INFO - __main__ -     Num examples = 1725\n","08/05/2022 14:00:56 - INFO - __main__ -     Batch size = 64\n","08/05/2022 14:01:09 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 14:01:09 - INFO - __main__ -     accuracy = 71.2464\n","08/05/2022 14:01:09 - INFO - __main__ -     loss = 0.7210\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n","              precision    recall  f1-score   support\n","\n","     neutral     0.6090    0.3962    0.4801       578\n","     entails     0.7413    0.8718    0.8013      1147\n","\n","    accuracy                         0.7125      1725\n","   macro avg     0.6752    0.6340    0.6407      1725\n","weighted avg     0.6970    0.7125    0.6937      1725\n","\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7xNxHo9NNbuK","outputId":"b9afef59-64ff-4193-d3c3-2662f903a905","executionInfo":{"status":"ok","timestamp":1659705066014,"user_tz":-120,"elapsed":96617,"user":{"displayName":"AmirHossein Dabiri Aghdam","userId":"03377875757005762455"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","08/05/2022 13:09:28 - INFO - __main__ -   ***** Running training *****\n","08/05/2022 13:09:28 - INFO - __main__ -     Num examples = 380\n","08/05/2022 13:09:28 - INFO - __main__ -     Num Epochs = 10\n","08/05/2022 13:09:28 - INFO - __main__ -     Total train batch size = 32\n","08/05/2022 13:09:28 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/05/2022 13:09:28 - INFO - __main__ -     Total optimization steps = 120\n","08/05/2022 13:09:28 - INFO - __main__ -     Logging steps = 200\n","08/05/2022 13:09:28 - INFO - __main__ -     Save steps = 500\n","Epoch:   0%|          | 0/10 [00:00<?, ?it/s]08/05/2022 13:09:37 - INFO - __main__ -   Train loss = 0.6716\n","08/05/2022 13:09:38 - INFO - __main__ -   train_acc = 57.3684\n","Epoch:  10%|█         | 1/10 [00:10<01:31, 10.17s/it]08/05/2022 13:09:43 - INFO - __main__ -   Train loss = 0.6734\n","08/05/2022 13:09:45 - INFO - __main__ -   train_acc = 60.0000\n","Epoch:  20%|██        | 2/10 [00:17<01:09,  8.63s/it]08/05/2022 13:09:49 - INFO - __main__ -   Train loss = 0.6692\n","08/05/2022 13:09:53 - INFO - __main__ -   train_acc = 61.5789\n","Epoch:  30%|███       | 3/10 [00:25<00:57,  8.20s/it]08/05/2022 13:09:56 - INFO - __main__ -   Train loss = 0.6691\n","08/05/2022 13:10:01 - INFO - __main__ -   train_acc = 63.4211\n","Epoch:  40%|████      | 4/10 [00:33<00:48,  8.02s/it]08/05/2022 13:10:02 - INFO - __main__ -   Train loss = 0.6628\n","08/05/2022 13:10:09 - INFO - __main__ -   Train loss = 0.6491\n","08/05/2022 13:10:09 - INFO - __main__ -   train_acc = 75.7895\n","Epoch:  50%|█████     | 5/10 [00:41<00:39,  7.97s/it]08/05/2022 13:10:15 - INFO - __main__ -   Train loss = 0.6289\n","08/05/2022 13:10:17 - INFO - __main__ -   train_acc = 80.2632\n","Epoch:  60%|██████    | 6/10 [00:48<00:31,  7.97s/it]08/05/2022 13:10:22 - INFO - __main__ -   Train loss = 0.6104\n","08/05/2022 13:10:25 - INFO - __main__ -   train_acc = 88.4211\n","Epoch:  70%|███████   | 7/10 [00:57<00:24,  8.03s/it]08/05/2022 13:10:29 - INFO - __main__ -   Train loss = 0.5847\n","08/05/2022 13:10:33 - INFO - __main__ -   train_acc = 92.8947\n","Epoch:  80%|████████  | 8/10 [01:05<00:16,  8.13s/it]08/05/2022 13:10:36 - INFO - __main__ -   Train loss = 0.5613\n","08/05/2022 13:10:42 - INFO - __main__ -   train_acc = 95.7895\n","Epoch:  90%|█████████ | 9/10 [01:14<00:08,  8.26s/it]08/05/2022 13:10:43 - INFO - __main__ -   Train loss = 0.5383\n","08/05/2022 13:10:50 - INFO - __main__ -   Train loss = 0.5184\n","08/05/2022 13:10:50 - INFO - __main__ -   train_acc = 96.3158\n","Epoch: 100%|██████████| 10/10 [01:22<00:00,  8.28s/it]\n","08/05/2022 13:10:50 - INFO - __main__ -   \n","***** Running evaluation on test dataset *****\n","08/05/2022 13:10:50 - INFO - __main__ -     Num examples = 1725\n","08/05/2022 13:10:50 - INFO - __main__ -     Batch size = 64\n","08/05/2022 13:11:05 - INFO - __main__ -   ***** Eval results *****\n","08/05/2022 13:11:05 - INFO - __main__ -     accuracy = 60.8116\n","08/05/2022 13:11:05 - INFO - __main__ -     loss = 0.6977\n"]},{"output_type":"stream","name":"stdout","text":["{0, 1}\n","              precision    recall  f1-score   support\n","\n","     neutral     0.3930    0.3114    0.3475       578\n","     entails     0.6859    0.7576    0.7200      1147\n","\n","    accuracy                         0.6081      1725\n","   macro avg     0.5394    0.5345    0.5337      1725\n","weighted avg     0.5877    0.6081    0.5952      1725\n","\n"]}],"source":["# forgetable\n","if args.do_train:\n","    # trainer.load_model()\n","    trainer.train()\n","\n","if args.do_eval:\n","    # trainer.load_model()\n","    trainer.evaluate(\"test\")"]},{"cell_type":"code","source":["Prob_per_epoch.shape"],"metadata":{"id":"MdLTY5tCHZJ1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659705066014,"user_tz":-120,"elapsed":29,"user":{"displayName":"AmirHossein Dabiri Aghdam","userId":"03377875757005762455"}},"outputId":"7d678c78-7fe4-4875-fe35-ac858bda47f7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10, 380, 2)"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["import pickle\n","with open(f'Paraphrase_prob_per_{NUM_EPOCHS}epochs.pkl', 'wb') as f:\n","    data = pickle.dump(Prob_per_epoch, f)"],"metadata":{"id":"mTIZuWT-LWt5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q datasets"],"metadata":{"id":"DOhl3B-nUjGJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","\n","with open('Paraphrase_prob_per_10epochs.pkl', 'rb') as f:\n","    data = pickle.load(f)"],"metadata":{"id":"3e0e9DcWTO_c","executionInfo":{"status":"error","timestamp":1659705070099,"user_tz":-120,"elapsed":1017,"user":{"displayName":"AmirHossein Dabiri Aghdam","userId":"03377875757005762455"}},"colab":{"base_uri":"https://localhost:8080/","height":217},"outputId":"4267850d-b4ca-422b-e9fa-20ae7e2d3eb3"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-f814cc484e1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Paraphrase_prob_per_10epochs.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Paraphrase_prob_per_10epochs.pkl'"]}]},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset = load_dataset('glue','mrpc')"],"metadata":{"id":"zA3EQ3BaUEO4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_forgettable_samples(prob_arr,true_label):\n","  forgettable_info = {'sample_index':[],'forgetting_events':[],'True_label_indexes':[]}\n","  for i in range(prob_arr.shape[1]):\n","    forgetting_events = 0\n","    prob_per_sample = prob_arr[:,i,:]\n","    label_per_sample = np.argmax(prob_per_sample,axis=1)\n","    true_label_indexs = np.where(label_per_sample == true_label[i])[0]\n","\n","    for item in (true_label_indexs + 1):\n","      if item not in true_label_indexs and item < prob_arr.shape[0]:\n","          forgetting_events += 1\n","    if forgetting_events > 0:\n","      forgettable_info['sample_index'].append(i)\n","      forgettable_info['forgetting_events'].append(forgetting_events)\n","      forgettable_info['True_label_indexes'].append(true_label_indexs)\n","    else:\n","      continue\n","\n","  return forgettable_info\n","    "],"metadata":{"id":"ggur0PAcI8fD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["true_label = dataset['train']['label'] # Labels should be numerical \n","prob_arr = data\n","F_info = extract_forgettable_samples(prob_arr,true_label)"],"metadata":{"id":"U06vXrqNUs9J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","pd.DataFrame.from_dict(F_info)"],"metadata":{"id":"dahwcO3RpvDx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.shape[1]"],"metadata":{"id":"GhPH9KsbqKFp"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JIOazWmCumuf"},"outputs":[],"source":["# # Save the model to drive\n","# !cp /content/Textual_Entailement_model/pytorch_model.bin /content/drive/MyDrive/Textual_Entailement_model\n","# !cp /content/Textual_Entailement_model/config.json /content/drive/MyDrive/Textual_Entailement_model"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Paraphrase.ipynb","provenance":[{"file_id":"15DcvKQd-hFQsyTMehaz2aTR23NjhlO_U","timestamp":1659430721406},{"file_id":"1UbAth0ImdSdle9I6aPfqUIg667sxTGf_","timestamp":1659397233383}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}